---
title: QEUR23_LMABYS6: GPT4ALLを使って、アメリカに行きたいか！！
date: 2023-08-09
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

# QEUR23_LMABYS6: GPT4ALLを使って、アメリカに行きたいか！！

## ～ 結果からいうとGPT4Allは大したことない ～

QEU:FOUNDER ： “今回は一旦、ＧＰＴのロジックを簡単にして、小生が準備した「評価用データセット」を使ってみようと思います。”

![imageJRL4-7-1](/2023-08-09-QEUR23_LMABYS6/imageJRL4-7-1.jpg)

D先生 ： “このデータセットの源流はなんですか？”

QEU:FOUNDER ： “アメリカの永住権認証テストが元です。ただし、いろいろ内容を変更させていますが・・・。データ数は100あります。そもそも、GPTALLの学習用データセットは英語が主なはずなので、そこそこ解けるだろうと思いました。”

D先生 ： “その結果は？”

QEU:FOUNDER ： “それでは、プログラムをドン！！以前のモノとほとんど変わらないです。CSVファイルでデータフレームを導入しているくらいです、変化点は・・・。”

```python
# GPT4ALLによる連続回答プログラム
# 注意：評価用データセットを使います（100件）
import pandas as pd
import numpy as np
from langchain import PromptTemplate, LLMChain
from gpt4allj.langchain import GPT4AllJ
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Prompts: プロンプトを作成する(default)
template = """Question: {question} Write answer within 10 words.
Answer:"""

# Models: 変換後のモデルを読み込む(j-groovy)
local_path = "D:./ggml-gpt4all-j-v1.3-groovy.bin"

callbacks = [StreamingStdOutCallbackHandler()]
# If you want to use GPT4ALL_J model add the backend parameter
llm = GPT4AllJ(model=local_path, repeat_penalty=1.2, callbacks=callbacks, reset=True)

df = pd.read_csv('QEU-verify100-ja(en).csv')
#print(df)
len(df)

question_list = df["instructions(en)"]
print(question_list)

response_list = df["response(en)"]
print(response_list)

# ----
# 個別の質問応答のタスクを実行する関数を定義する
def qa(question):

    # Promptを完成する
    prompt = PromptTemplate(template=template, input_variables=["question"])
    #print(f"prompt: {prompt}")
    
    # Chains: 作成したモデルを利用可能な状態にする
    llm_chain = LLMChain(prompt=prompt, llm=llm)

    # 質問に回答する
    print(f"No.{i}: Question: {question}")
    answer = llm_chain.predict(question=question)
    print()

    # 答えを表示する
    print(f"Answer: {answer}")
    return answer

# 繰り返し質問する
answer_output = []
mx_output = []
for i in range(100):     # range(len(question_list))

    # 今回の質問を抽出する
    question = question_list[i]
    reference = response_list[i]
    # 質問に回答する
    answer = qa(question)
    answer_output.append(answer)
    # 質問と回答のマトリックス（リスト）を生成する
    mx_output.append([question, answer, reference])

# リストを出力する
#print("--------")
#print("generate answer list")
#print(answer_output)
print("--------")
print("generate output matrix")
print(np.array(mx_output))

# Create DataFrame
mx_out = np.array(mx_output)
columns1 = ["Question", "Answer", "Reference"]
df_out = pd.DataFrame(data=mx_out, columns=columns1)
print(df_out)

# CSV ファイル (result_gpt4all.csv) として出力
df_out.to_csv("verify_gpt4all(en).csv")

```

D先生 ： “コレ（↓）が実行結果ですね。全然ダメですね。ひょっとして、プロンプト（質問）に問題があるんじゃないんですか？”

![imageJRL4-7-2](/2023-08-09-QEUR23_LMABYS6/imageJRL4-7-2.jpg)

QEU:FOUNDER ： “（プロンプトに）全く問題ないとは言わない。ただし、高級なGPTでやってみたら、思い通りの結果が出ましたよ。”

D先生 ： “英語でこの程度の出来だったら、日本語なんか完全に無理ですね。・・・了解です。このテストがベースラインですね。”

QEU:FOUNDER ： “そうですね。今回の質問には「open_qa」と「classification」の2種類の型式を使っています。LangChainでは、open-qaの問題は解決できるでしょうが、classificationの問題を解けるかどうかはわかりません。”

D先生 ： “なるほど、おもしろそうですね。”

QEU:FOUNDER ： “・・・というわけで、次回につづく。”


## ～ まとめ ～

QEU:FOUNDER ： “C部長、コレ（↓）是非見てください。J国の夜明けだ。まだまだ捨てたものでもないよ。”

[![MOVIE1](http://img.youtube.com/vi/kwye3XbVMNs/0.jpg)](http://www.youtube.com/watch?v=kwye3XbVMNs  "【ローカル戦化するEV競争の未来地図】テスラ・BYD・VW・トヨタが各地域のリーダー／グローバルNo.1に意味はない")

C部長 : “ほう・・・。そうなんですか？”

QEU:FOUNDER ： “我々J国は自動車に関していろいろな技術の開発を同時に手掛けているので、その分だけニーズの違う多くの国に対応できます。ですから、昔、GMがFORDに勝ったように車種の多様性で勝てると言っています。”

C部長 : “ほほう・・・、〇〇生産方式とかいう奴ですね。製品をモジュール化させて、より少ない部品でより多くの車種を効率的に生み出すという奴・・・。・・・でも、その生産方式はプラットフォームは同じで「見せかけ」を変える場合に成功したモデルでしょ？”

QEU:FOUNDER ： “お前は何をいっているんだ・・・。”

C部長 : “今、お客様が衆目しているのは「EV」、「エンジン」、「HV」などのプラットフォーム間の差異です。このように大きな差異は生産方式では吸収できないですよ。”


### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠とある会社の検査不正発覚の報道を聞いて）；“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ。“

### オッサン（＠海外子会社の責任者に就任ときのお言葉）；“私の使命はこの会社で終身雇用制を実現することにある。”


QEU:FOUNDER ： “俺はネトユヨだ！！「J国スゴイ！」、文句はあるかァ！！！”


---
title: QEUR23_LMABYS3: GPT4ALLに連続して質問してみた
date: 2023-08-06
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

QEUR23_LMABYS3: GPT4ALLに連続して質問してみた

## ～ 「長いブランクの意味は？」 ～

### ・・・ 前回の続きなんですが ・・・

D先生 ： "これから、LangChainを深く使ってみたいですね。”

QEU:FOUNDER ： "あくまでGPT4Allに限界があることを念頭におきながらね・・・。いつでも、ゲームをチェンジさせる用意があります。”

D先生 ： "いつ、「ゲームチェンジャ(Game Changer)」が現れるのか、わかりませんからね。”

![imageJRL4-4-1](/2023-08-06-QEUR23_LMABYS3/imageJRL4-4-1.jpg)

QEU:FOUNDER ： "もう出て来ているんだけどね、***Llama2っていうすごいのが***・・・。学習方法もどんどん進化して、より軽くなってきています。無料で計算をやれればともかく、「お金」がかかるんだったら、「もうちょっと待つ！」っていうのが妥当な考え方になる「ば・あ・い（場合）」がありますよね。・・・ということで、無理やりGPT4Allをもうちょっと推してみようと思っています。まずは、第一歩として、GPTに対して複数の質問をやってみましょう。”

D先生 ： "そんな邪魔くさいことをナゼするんですか？”

QEU:FOUNDER ： "それは、**モデルを「評価」するため**なんです。それでは、プログラムを晒しましょう。前回からかなり変わっていますよ。ドン！！”

```python
# GPT4ALLによる連続回答プログラム
import pandas as pd
import numpy as np
from langchain import PromptTemplate, LLMChain
from gpt4allj.langchain import GPT4AllJ
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Prompts: プロンプトを作成する(default)
template = """Question: {question} Write answer within 10 words if possible.
Answer:"""

# Prompts: プロンプトを作成する(USC検証用、questionのみの場合)
#template = f"""You are taking U.S. Citizenship Test and you should answer the following ques-tion.\n  Question: {question} \n
#Write within 10 words.\n
#Answer:"""

# Prompts: プロンプトを作成する(USC検証用、questionとcontentを使う場合)
#template = f"""You are taking U.S. Citizenship Test and you should answer the following question. You can refer the following context accordingly.\n  Question: {question} \n
#context:```{context}```\n
#Write within 10 words.\n
#Answer:"""

# Models: 変換後のモデルを読み込む(j-groovy)
local_path = "D:./ggml-gpt4all-j-v1.3-groovy.bin"

callbacks = [StreamingStdOutCallbackHandler()]
# If you want to use GPT4ALL_J model add the backend parameter
llm = GPT4AllJ(model=local_path, repeat_penalty=1.2, callbacks=callbacks, reset=True)

# 質問の文字列リストを定義する
question_list = [
    "What is the capital of Japan?",
    "Who is the author of Harry Potter?",
    "What is the name of the largest bone in the human body?",
    "How many planets are there in the solar system?",
    "What is the chemical formula of water?",
    "Who painted the Mona Lisa?",
    "What is the most spoken language in the world?",
    "Who is the current president of the United States?",
    "What is the square root of 81?",
    "What is the name of the first artificial satellite launched by the Soviet Union?"
]

context = """
Wikipedia was launched on January 15, 2001, by Jimmy Wales and Larry Sanger; its name was coined as a portmanteau of "wiki" and "encyclopedia". Initially available only in English, versions in other languages were quickly developed. The English Wikipedia, with 6.3 million articles as of Feb-ruary 2021, is the largest of the 321 language editions. Combined, Wikipedia's editions comprise more than 55 million articles, and attract more than 17 million edits and more than 1.7 billion unique visitors per month.
"""

# 個別の質問応答のタスクを実行する関数を定義する
def qa(question, context):

    # Promptを完成する
    prompt = PromptTemplate(template=template, input_variables=["question"])
    #print(f"prompt: {prompt}")
    
    # Chains: 作成したモデルを利用可能な状態にする
    llm_chain = LLMChain(prompt=prompt, llm=llm)

    # 質問に回答する
    print(f"No.{i}: Question: {question}")
    answer = llm_chain.predict(question=question)
    print()

    # 答えを表示する
    print(f"Answer: {answer}")
    return answer

# 繰り返し質問する
answer_output = []
mx_output = []
for i in range(len(question_list)):     # range(len(question_list))

    # 今回の質問を抽出する
    question = question_list[i]
    # 質問に回答する
    answer = qa(question, context)
    answer_output.append(answer)
    # 質問と回答のマトリックス（リスト）を生成する
    mx_output.append([question, answer])

# リストを出力する
print("--------")
print("generate answer list")
print(answer_output)
print("--------")
print("generate output matrix")
print(np.array(mx_output))

# Create DataFrame
mx_out = np.array(mx_output)
columns1 = ["Question", "Answer"]
df_out = pd.DataFrame(data=mx_out, columns=columns1)
print(df_out)

# CSV ファイル (result_gpt4all.csv) として出力
df_out.to_csv("result_gpt4all.csv")
```

D先生 ： "ん！？何か変わったんですか？確かに、連続して質問できるようになっていますし、その回答の結果がデータフレームに変換されて、CSVファイルに出力されています。”

QEU:FOUNDER ： "細かく見れば違いが判りますよ、今回の改造でずいぶんと速度が改善しましたよ。何はともあれ、今回は10件の質問を考えまして、それに対するGPT(4all)様の回答結果を得ました。”

![imageJRL4-4-2](/2023-08-06-QEUR23_LMABYS3/imageJRL4-4-2.jpg)

D先生 ： "今回は2件が不正解だったわけですね。1件は計算に関する問題なので、LLMの不得意分野で（失敗は）ありがちです。でも、もう一つの質問は意外でした。”

QEU:FOUNDER ： "まあ、GPTの学習データの中には今回の質問にうまく該当するものがなかったんだろうね。”

D先生 ： "つまり、正解率「8/10」がGPT4ALLの評価のわけですね。”

QEU:FOUNDER ： "すごく簡単な評価だけですがね・・・(笑)。以上、今回はトライアルであり、実際の評価はもっと「いいもの」を使うつもりです。・・・というわけで、次回につづく。


# ～ まとめ ～

QEU:FOUNDER ： "いやあ、面白い・・・。”

[![MOVIE1](http://img.youtube.com/vi/yMcgk2d_iww/0.jpg)](http://www.youtube.com/watch?v=yMcgk2d_iww "古谷経衡氏初出演！ 新刊『シニア右翼』＆政治トピックス・時事問題深掘り」ゲスト：古谷経衡")

C部長 : "以前から、**「ネトウヨさん」ってどういう人なのか**が不思議でした。今回のゲストの彼はその存在量まで推定してますね。”

![imageJRL4-4-3](/2023-08-06-QEUR23_LMABYS3/imageJRL4-4-3.jpg)

QEU:FOUNDER ： "もともと、「ネトウヨ=貧乏な若者」というイメージだったのが、実際には違うんですよね。**ネットに遅れて入ってきたシニア層だった**と・・・。”

C部長 : "FOUNDERって、昔、ネトウヨだったんでしょ？”

QEU:FOUNDER ： "ばれたか・・・（笑）。”


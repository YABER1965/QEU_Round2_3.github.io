---
title: QEUR23_LMABYS10: Llama2のパワーを痛感する	
date: 2023-08-16
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_LMABYS10: Llama2のパワーを痛感する	

## ～ やっぱ、このモデルは異次元だね・・・。 ～

D先生 ： “これから、**「モデル評価プロジェクト」**を始めるにあたり、どのモデルから始めましょうか？”

QEU:FOUNDER ： “これなんか、どう？”

![imageJRL4-11-1](/2023-08-16-QEUR23_LMABYS10/imageJRL4-11-1.jpg)

D先生 ： “えっ！？Llama-v2なの？いきなり**「ラスボス」**じゃないですか？”

![imageJRL4-11-2](/2023-08-16-QEUR23_LMABYS10/imageJRL4-11-2.jpg)

QEU:FOUNDER ： “H2O社の評価ボードによると、**Llama-v2-13b**って、小さいモデルなのにめちゃクチャにパフォーマンスがいいでしょ？あと、このモデルは**「GPTQ」**なんです。”

D先生 ： “4bit量子化されて、コンパクト化されていると・・・。”

QEU:FOUNDER ： “いままで種々のモデルで推論（QA）を試してきました。残念ながら、7b(70億パラメタ)のモデルでも、ハードウェアの要求は厳しいですよ。普通のモデルでは**メモリRAMは20MBぐらい、GPUのVRAMは25GBがないと安定して動きません**。”

D先生 ： “つまり、有名なGPU名だとA100を使わないとうまく行かない。”

QEU:FOUNDER ： “GPTQになると、ずっと軽くなります。今回のモデルは**メモリRAMは15GB、GPUのVRAMは15GBでイケます**よ。有名なGPU名ではテスラのV100とかT4になります。”

D先生 ： “要するに、FOUNDERにはお金がないんですね。”

[＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER ： “カンパしてください（笑）。何はともあれ、まずは本モデルで評価してみましょうよ。今回の質問集はかなりスゴイですよ。”

```python
# 質問の文字列リストを定義する
question_list = [
    "What is the capital of Japan?",
    "Who is the author of Harry Potter?",
    "What is the name of the largest bone in the human body?",
    "How many planets are there in the solar system?",
    "What is the chemical formula of water?",
    "Who painted the Mona Lisa?",
    "What is the most spoken language in the world?",
    "Who is the current president of the United States?",
    "What is the square root of 81?",
    "What is the name of the first artificial satellite launched by the Soviet Union?",
    "安倍晋三は誰ですか？",
    "日本の都道府県で最も面積が大きいのはどれか？",
    "「テニスの王子様」の作者はだれですか？",
    "数学の足し算の問題です。数字4に数字5を加えると、どの数字になりますか？",
    "アメリカの首都は何ですか？"
]
```

D先生 ： “あれ？日本語の質問を付けたんですか？”

![imageJRL4-11-3](/2023-08-16-QEUR23_LMABYS10/imageJRL4-11-3.jpg)

QEU:FOUNDER ： “Webで調べていたのだが、Llama-v2って、**13bぐらいになると多国語能力がついてくる**らしいよ。確認のためにH2Oでデモをしていたんだが、ちゃんとFinetuneされていれば7bでも日本語が出ますよ。”

D先生 ： “時間と共に多くのモデルやツールが開発されて、楽になってきましたね。”

QEU:FOUNDER ： “それでは、事前準備用プログラムに行きましょう。”

```python
# llama2-13B-chat-GPTQによる連続回答プログラム
import pandas as pd
import numpy as np
from langchain import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain
from transformers import AutoTokenizer, pipeline, logging
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# モデル名
model_name_or_path = "TheBloke/h2ogpt-4096-llama2-13B-chat-GPTQ"
use_triton = False
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,
        use_safetensors=True,
        trust_remote_code=False,
        device="cuda:0",
        use_triton=use_triton,
        quantize_config=None)

"""
# To download from a specific branch, use the revision parameter, as in this example:
# Note that `revision` requires AutoGPTQ 0.3.1 or later!

model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,
        revision="gptq-4bit-32g-actorder_True",
        use_safetensors=True,
        trust_remote_code=False,
        device="cuda:0",
        quantize_config=None)
"""

# -----
# Inference can also be done using transformers' pipeline
# Prevent printing spurious transformers error when using pipeline with AutoGPTQ
logging.set_verbosity(logging.CRITICAL)

#print("*** Pipeline:")
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0,            # GPUを使うことを指定 (cuda:0と同義)
    max_new_tokens=512,
    temperature=0.2,
    top_p=0.95,
    repetition_penalty=1.15
)

#################
##################################

llm = HuggingFacePipeline(
    pipeline = pipe, 
    model_kwargs = {"temperature": 0.2}
)

# Prompts: プロンプトを作成する(default)
template = """<prompt>Question: {question} Write answer within 10 words if possible.</prompt>
<answer>Answer:"""

# Promptを完成する
prompt = PromptTemplate(template=template, input_variables=["question"])
#print(f"prompt: {prompt}")

# 質問の文字列リストを定義する
question_list = [
    "What is the capital of Japan?",
    "Who is the author of Harry Potter?",
    "What is the name of the largest bone in the human body?",
    "How many planets are there in the solar system?",
    "What is the chemical formula of water?",
    "Who painted the Mona Lisa?",
    "What is the most spoken language in the world?",
    "Who is the current president of the United States?",
    "What is the square root of 81?",
    "What is the name of the first artificial satellite launched by the Soviet Union?",
    "安倍晋三は誰ですか？",
    "日本の都道府県で最も面積が大きいのはどれか？",
    "「テニスの王子様」の作者はだれですか？",
    "数学の足し算の問題です。数字4に数字5を加えると、どの数字になりますか？",
    "アメリカの首都は何ですか？"
]

context = """
Wikipedia was launched on January 15, 2001, by Jimmy Wales and Larry Sanger; its name was coined as a portmanteau of "wiki" and "encyclopedia". Initially available only in English, versions in other languages were quickly developed. The English Wikipedia, with 6.3 million articles as of Feb-ruary 2021, is the largest of the 321 language editions. Combined, Wikipedia's editions comprise more than 55 million articles, and attract more than 17 million edits and more than 1.7 billion unique visitors per month.
"""

# 個別の質問応答のタスクを実行する関数を定義する
def qa(i, question, context):

    # Chains: 作成したモデルを利用可能な状態にする
    llm_chain = LLMChain(prompt=prompt, llm=llm)

    # 回答を表示する
    print(f"No.{i}: Question: {question}")
    answer = llm_chain.predict(question=question)
    pos_str = answer.find('</answer>')  # </answer>記号以前の情報を使う
    cut_answer = answer[0:pos_str]
    print(f"Answer: {cut_answer}")
    
    return cut_answer

# 繰り返し質問する
mx_output = []
for i in range(len(question_list)):     # range(len(question_list))

    # 今回の質問を抽出する
    question = question_list[i]
    # 質問に回答する
    answer = qa(i, question, context)
    # 質問と回答のマトリックス（リスト）を生成する
    mx_output.append([question, answer])

# リストを出力する
print("--------")
print("generate output matrix")
print(np.array(mx_output))


# -----
# Create DataFrame
mx_out = np.array(mx_output)
columns1 = ["Question", "Answer"]
df_out = pd.DataFrame(data=mx_out, columns=columns1)
print(df_out)

# CSV ファイル (GPTQ-llama2-13B(Raw).csv) として出力する
df_out.to_csv("/content/drive/MyDrive/GPTQ-llama2-13B(Raw).csv")

```

QEU:FOUNDER ： “結果をCSVで出力してみました。”

![imageJRL4-11-4](/2023-08-16-QEUR23_LMABYS10/imageJRL4-11-4.jpg)

D先生 ： “えっ！？スゴイじゃないですか。**日本語の質問も正解している**・・・。”

QEU:FOUNDER ： “この結果を見れば、他のモデルを試すのも面倒になってきますよね。もちろん、**GPTQのモデルを我々がうまくFinetuneできるのかは知りませんが**・・・。”

D先生 ： “それでは、まず最初にこのモデルを評価してみましょう。”


## ～ まとめ ～

C部長 : “そういえば、FOUNDERはなぜLLM（大規模言語モデル）の開発をやろうと思っていたんでしたっけ・・・。”

QEU:FOUNDER ： “これは、**世界平和（のため）**・・・。”

C部長 : “プッ（ワロタ）・・・。”

```python
(title)
コンゴ自由国

(original)
コンゴ自由国（フランス語:État indépendant du Congo、Kongo-Vrijstaat）は、かつてアフリカのザイール川流域に存在した国である。国と称しているが、実態はベルギー国王レオポルド2世の私領地であった。植民地時代を経て、のちにコンゴ共和国（後のコンゴ民主共和国）として独立を果たした。
ベルギー国王レオポルド2世はスタンリーにザイール川流域を探検させる。国王の支援による探検だったためその成果は国王に帰属し、国王は1882年に「コンゴ国際協会」に委託支配させ、1885年のベルリン会議では公式に国王の私領地になった。
国王の私領となったコンゴ自由国では耕作地も全てが国王の所有となり、住民は象牙やゴムの採集を強制された。規定の量に到達できないと手足を切断するという残虐な刑罰が容赦なく科され、前代未聞の圧制と搾取が行われていた。コンゴ自由国の自由国とは、「住民が自由な国」という意味ではなく、自由貿易の国という意味の英語（Congo Free State）であり、公用語であるフランス語における正式国号はコンゴ独立国であった。
当時は多かれ少なかれ抑圧的な植民地政策が行われていた欧米列強各国からも、人道主義の立場に基づく非難が殺到した。特にイギリスは領事に実態調査を行わせている。またジャーナリストのエドモンド・モレルが「赤いゴム」という著作で、手足を切り落とす過酷な刑罰の下でのゴム採集の実情を白日のもとにさらけ出した。
国際社会の非難の声はますます高まり、国王の恣意的な暴政にベルギー政府も黙っていられなくなった。1908年10月、ベルギー政府は植民地憲章を制定し、国王はベルギー政府からの補償金と引き換えにコンゴ自由国を手放すことになった。
同年11月、コンゴ自由国はベルギー政府の直轄植民地ベルギー領コンゴになった。これにより統治の実情は多少なりとも改善され、初等教育の導入などが行われた。
コンゴ自由国を圧政により支配したレオポルド2世への批判は、21世紀も続いている。2020年6月、アメリカ合衆国で発生した反人種差別デモがベルギーに波及した際には、アントワープ市内にあったレオポルド2世の像が襲撃を受け、放火された上に塗料が掛けられていたため、市によって撤去されている。

(summary)
コンゴ自由国は、かつてアフリカのザイール川流域に存在した国で、実態はベルギー国王レオポルド2世の私領地であった。ベルギー国王レオポルド2世はスタンリーにザイール川流域を探検させ、その成果は国王に帰属し、1885年のベルリン会議では公式に国王の私領地になった。国王の私領となったコンゴ自由国では耕作地も全てが国王の所有となり、住民は象牙やゴムの採集を強制され、規定の量に到達できないと手足を切断するという残虐な刑罰が容赦なく科された。当時は多かれ少なかれ抑圧的な植民地政策が行われていた欧米列強各国からも、人道主義の立場に基づく非難が殺到した。1908年10月、ベルギー政府は植民地憲章を制定し、国王はベルギー政府からの補償金と引き換えにコンゴ自由国を手放すことになり、同年11月、コンゴ自由国はベルギー政府の直轄植民地ベルギー領コンゴになった。これにより統治の実情は多少なりとも改善され、初等教育の導入などが行われた。

```

QEU:FOUNDER ： “ちょっと、笑いごとじゃないんですがね・・・。歴史がわからないと、今、現在に起こっていることの本当の意味はわかりません。”

C部長 : “今、起こっていること？確かに、**コンゴ自由国**の件は、ひどいとは思いますが・・・。ずっと昔のことじゃないですか。”

![imageJRL4-11-5](/2023-08-16-QEUR23_LMABYS10/imageJRL4-11-5.jpg)

QEU:FOUNDER ： “あのときにG〇の国に虐げられてきた人たちは、その痛みを今でも、ちゃんと覚えているんです。彼らは**近年に急速に力をつけて来て、その対応策を具体的な行動に移してきて**います。”

C部長 : “こんなにたくさんの国が入りたいと思っているんですね。”

![imageJRL4-11-6](/2023-08-16-QEUR23_LMABYS10/imageJRL4-11-6.jpg)

QEU:FOUNDER ： “特に歴史というのは、**事象の一面だけ見ていると本当の事がわからない**んですよ。さらにいうと、わかりやすいモノだけを拾って、そのまま歴史的な偏見を持って大きくなっちゃうと、大変に困ったことが起きてしまいます。”

[![MOVIE1](http://img.youtube.com/vi/yMcgk2d_iww/0.jpg)](http://www.youtube.com/watch?v=yMcgk2d_iww "古谷経衡氏初出演！ 「新刊『シニア右翼』＆政治トピックス・時事問題深掘り」(2023年３月30日・前半無料パート)ゲスト：古谷経衡")

C部長 : “（お〇カな）ネトウヨが**量産される**・・・（笑）。”

- **おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」**
- **オッサン（＠車中、N社検査不正について）： 「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」**
- **オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」**

QEU:FOUNDER ： “ネトウヨって**YouTubeの「申し子」**だと思っているんです。つまり、歴史のような複雑なモノを取り扱うには、現状のメディアは不完全です。**個人の好奇心が必要になるのは大前提にはなります。**しかし、今の技術では「ネトウヨの量産」は回避できるんですよ。”


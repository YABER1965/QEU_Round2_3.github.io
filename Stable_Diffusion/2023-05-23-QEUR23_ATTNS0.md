---
title: QEUR23_ATTNS0:　今話題のAttentionを使っている(yelp review - その1) 
date: 2023-05-23
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_ATTNS0:　今話題のAttentionを使っている(yelp review - その1) 

## ～　よくも、こんなにすごいメトリックスを発明したもんだ　～

### ・・・　前回のつづきです　・・・

D先生 ： “う～ん・・・。今回は記念的なイベントになりました。外観検査自動機のプロジェクトでCNNの代替としてRT法が使えることをすでに証明しています。今回は、Embeddingの代替としてT法が使えることがわかったわけです。”

![imageJRL3-40-1](/2023-05-23-QEUR23_ATTNS0/imageJRL3-40-1.jpg)

QEU:FOUNDER  ： “率直、今後ますますディープラーニングを使ったAIが存在感を高めることは自明です。そして、必然的に**タグチメソッドは「昔の名前で・・・」になっていく**でしょう。それでも、我々の試みを通じて次世代に何かを残せるとうれしい・・・。”

D先生 ： “しかし、前回の結果はまだ満足できるものとは言えませんでした。もちろん、前回のトライアルでは単位空間を10個しかなく、データ量1000件以下とディープラーニングとしては少ないです。せっかく、**NON-DeepLearningでやっている**んだから、もうちょっとひと踏ん張りしてみないと・・・。”

![imageJRL3-40-2](/2023-05-23-QEUR23_ATTNS0/imageJRL3-40-2.jpg)

QEU:FOUNDER  ： “じゃあ、今、**話題のAttention**でも使ってみる？”

C部長  ： “ん？注意(Attention)がなんだって・・・？”

QEU:FOUNDER  ： “そう、そのAttentionです・・・（笑）。今回は、Attentionについてのわかりやすい説明をしたブログ(↓)を見つけたので、それをまずは見てみましょう。”

![imageJRL3-40-3](/2023-05-23-QEUR23_ATTNS0/imageJRL3-40-3.jpg)

C部長  ： “Attentionって、今調べたらNLP（自然言語処理）分野で圧倒的な性能をたたき出したディープラーニング手法じゃないですか・・・。そんなもの、我々が手を出すのはちょっと・・・。”

QEU:FOUNDER  ： “しかし、やってみましょう。怖がって、なにも知らなくなるよりマシ。それではプログラムと実行結果をドン！！前述のブログの内容とほとんど同じですが、この手法の本質をわかりやすくするために重みの形を正方行列(3x3)ではなく、**長方形(4x3)**にしています。”

```python

# 今話題のAttentionをやってみる
from numpy import array
from numpy import random
from numpy import dot
from scipy.special import softmax

# この例題では、入力文は「word1-2-3-4」で成り立っている
# 語彙はエンベディングでベクトル（4ケタ）に変換されている
word_1 = array([1, 0, 1, 0])
word_2 = array([0, 1, 0, 1])
word_3 = array([1, 1, 0, 0])
word_4 = array([0, 0, 1, 0])

words = array([word_1, word_2, word_3, word_4])
print(words)
#[[1 0 1 0]
# [0 1 0 1]
# [1 1 0 0]
# [0 0 1 0]]

# -----
# 重み（Weight）を作る
# このWを機械学習でつくるか、それとも他の方法で作るのかはアナタ次第
# ここでは、デモのためにランダム生成
random.seed(42) # to allow us to reproduce the same attention values
W_Q = random.randint(3, size=(4, 3))
W_K = random.randint(3, size=(4, 3))
W_V = random.randint(3, size=(4, 3))
print("--Q--")
print(W_Q)
print("--K--")
print(W_K)
print("--V--")
print(W_V)
#--Q--
#[[2 0 2]
# [2 0 0]
# [2 1 2]
# [2 2 2]]
#--K--
#[[0 2 1]
# [0 1 1]
# [1 1 0]
# [0 1 1]]
#--V--
#[[0 0 0]
# [2 2 2]
# [1 2 1]
# [1 2 1]]

# -----
# ここで重み行列を正方にしていないことに注意。エンベッドはタテです。
# 行列計算が行われます。
Q = words @ W_Q
K = words @ W_K
V = words @ W_V
print("--Q--")
print(Q)
print("--K--")
print(K)
print("--V--")
print(V)
#--Q--
#[[4 1 4]
# [2 0 0]
# [4 0 2]
# [2 1 2]]
#--K--
#[[2 3 3]
# [0 2 1]
# [2 4 3]
# [0 1 1]]
#--V--
#[[1 1 0]
# [0 1 1]
# [1 2 1]
# [0 0 0]]

# -----
scores = Q @ K.transpose()
print(scores)
#[[11 10 11  5]
# [12  8 10  6]
# [ 6  4  4  4]
# [ 7  6  7  3]]

# -----
# Softmax関数で確率値(probability value)の群に計算されます
probs = softmax(scores / K.shape[1] ** 0.5)
print(probs)
#[[0.1804795  0.10131829 0.1804795  0.00564921]
# [0.32149034 0.03193065 0.10131829 0.01006301]
# [0.01006301 0.00317138 0.00317138 0.00317138]
# [0.01792535 0.01006301 0.01792535 0.00178036]]

# -----
# 最後にattention値が計算されます
# 大きな値を持つ要素が1個と、あとは無視できる値の要素群になる
attention = probs @ V 
print(probs)
#[[9.92901880e-01 1.62883281e+00 6.35930933e-01]
# [1.22920950e-05 1.90485723e-05 6.75647734e-06]
# [3.95390534e-03 5.93279494e-03 1.97888961e-03]
# [3.08678210e-03 5.06988081e-03 1.98309872e-03]]

```

QEU:FOUNDER ： “インプットと重みの行列計算がポイントです。この処理は**インプットベクトルと重みベクトルの内積計算と解釈できます**。この先生（↓）によると、内積とは類似度のメトリックス化のことだから、重みが意味する情報に最も近いインプットベクトルの要素はどこになるのかを示しているんです。”

[![MOVIE1](http://img.youtube.com/vi/bPdyuIebXWM/0.jpg)](http://www.youtube.com/watch?v=bPdyuIebXWM "Attention - 全領域に応用され最高精度を叩き出す注意機構の仕組み")

C部長  ： “この重み行列って、ディープラーニングで計算するしかないんですか？”

D先生 ： “さあ・・・。バック・プロパゲーションで形成するのが一般的ではあるのだが・・・。”

QEU:FOUNDER  ： “小生は**Attentionという手法をあくまでメトリックスを生成する手段としてみている**んです。そうすることで、いったい何ができるのか？是非、カンパをお願いします！！”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-created&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)
 
D先生 ： “常に新しい着眼点を持つことは良いことです。”


## ～　まとめ　～

### ・・・　もう、ChatGPTの大騒ぎは終わりにしない？　・・・

QEU:FOUNDER ： “とうとう、**ChatGPT騒動の解釈についての「決定版」が出た**ね。”

[![MOVIE2](http://img.youtube.com/vi/vw-KWfKwvTQ/0.jpg)](http://www.youtube.com/watch?v=vw-KWfKwvTQ "GPT-4 - How does it work, and how do I build apps with it? - CS50 Tech Talk")

C部長 : “おおっ・・・。天下のH大学様じゃないですか・・・。”

QEU:FOUNDER ： “内容はわかりやすいよ。GPTというのは次の文字を予測するためのエンジンにすぎません。チャット(する)GPTって、ここでいうGPTとは違うんです。”

![imageJRL3-40-4](/2023-05-23-QEUR23_ATTNS0/imageJRL3-40-4.jpg)

C部長 : “じゃあ、ChatGPTってなんですか？”

QEU:FOUNDER ： “チャット用に作ったAPIです。チャットのためのラッパ(wrapper)がついています。ラッパを変えれば他の事もできるんです。”

![imageJRL3-40-5](/2023-05-23-QEUR23_ATTNS0/imageJRL3-40-5.jpg)

C部長 : “**プロンプト・エンジニアリング**のことですね？”

QEU:FOUNDER ： “小生がQEUプロジェクトに掲げたプロンプト・エンジニアリングはStable_Diffusionのものです。GPTのような純粋な言語処理のものじゃないです。長期的には、GPTを使用する用途が決まってくると専用のAPIが出来てきて、ユーザーがプロンプトエンジニアリングを気にする必要はなくなります。むしろ・・・。”

![imageJRL3-40-6](/2023-05-23-QEUR23_ATTNS0/imageJRL3-40-6.jpg)

C部長 : “むしろ？”

D先生： “あれ、Creativity？**ドメイン知識がシステム図に入っています**ね。”

QEU:FOUNDER ： “今、GPTの事で大騒ぎをしているが、アレは実は大したものじゃないです。**中に入っているデータが少なすぎて現実の用途にはほとんど使いモノになりません。**”

D先生： “自分で貧弱なプロンプトを打ってみて、思いがけず自然な回答が返ってきて驚いているだけでしょ？猫ちゃんが、あたらしいおもちゃに触ってビックリしているような・・・。”

[BLOG-LINK](https://qeuslife.blogspot.com/2023/05/why-taiwanese-love-japanese-culture.html)

QEU:FOUNDER ： “ブログのように、まとまった情報量の記事を書く場合、GPT単体では全然情報が足らないのが見えてきますよ。ドメイン知識は人間が準備するしかないから、**人間がCreativeでありたいと思う限り**において、これからもAIが人間を超えることはありえません。”


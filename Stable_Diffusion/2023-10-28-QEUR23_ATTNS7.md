---
title: QEUR23_ATTNS7: ゲーム2048用の強化学習をやってみた（畳み込み:convolution）
date: 2023-10-28
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_ATTNS7: ゲーム2048用の強化学習をやってみた（畳み込み:convolution）

## ～ 意外とうまく行った！メトリックスのクセが見えた！ ～

C部長 : “ちょっと教えてください。昔、2048で強化学習をやったときには**「もっとハイスコア」**だったように思います。スコアは1万ポイントを軽く超えていました。あれは、いったい何だったんですか？”

![imageJRL3-47-1](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-1.jpg)

QEU:FOUNDER ： “2048ゲームの弱点にあらかじめ手を打っていたんですよ。命令データの処理には、**「ワンホットエンコーディング」**を使っていたし、命令を決めるときにはあらかじめ「命令ミス（ボードが動かない）」を避ける選択肢を入れていました。”

C部長 : “このアイデアを今回のプログラムに入れると・・・。”

QEU:FOUNDER ： “簡単に1万を超えると思うよ。学習がレベルアップされているから、1万回学習させて、スコアが3万ぐらいになっても驚かない。さて、今回の本題である「畳み込みメトリックスの活用」ですね。プログラムを少しだけ変えればいいです。いきなりプログラムの紹介に入りますか。ドン・・・！”

```python
# ----
# 2048ゲームの簡単な強化学習(CONV-SCORE)
# 強化学習の参考HP(PYTORCH OFFICIAL)
# -ttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
# ----
import math
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple, deque, Counter

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# ---------------- 
#import matplotlib.pyplot as plt
#%matplotlib inline

# ---------------- 
# set up matplotlib
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display
plt.ion()

# if GPU is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---
Transition = namedtuple('Transition',
                        ('state', 'action', 'next_state', 'reward'))

#=================================================
# SIMPLE CONVOLUTION DATA
#=================================================
mx_cw1 = torch.tensor([[1,1,1],[0,0,1],[0,0,1]]).float()
mx_cw2 = torch.tensor([[0,0,1.66],[0,1.66,0],[1.66,0,0]]).float()
mx_cw3 = torch.tensor([[1,0,0],[1,0,0],[1,1,1]]).float()
mx_cc1 = torch.tensor([[1,1,1],[1,0,0],[1,0,0]]).float()
mx_cc2 = torch.tensor([[1.66,0,0],[0,1.66,0],[0,0,1.66]]).float()
mx_cc3 = torch.tensor([[0,0,1],[0,0,1],[1,1,1]]).float()
mx_dtm = torch.tensor([[0,1,0],[1,1,1],[0,1,0]]).float()
print("--- mx_dtm ---")
print(mx_dtm)

# ---------------------------
# 畳み込みカーネルを生成する
cw_kernels  = torch.stack([mx_cw1, mx_cw2, mx_cw3])
cc_kernels  = torch.stack([mx_cc1, mx_cc2, mx_cc3])
tani_kernel = mx_dtm
print("--- cw_kernels ---")
print(cw_kernels)
print(cw_kernels[0])

#=================================================
# CONVOLUTION FUNCTIONS
#=================================================
# 畳み込み処理を実施する
def apply_kernel(row, col, kernel, mx_tensor):
    return (mx_tensor[row:row+3,col:col+3] * kernel).sum()

# ---------------------------
# 畳み込みテンソルイメージを生成する
def create_tsrConv(mx_signal):

    # ----
    # パネルスコアを線形化する
    for i in range(4):
        for j in range(4):
            if mx_signal[i][j] > 0:
                mx_signal[i][j] = math.log2(mx_signal[i][j])

    # テンソル化する
    mx_signal = torch.tensor(mx_signal).float()
    #print("--- mx_signal ---")
    #print(mx_signal)

    # ----
    # 単位空間の畳み込み
    tani_image = torch.tensor([[apply_kernel(i,j, tani_kernel, mx_signal) for j in range(2)] for i in range(2)])
    tani_array = tani_image.numpy().flatten()
    tani_mean  = np.mean(tani_array) + 0.0001
    #print(tani_array)

    # ----
    # 信号空間（Clockwise群）の畳み込みとメトリックス化
    acc_maxcw = np.zeros(3)
    acc_sigcw = np.zeros((3,4))
    #print(acc_sigcc)
    for k in range(3):
        sig_image = torch.tensor([[apply_kernel(i,j, cw_kernels[k], mx_signal) for j in range(2)] for i in range(2)])
        sig_array = sig_image.numpy().flatten()
        sig_max   = np.max(sig_array)
        #print(sig_array)
        acc_sigcw[k,:] = sig_array / tani_mean
        acc_maxcw[k]   = round(math.log(sig_max/tani_mean+1),4)
    #print(acc_sigcw)
    #print(acc_maxcw)

    # ----
    # 信号空間（Counter Clockwise群）の畳み込みとメトリックス化
    acc_maxcc = np.zeros(3)
    acc_sigcc = np.zeros((3,4))
    #print(acc_sigcc)
    for k in range(3):
        sig_image = torch.tensor([[apply_kernel(i,j, cc_kernels[k], mx_signal) for j in range(2)] for i in range(2)])
        sig_array = sig_image.numpy().flatten()
        sig_max   = np.max(sig_array)
        #print(sig_array)
        acc_sigcc[k,:] = sig_array / tani_mean
        acc_maxcc[k]   = round(math.log(sig_max/tani_mean+1),4)
    #print(acc_sigcc)
    #print(acc_maxcc)

    # 特徴ベクトルを結合する
    all_max = np.append(acc_maxcw,acc_maxcc)
    #print(all_max)

    return all_max

```

QEU:FOUNDER ： “前回のプログラムとの違いは、メトリックスの採用だけです。あと、前回で説明し忘れていたことを1件・・・。スコアを線形化するために、パネルの数字を2の対数で処理しています。”

C部長 : “この**対数処理も、2048の強化学習で広く知られている秘訣**です。これを「する・しない」で結果が変わりますか？”

QEU:FOUNDER ： “全然、結果が全然変わりますよ。一度、体験してみればどうですか？”

```python
# ---------------- 
# 2048ゲームのプログラム
# ---------------- 
# ボードのサイズ
SIZE = 4

省略


#=================================================
# REINFORCEMENT LEARNING
#=================================================
# ReplayMemory
class ReplayMemory(object):

省略

#=================================================
# MAIN ROUTINE
#=================================================
# ----------------
# 学習結果のグラフ化
# ----------------
# plot_durations
def plot_durations(show_result=False):

省略

#=================================================
# PROGRAM INITIALIZATION
#=================================================
# Get number of actions from 2048's action space
n_actions = 4
# Get the number of state observations
n_observations = 6

# ----
# HYPER PARAMETERS
# BATCH_SIZE は、リプレイ バッファからサンプリングされたトランジションの数です。
# GAMMA は割引係数です。
# EPS_START is the starting value of epsilon
# EPS_END is the final value of epsilon
# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay
# TAU is the update rate of the target network
# LR is the learning rate of the ``AdamW`` optimizer
BATCH_SIZE = 128
GAMMA = 0.92
EPS_START = 0.999
EPS_END = 0.02
EPS_DECAY = 20000
TAU = 0.005
LR = 1e-4

# ---
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())

optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = ReplayMemory(1000)

num_failed = 1   # ボードが動かない(Nomove)
steps_done = 0
episode_durations = []
num_episodes = 1000
max_turn = 800

# --------------------------
# 初期化       
# --------------------------
# ボードを初期化する
board = init_board()

# 学習評価用の配列を初期化する
acc_episode = []
acc_turn = []
acc_score = []
acc_maxconv = []
acc_loss_epi = []
acc_failed_epi = []
acc_eps = []

#=================================================
# PROGRAM RUN!
#=================================================
# ゲームが終了するまで繰り返す
# メインの処理を実行する
for i_episode in range(num_episodes):

    # ボードをリセットする関数(ただし、ゲームを5ターンだけ、あらかじめ動かしておく)
    board = reset_board(board)
    # ボードを表示する
    print("--- RESETED, GAME NO: {} ---".format(i_episode))
    print_board(board)
    # ----------   
    # 畳み込みテンソルイメージを生成する
    np_board = np.array(board)
    observation = create_tsrConv(np_board)
    #print(observation)
    np_observation = np.array(observation)
    state = torch.tensor(np_observation, dtype=torch.float32, device=device).unsqueeze(0)
    #print("state:",state)

    # 単一ゲーム評価用の配列を初期化する
    arr_turn = []
    arr_reward = []
    arr_score = []
    arr_yzero = []
    arr_loss_game = []
    # ----
    last_nomove = False   # 前回にミス命令が起きたか
    i_turn = 0  # ターン数
    score = 0   # 累積スコア（マイナスな含まない）
    num_failed = 1   # ボードが動かない(Nomove)

    # ゲームを始める
    while True:

        # -----------
        # ゲームの進め方(ACTION)
        #arr_action = ["s", "a", "w", "d"]
        # s - 0 : DOWN
        # a - 1 : LEFT
        # w - 2 : UP
        # d - 3 : RIGHT
        # -----------
        # ゲームを実行する
        action, eps_threshold = select_action(state, last_nomove, num_failed)
        next_board, reward_raw, done = run_game(board, action.item())
        # NO MOVEの処理
        if reward_raw < 0:
            last_nomove = True
            num_failed = num_failed + 1
            score = score
        else:
            last_nomove = False
            score = score + reward_raw
        reward = torch.tensor([reward_raw], device=device)
        #print("state:",state)
        #print("reward:",reward)
        #print("done:",done)

        # ----------   
        # 畳み込みテンソルイメージを生成する
        np_next_board = np.array(next_board)
        next_observation = create_tsrConv(np_next_board)
        #print(observation)

        if done or i_turn > max_turn:
            done = True
            next_state = None
        else:
            np_observation = np.array(next_observation)
            next_state = torch.tensor(np_observation, dtype=torch.float32, device=device).unsqueeze(0)

        # ------
        # DQN experience replay
        # Store the transition in memory
        memory.push(state, action, next_state, reward)

        # Move to the next state
        observation = next_observation
        board = next_board
        state = next_state

        # 最適化の 1 ステップを実行します (ポリシー ネットワーク上で)
        # NO MOVE以外で学習します
        loss = 0
        if (i_turn+1)%20 == 0:      #reward_raw >= 0:
            loss = optimize_model()
            #print("loss:",loss)
            # ターゲットネットワークの重みのソフトアップデート
            # θ′ ← τ θ + (1 −τ )θ′
            target_net_state_dict = target_net.state_dict()
            policy_net_state_dict = policy_net.state_dict()
            for key in policy_net_state_dict:
                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + tar-get_net_state_dict[key]*(1-TAU)
            target_net.load_state_dict(target_net_state_dict)
            # EPSのカウントアップ
            steps_done += 1

        # ------
        # ゲーム実行の結果を出力する
        if (i_turn+1)%100 == 0:
            print("i_episode:{0},i_turn:{1},action:{2},reward:{3},done:{4},score:{5}".format(i_episode,i_turn,action,reward,done,score))
            if i_turn > 100 and (i_turn+1)%200 == 0:
                print_board(board)

        # マトリックス化
        if i_turn == 0:
            mx_cvmax = np.array([observation])
        else:
            mx_cvmax = np.vstack((mx_cvmax,observation)) # 結合してみる

        # ----------   
        # ZERO COUNTER
        state_flatten = np.array(board).flatten()
        cc = Counter(state_flatten)
        arr_yzero.append(cc[0]) 

        # 配列へ要素を追加する
        arr_turn.append(i_turn)
        arr_reward.append(reward_raw)
        arr_score.append(score)
        arr_loss_game.append(loss)

        # ボードに動きがなければ、ゲームオーバーとする
        if done == True or i_turn > max_turn:
            print("Game over")
            # -----
            # ゲーム結果のグラフ化
            #show_graph_game(arr_turn, arr_reward, arr_score, arr_yzero, mx_cvmax)
            episode_durations.append(i_turn + 1)
            plot_durations()

            # ----
            # METRICS
            acc_conv = []
            mask_turn = np.min([80,i_turn-20])
            acc_conv.append(np.max(mx_cvmax[mask_turn:,0]))
            acc_conv.append(np.max(mx_cvmax[mask_turn:,1]))
            acc_conv.append(np.max(mx_cvmax[mask_turn:,2]))
            acc_conv.append(np.max(mx_cvmax[mask_turn:,3]))
            acc_conv.append(np.max(mx_cvmax[mask_turn:,4]))
            acc_conv.append(np.max(mx_cvmax[mask_turn:,5]))

            # -----
            # 学習評価用の配列を初期化する
            acc_episode.append(i_episode)
            acc_turn.append(i_turn)
            acc_score.append(score)
            acc_maxconv.append(np.max(acc_conv))
            acc_failed_epi.append(round(num_failed/(i_turn+1),5))
            acc_eps.append(eps_threshold)
            if None in arr_loss_game:
                print("None is present in the list")
                acc_loss_epi.append(10)
            else:
                acc_loss_epi.append(np.min(arr_loss_game))
            
            # -----
            # マトリックス化
            if i_episode == 0:
                mx_accmax = np.array([acc_conv])
            else:
                mx_accmax = np.vstack((mx_accmax,acc_conv)) # 結合してみる
            break
        else:
            i_turn = i_turn + 1

# -----
# 評価結果のグラフ化
print('Complete')
plot_durations(show_result=True)
plt.ioff()
plt.show()

```

QEU:FOUNDER ： “今回は、今回はいきなり**TAU=0.05**として10000回の学習結果をみてみましょう。”

**（今回の学習結果：CONVでの学習）**

![imageJRL3-47-2](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-2.jpg)

C部長 : “あれ？このグラフは前回には紹介しなかったですね。”

QEU:FOUNDER ： “これは、もともとのPyTorchの事例についていたグラフです。ゲーム完了後にグラフが動的に更新されるスグレモンです。ただし、これはターン数の表示するグラフなので、この情報自体には、それほどの価値はないんだけどね。それでは、メトリックスの出力結果を見てみましょう。おっと、その前に、「前回の復習」をしておきます。”

**（前回の学習結果：RAWでの学習）**

![imageJRL3-47-3](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-3.jpg)

C部長 : “4x4の生データを使って行った1万回の学習ですから、その結果は「そこそこ、こんなもん」ですよね。それにしても、**ゲームに命令ミスが多い**です。命令ミスが少ないゲームも、かなりあるんですが・・・。”

QEU:FOUNDER ： “そこまでの経過を理解したところで、今回の結果を評価しましょう。”

**（今回の学習結果：CONVでの学習-1）**

![imageJRL3-47-4](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-4.jpg)

**（今回の学習結果：CONVでの学習-2）**

![imageJRL3-47-5](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-5.jpg)

D先生 ： “ある程度は予測はしていました。それにしても、パフォーマンスの変化に驚かされますね。特に、**「命令ミス発生率」のグラフの様子が全く変わりました**。”

QEU:FOUNDER ： “このグラフは本メトリックスの特性をよく表しています。このメトリックスは学習を加速し、**スコアを上げる能力がとても高い**んです。しかし、命令ミスのリスクを評価する能力が弱いんです。”

D先生 ： “そこを「敢て」・・・、もっと命令ミスを減らす方法はないかなあ・・・？”

QEU:FOUNDER ： “メトリックス以外の方法で！？う～ん・・・、大して期待はできないが、（方法は）ないことはないです。ポリシー選択のロジックを変えてもいいですよ。現状はQ（価値）関数が最大になる命令を選択していますが、ボルツマン選択という確率を見て命令を選択する方法があります。2048ゲームのように環境が複雑な場合には、この**ボルツマン選択のほうがパフォーマンスが上がる**と思います。”

C部長 ： “面白そうですね。”

QEU:FOUNDER ： “今回のシリーズは**テクノ・メトリックスの性能を評価する**ためにやっているんです。こんな根本的な部分を途中で変えることはできない。一連のメトリック評価の作業が終わったあとで、もし小生が覚えていたらやります（笑）。”

D先生 ： “次回のSOART3メトリックス（感度のみ）につづく。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

D先生(設定年齢65歳) ： “ホント！**私の「J国スゴイ」のロス**をどうやって埋め合わせればいいんですか！！ちょっと、FOUNDER、私の心の声を聞いてください！！私のアイデンティティは**「A国のつぎにスゴイJ国」**なんですよ！“

![imageJRL3-47-6](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-6.jpg)

QEU:FOUNDER(設定年齢65歳) ： “「A国のつぎにスゴイJ国」なのでC国を排除って・・・（笑）。**シニア右翼（↓）**か・・・。”

![imageJRL3-47-7](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-7.jpg)

QEU:FOUNDER ： “なんと、あのD先生にもこんな病気が蔓延・・・。う～ん、しようがない・・・。じゃあ、みんなで「J国スゴイ」ロスの対策を考えましょう。”

[![MOVIE1](http://img.youtube.com/vi/WT16I7b8PLo/0.jpg)](http://www.youtube.com/watch?v=WT16I7b8PLo "異色のコラボがついに実現 金子勝VSダースレイダー 日本の政治と経済に未来はあんの？")

QEU:FOUNDER ： “この爺さん（↑）、ほんとに**「異色の業界人」**だよね。普通、世の中で見かけるのは、**「ワン・フレーズ」**で簡単な対策をやたら騒ぎ立てる人たちとか・・・。”

![imageJRL3-47-8](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-8.jpg)

D先生 ： “思いっきり**「癖のある」**人たちとか・・・。 “

![imageJRL3-47-9](/2023-10-28-QEUR23_ATTNS7/imageJRL3-47-9.jpg)

QEU:FOUNDER ： “まあ、あの爺さんは話が具体的なんですね。ただ、本来、複雑な現象を「ありのままに」語っているから、あの人の話は複雑ですよね。同時に**「悲観的」になる**のは、しようがない。・・・それにしても、この10年のJ国の迷走の原因は**「生産性に関する議論」**につきるよね。”

### 生産性 = （製品の付加価値）/(製品を作るための資源の投入金額)

D先生 ： “この10年間、分母の**「資源の投入金額を減らせば経済が成長する」と宣伝していました**よね。 誰とはいわんが・・・。“

QEU:FOUNDER ： “じゃあ、プロセスを革命的に変える力を持つ生成AI(GPT etc)が普及して人が極端に減ってきたらどう説明するんだろうか？Ｃ部長、プラクティショナーの立場から教えてください。**ＡＩで人が減ってきたら、経済は成長する**の？”

[![MOVIE2](http://img.youtube.com/vi/SUJsvJJXJh8/0.jpg)](http://www.youtube.com/watch?v=SUJsvJJXJh8 "Before生成AI ＆ After 生成AI【児玉龍彦×辻野晃一郎×金子勝の未来への対話】 20231011")

C部長 : “逆でしょ？もちろん、AIを正しいアプローチで活用すれば、**分子の「付加価値」が上がることにより、AIで経済が良くなる期待もある**し・・・。”

QEU:FOUNDER ： “この爺さんは（ふつうの）経済学者らしくないからこそ、本当の経済を語る資格があるんだろうね。まあ・・・、「経済学者や経済評論家が経済を（大衆に）語る」のはもうやめたほうがいいんじゃないのか？いい迷惑・・・。”

D先生 ： “それは必要ですよ。ただ、「自称エセ（学者、評論家）」がいけないだけで・・・。“

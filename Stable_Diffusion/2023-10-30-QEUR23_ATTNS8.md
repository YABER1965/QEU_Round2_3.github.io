---
title: QEUR23_ATTNS8: ゲーム2048用の強化学習をやってみた（SOART-BETA only）
date: 2023-10-30
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_ATTNS8: ゲーム2048用の強化学習をやってみた（SOART-BETA only）

## ～ 「ゲーム向け」のメトリックスじゃないね・・・ ～

QEU:FOUNDER ： “それでは、ひきつづいて2048ゲームをSOARTのメトリ「ックスで強化学習してみましょうか。事前検討では、SOARTメトリックスの感度(β)は畳み込みメトリックスとは大した差がないのではないかという考え方があるが・・・。”


**（その１：ハイ・スコア）**

![imageJRL3-48-1](/2023-10-30-QEUR23_ATTNS8/imageJRL3-48-1.jpg)

**（その２：ロー・スコア）**

![imageJRL3-48-2](/2023-10-30-QEUR23_ATTNS8/imageJRL3-48-2.jpg)

D先生 : “それは、メトリックスの挙動が似ていることからくるインスピレーションですね。ただし、微妙なトレンドに差があるが、それが結局、パフォーマンスにどのような差をもたらすのか・・・。”

QEU:FOUNDER ： “まあ、つまるところ、やってみなければわからないですね。いきなりプログラムの紹介に入りますか。ドン・・・！”

```python
# ----
# 2048ゲームの簡単な強化学習(RT-SCORE BETA only)
# 強化学習の参考HP(PYTORCH OFFICIAL)
# -ttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
# ----
import time
import math
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple, deque, Counter
from IPython.display import clear_output
# ---------------- 
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# ---------------- 
#import matplotlib.pyplot as plt
#%matplotlib inline

# ---------------- 
# set up matplotlib
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display
plt.ion()

# if GPU is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---
Transition = namedtuple('Transition',
                        ('state', 'action', 'next_state', 'reward'))

#=================================================
# SIMPLE CONVOLUTION DATA
#=================================================

省略

#=================================================
# CONVOLUTION FUNCTIONS
#=================================================
# インスタンス化
L1_loss  = torch.nn.L1Loss()
MSE_loss = torch.nn.MSELoss()

# 畳み込み処理を実施する
def apply_kernel(row, col, kernel, mx_tensor):
    return (mx_tensor[row:row+3,col:col+3] * kernel).sum()

# ---------------------------
# soaRT(3 items version)メトリックスを計算する
def calc_soaRT3(L1_loss, MSE_loss, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    btY1_yarray, Y2_yarray, gmY3_yarray = [], [], []

    # 繰り返し
    for iCmx in range(3):

        y = torch.tensor(tsr_sig_matrix[iCmx]).float()
        x = torch.tensor(tsr_tani_array).float()
        #print(y)
        #print(x)

        # 回転を計測(BETA)
        xx = torch.dot(x,x) + 0.0001
        xy = torch.dot(x,y) + 0.0001
        beta = xy/xx
        #print("iCmx:{}, beta:{}".format(iCmx, beta))

        # ユーグリッド距離(GAMMA) 
        gamma = MSE_loss(y, beta*x)
        
        # マンハッタン距離(ITA)
        mDistance   = L1_loss(y, beta*x)
        #print("mDistance: ", mDistance.item())
        
        # 値の対数変換（及び補正）
        log_beta  = math.log(beta.item())
        log_distance = math.log(mDistance.item()+1)
        log_gamma = 0.5*math.log(gamma.item()+1) - log_distance
        
        # 配列を追加する
        btY1_yarray.append(log_beta)
        gmY3_yarray.append(log_gamma)
        Y2_yarray.append(log_distance)

    return btY1_yarray, Y2_yarray, gmY3_yarray

# ---------------------------
# 畳み込みテンソルイメージを生成する
def create_tsrConv(mx_signal):

    # ----
    # パネルスコアを線形化する
    for i in range(4):
        for j in range(4):
            if mx_signal[i][j] > 0:
                mx_signal[i][j] = math.log2(mx_signal[i][j])

    # テンソル化する
    mx_signal = torch.tensor(mx_signal).float()
    #print("--- mx_signal ---")
    #print(mx_signal)

    # ----
    # 単位空間の畳み込み
    tani_image = torch.tensor([[apply_kernel(i,j, tani_kernel, mx_signal) for j in range(2)] for i in range(2)])
    tani_array = tani_image.numpy().flatten()
    tani_mean  = np.mean(tani_array) + 0.0001
    #print(tani_array)

    # ----
    # 信号空間（Clockwise）の畳み込みとメトリックス化
    acc_maxcw = np.zeros(3)
    acc_sigcw = np.zeros((3,4))
    #print(acc_sigcc)
    for k in range(3):
        sig_image = torch.tensor([[apply_kernel(i,j, cw_kernels[k], mx_signal) for j in range(2)] for i in range(2)])
        sig_array = sig_image.numpy().flatten()
        sig_max   = np.max(sig_array)
        #print(sig_array)
        acc_sigcw[k,:] = sig_array
        acc_maxcw[k]   = round(math.log(sig_max/tani_mean+1),4)
    #print(acc_sigcw)
    #print(acc_maxcw)

    # soaRT(3 items version)メトリックスを計算する
    btY1_cw, Y2_cw, gmY3_cw = calc_soaRT3(L1_loss, MSE_loss, acc_sigcw, tani_array)
    #print("btY1_cw",btY1_cw)
    #print("Y2_cw",Y2_cw)
    #print("gmY3_cw",gmY3_cw)
    #all_cw = np.array([np.max(btY1_cw),np.max(Y2_cw),np.max(gmY3_cw)])
    all_cw = np.array(btY1_cw)
    
    # ----
    # 信号空間（Counter Clockwise）の畳み込みとメトリックス化
    acc_maxcc = np.zeros(3)
    acc_sigcc = np.zeros((3,4))
    #print(acc_sigcc)
    for k in range(3):
        sig_image = torch.tensor([[apply_kernel(i,j, cc_kernels[k], mx_signal) for j in range(2)] for i in range(2)])
        sig_array = sig_image.numpy().flatten()
        sig_max   = np.max(sig_array)
        #print(sig_array)
        acc_sigcc[k,:] = sig_array
        acc_maxcc[k]   = round(math.log(sig_max/tani_mean+1),4)
    #print(acc_sigcc)
    #print(acc_maxcc)

    # soaRT(3 items version)メトリックスを計算する
    btY1_cc, Y2_cc, gmY3_cc = calc_soaRT3(L1_loss, MSE_loss, acc_sigcc, tani_array)
    #print("btY1_cc",btY1_cc)
    #print("Y2_cc",Y2_cc)
    #print("gmY3_cc",gmY3_cc)
    #all_cc = np.array([np.max(btY1_cc),np.max(Y2_cc),np.max(gmY3_cc)])
    all_cc = np.array(btY1_cc)

    # 特徴ベクトルを結合する(Y1のみ)
    array_Y1s = np.append(all_cw,all_cc)
    #print(array_Y1s)

    return array_Y1s

```

C部長 : “畳み込みからRTメトリックスに代って、この部分のみ影響を受けたという事でいいですか？”

QEU:FOUNDER ： “残りは、ほとんど同じですよ、結局のところ・・・。あとはほとんど省略しましょう。”

```python
#=================================================
# PROGRAM INITIALIZATION
#=================================================
# Get number of actions from 2048's action space
n_actions = 4
# Get the number of state observations
n_observations = 6
SLEEP_TIME  = 0.01

# ----
# HYPER PARAMETERS
# BATCH_SIZE は、リプレイ バッファからサンプリングされたトランジションの数です。
# GAMMA は割引係数です。
# EPS_START is the starting value of epsilon
# EPS_END is the final value of epsilon
# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay
# TAU is the update rate of the target network
# LR is the learning rate of the ``AdamW`` optimizer
BATCH_SIZE = 128
GAMMA = 0.92
EPS_START = 0.999
EPS_END = 0.02
EPS_DECAY = 20000
TAU = 0.002
LR = 3e-4

# ---
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())

optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = ReplayMemory(1000)

num_failed = 1   # ボードが動かない(Nomove)
steps_done = 0
episode_durations = []
num_episodes = 1500
max_turn = 800

# --------------------------
# 初期化       
# --------------------------
# ボードを初期化する
board = init_board()

# 学習評価用の配列を初期化する
acc_episode = []
acc_turn = []
acc_score = []
acc_maxY1s = []
acc_loss_epi = []
acc_failed_epi = []
acc_eps = []

#=================================================
# PROGRAM RUN!
#=================================================
# ゲームが終了するまで繰り返す
# メインの処理を実行する
for i_episode in range(num_episodes):

    # ボードをリセットする関数(ただし、ゲームを5ターンだけ、あらかじめ動かしておく)
    board = reset_board(board)
    # ボードを表示する
    print("--- RESETED, GAME NO: {} ---".format(i_episode))
    print_board(board)
    # ----------   
    # 畳み込みテンソルイメージを生成する
    np_board = np.array(board)
    observation = create_tsrConv(np_board)
    #print(observation)
    np_observation = np.array(observation)
    state = torch.tensor(np_observation, dtype=torch.float32, device=device).unsqueeze(0)
    #print("state:",state)

    # 単一ゲーム評価用の配列を初期化する
    arr_turn = []
    arr_reward = []
    arr_score = []
    arr_yzero = []
    arr_loss_game = []
    # ----
    last_nomove = False   # 前回にミス命令が起きたか
    i_turn = 0  # ターン数
    score = 0   # 累積スコア（マイナスな含まない）
    num_failed = 1   # ボードが動かない(Nomove)

    # ゲームを始める
    while True:

        # -----------
        # ゲームの進め方(ACTION)
        #arr_action = ["s", "a", "w", "d"]
        # s - 0 : DOWN
        # a - 1 : LEFT
        # w - 2 : UP
        # d - 3 : RIGHT
        # -----------
        # ゲームを実行する
        action, eps_threshold = select_action(state, last_nomove, num_failed)
        next_board, reward_raw, done = run_game(board, action.item())
        # NO MOVEの処理
        if reward_raw < 0:
            last_nomove = True
            num_failed = num_failed + 1
            score = score
        else:
            last_nomove = False
            score = score + reward_raw
        reward = torch.tensor([reward_raw], device=device)
        #print("state:",state)
        #print("reward:",reward)
        #print("done:",done)

        # ----------   
        # 畳み込みテンソルイメージを生成する
        np_next_board = np.array(next_board)
        next_observation = create_tsrConv(np_next_board)
        #print(observation)

        if done or i_turn > max_turn:
            done = True
            next_state = None
        else:
            np_observation = np.array(next_observation)
            next_state = torch.tensor(np_observation, dtype=torch.float32, device=device).unsqueeze(0)

        # ------
        # DQN experience replay
        # Store the transition in memory
        memory.push(state, action, next_state, reward)

        # Move to the next state
        observation = next_observation
        board = next_board
        state = next_state

        # 最適化の 1 ステップを実行します (ポリシー ネットワーク上で)
        # NO MOVE以外で学習します
        loss = 0
        if (i_turn+1)%20 == 0:      #reward_raw >= 0:
            loss = optimize_model()
            #print("loss:",loss)
            # ターゲットネットワークの重みのソフトアップデート
            # θ′ ← τ θ + (1 −τ )θ′
            target_net_state_dict = target_net.state_dict()
            policy_net_state_dict = policy_net.state_dict()
            for key in policy_net_state_dict:
                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + tar-get_net_state_dict[key]*(1-TAU)
            target_net.load_state_dict(target_net_state_dict)
            # EPSのカウントアップ
            steps_done += 1

        # ------
        # ゲーム実行の結果を出力する
        if (i_turn+1)%100 == 0:
            print("i_episode:{0},i_turn:{1},action:{2},reward:{3},done:{4},score:{5}".format(i_episode,i_turn,action,reward,done,score))
            if i_turn > 100 and (i_turn+1)%200 == 0:
                print_board(board)

        # マトリックス化
        if i_turn == 0:
            mx_cvmax = np.array([observation])
        else:
            mx_cvmax = np.vstack((mx_cvmax,observation)) # 結合してみる

        # ----------   
        # ZERO COUNTER
        state_flatten = np.array(board).flatten()
        cc = Counter(state_flatten)
        arr_yzero.append(cc[0]) 

        # 配列へ要素を追加する
        arr_turn.append(i_turn)
        arr_reward.append(reward_raw)
        arr_score.append(score)
        arr_loss_game.append(loss)

        # ボードに動きがなければ、ゲームオーバーとする
        if done == True or i_turn > max_turn:
            print("Game over")
            # -----
            # ゲーム結果のグラフ化
            episode_durations.append(i_turn + 1)
            #plot_durations()

            # ----
            # METRICS
            acc_Y1s = []
            mask_turn = np.min([80,i_turn-20])
            acc_Y1s.append(np.max(mx_cvmax[mask_turn:,0]))
            acc_Y1s.append(np.max(mx_cvmax[mask_turn:,1]))
            acc_Y1s.append(np.max(mx_cvmax[mask_turn:,2]))
            acc_Y1s.append(np.max(mx_cvmax[mask_turn:,3]))
            acc_Y1s.append(np.max(mx_cvmax[mask_turn:,4]))
            acc_Y1s.append(np.max(mx_cvmax[mask_turn:,5]))

            # -----
            # 学習評価用の配列を初期化する
            acc_episode.append(i_episode)
            acc_turn.append(i_turn)
            acc_score.append(score)
            acc_maxY1s.append(np.max(acc_Y1s))
            acc_failed_epi.append(round(num_failed/(i_turn+1),5))
            acc_eps.append(eps_threshold)
            if None in arr_loss_game:
                print("None is present in the list")
                acc_loss_epi.append(10)
            else:
                acc_loss_epi.append(np.min(arr_loss_game))
            
            # -----
            # マトリックス化
            if i_episode == 0:
                mx_accY1s = np.array([acc_Y1s])
            else:
                mx_accY1s = np.vstack((mx_accY1s,acc_Y1s)) # 結合してみる
            break
        else:
            i_turn = i_turn + 1

    # ----------
    # しばらくすれば表示が消えます
    if i_episode%50 == 0:
        time.sleep(SLEEP_TIME)
        clear_output(wait=True)

# -----
# 評価結果のグラフ化
print('Complete')
plot_durations(show_result=True)
plt.ioff()
plt.show()

```

QEU:FOUNDER ： “分析用のグラフを見てみましょう。”


**（今回の学習結果：RT-BETAでの学習-1）**

![imageJRL3-48-3](/2023-10-30-QEUR23_ATTNS8/imageJRL3-48-3.jpg)

**（今回の学習結果：RT-BETAでの学習-2）**

![imageJRL3-48-4](/2023-10-30-QEUR23_ATTNS8/imageJRL3-48-4.jpg)

C部長 : “スコアは学習とともに、少し下がっているような・・・。どう思います？D先生・・・。”

D先生： “失敗率も見ないとね・・・。過去の結果と見比べないと・・・。”

**（以前の学習結果：RAWデータでの学習）**

![imageJRL3-48-5](/2023-10-30-QEUR23_ATTNS8/imageJRL3-48-5.jpg)

**（以前の学習結果：CONVメトリックスでの学習）**

![imageJRL3-48-6](/2023-10-30-QEUR23_ATTNS8/imageJRL3-48-6.jpg)

D先生 ： “「命令ミス発生率」のグラフの様子は、生データ学習と畳み込みメトリックス学習のちょうど中間の感じですよね。全体に、（これは）いいとも悪いともいえん・・・。”

QEU:FOUNDER ： “感度というメトリックスはバランス（スコア vs 命令ミス）のレベルは高いのだが、スコアを上げる能力はそれほどないのじゃないでしょうか。”

D先生 ： “そういう意味では、**これはゲーム用のメトリックスではなく、画像認識用ですね**。”

QEU:FOUNDER ： “ガンマ値はSN比（距離）から派生したモノであるわけです。じゃあ、SN比でやって比較してみないとね。”

C部長 ： “これも面白そうですね。”

QEU:FOUNDER ： “そういえば、ガンマ値とSN比の独立性を、まだ検証していなかったわい・・・（笑）。もちろん、ガンマ値とSN比は完全に独立であるわけがないがね・・・。じゃあ、次にいきましょう。”

D先生 ： “次回のSOART3メトリックス（感度 プラス SN比）につづく。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “Ｃ部長、ＡＩで企業内の人が減ってきたら、経済は成長するの？”

C部長 : “ふつう、その逆でしょ？もちろん、正しいアプローチでやれば、製品の「付加価値」が上がることにより、AIで経済が良くなる期待もあるし・・・。”

[![MOVIE1](http://img.youtube.com/vi/SUJsvJJXJh8/0.jpg)](http://www.youtube.com/watch?v=SUJsvJJXJh8 "Before生成AI ＆ After 生成AI【児玉龍彦×辻野晃一郎×金子勝の未来への対話】 20231011")

QEU:FOUNDER ： “この爺さんは（ふつうの）経済学者らしくないからこそ、本当の経済を語る資格があるんだろうね。まあ・・・、「経済学者や経済評論家が経済を（大衆に）語る」のはもうやめたほうがいいんじゃないのか？”

D先生 ： “それは必要ですよ。ただ、**「エセ（学者、評論家）」がいけない**だけで・・・。“

QEU:FOUNDER(設定年齢65歳) ： “そうじゃない・・・。当たり前のことを言うけど、「経済学者には経済をよくするためのノウハウはないよ」ってこと。別に、そんなことを期待して生まれた学問でもないし・・・。”

[![MOVIE2](http://img.youtube.com/vi/FCc8cojW3ZU/0.jpg)](http://www.youtube.com/watch?v=FCc8cojW3ZU "田内学×宮台真司：人を幸せにする経済とは")

QEU:FOUNDER ： “コレを見れば、「経済とは何か？」がわかっちゃうんですよね。もちろん、普通の人が知るべき一般的なレベルでね。”

D先生 ： “これを踏まえて、自称経済専門家の話を聞くと、似非をうのみをすることもない。人間、出会った瞬間の嫌悪感、好き嫌いが大体正しい・・・。 “

### 生産性 = （製品の付加価値）/(製品を作るための資源の投入金額)

QEU:FOUNDER ： “こんな、〇カな議論（↑）なんか忘れて結構！だって、創造的なモノを作る過程は**「ムダ（生産性が低い）」**だよ。そもそも・・・。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）： 「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

D先生 ： “いま気が付きました。これらの発言（↑）って、**「(分母の)生産性が高い」**ですね・・・。“

![imageJRL3-48-7](/2023-10-30-QEUR23_ATTNS8/imageJRL3-48-7.jpg)

QEU:FOUNDER ： “これらの呪文を会社の中にマントラのように唱えて、社会で徹底すれば**「J国スゴイ」が維持できる**と思っていたんだよね。”


---
title: QEUR23_SNLPS13:　簡単なNLP「続編」 (yelp review - その4) 
date: 2023-05-08
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_SNLPS13:　簡単なNLP「続編」 (yelp review - その4) 

## ～　惜しい・・・。データと「なにか」が足らない・・・。　～

QEU:FOUNDER  ： “前回はT法(2)のメトリックスを出力しました。そのメトリックスをPCA分解することにより、そこそこネガティブとポジティブ傾向の単語を判別できることがわかりました。いよいよ、本丸のディープラーニングに入りたいところですが・・・。”

**（T法(2)メトリックスの散布図）**

![imageJRL3-33-1](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-1.jpg)

C部長  ： “ん？ところですが・・・？”

QEU:FOUNDER  ： “しかし、思うところがあって、**メトリックスをT法(1)にきりかえました**。前回のプログラムの変更箇所を示します。”

```python
# ---------------------------
# T2メトリックス計算の関数定義
def calc_metricsT2m(len_signal, num_vocab, mx_Xnorm, arr_Ynorm):
    # -----
    # 感度とSN比
    arr_beta    = np.zeros(num_vocab)
    arr_snr     = np.zeros(num_vocab) 
    # -----
    # 有効除数の計算
    yuko = 0.0
    for jCol in range(len_signal):
        yuko += float(arr_Ynorm[jCol]) ** 2
    #print('有効除数=:{0:.4f}'.format(yuko))
    # -----
    if yuko > 0.0:
        for iRow in range(num_vocab):
            # -----
            # 線形式の計算
            sum_array = 0.0
            for jCol in range(len_signal):
                sum_array += float(arr_Ynorm[jCol]) * mx_Xnorm[jCol, iRow]
                #print('配列ix={0},jy={1}の合計値{2}'.format(iRow,jCol,sum_array))
            lnr_yvalue = sum_array
            beta_yvalue = sum_array / float(yuko)
            # -----
            # 全変動ST及び各種中間指標SB,SE,VE,η
            sum_array = 0.0
            for jCol in range(len_signal):
                sum_array += float(mx_Xnorm[jCol, iRow]) ** 2
                # print('配列i={0},jy={1}の合計値{2}'.format(iRow,iRow,sum_array))
            st_yvalue = sum_array
            sb_yvalue = lnr_yvalue ** 2 / float(yuko)
            se_yvalue = st_yvalue - sb_yvalue
            # -----
            # T法(1)用: Ve
            ve_yvalue = se_yvalue / float(num_vocab - 1.0)
            # -----
            # リストに代入する
            arr_beta[iRow]  = round(beta_yvalue, 6)
            # T法(1)
            if sb_yvalue - ve_yvalue > 0.0001 and se_yvalue > 0.0001:
                val_snr       = math.log((sb_yvalue - ve_yvalue) / ve_yvalue)
                if val_snr < 0:
                    val_snr == 0
                arr_snr[iRow] = round(val_snr, 6)
            else:
                arr_snr[iRow] = 0
            # T法(2)
            #if se_yvalue > 0.0001:
            #    arr_snr[iRow]   = round(sb_yvalue / se_yvalue, 6)
            #else:
            #    arr_snr[iRow]   = 0
                
        return arr_beta, arr_snr
    else:
        # -----
        # 異常処理
        return [0], [0]
```

C部長  ： “T法(1)のメトリックスだったら、ところどころに**マイナス値**が出ちゃうでしょ？そうすると、その後の計算で不都合がでちゃうでしょ？”

QEU:FOUNDER ： “しょうがないので、**SN比計算における分子成分(sb-ve)が十分に小さければ、SN比の値を強制的に0にしています**。SN比は大きな絶対値でなければ、基本、その単語は重要性がないので(stopword)・・・。あと、SN比は**対数変換をしています**。T法(1)の場合、値は1000以上大きな値になることもあります。それでは、この新メトリックスを使うと、どのような分布が得られるかをみてみましょう”

**（T法(2)メトリックスの散布図）**

![imageJRL3-33-2](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-2.jpg)

C部長  ： “ずいぶん、分布の様子がかわりましたねえ・・・。個人的には、こちらの方が分類がすっきりしているように思います。”

QEU:FOUNDER  ： “それでは、コレを使ってDLプログラムを動かしましょう。初めは、単純に文をメトリックスに変換して、それを当てはめる方法です。”

```python
# -------------------
# Practical Text Classification With Python and Keras
# Embeddingの代わりに、T法(1)メトリックスを使ってみる。
# -------------------
filepath = './yelp_labelled.txt'
df_yelp = pd.read_csv(filepath, names=['sentence', 'label'], sep='\t')
print(df_yelp)

# ------
# scikit-learnライブラリが提供するCountVectorizerを利用して文のベクトル化を行うことができます。
# これは各文章の単語を取り込み、文中のすべてのユニークな単語の語彙を作成します。
# この語彙を利用して単語の数を表す特徴ベクトルを作成することができます
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

# 例文の処理
sentences = ['John likes ice cream', 'John hates chocolate.']
vectorizer = CountVectorizer()
vectorizer.fit(sentences)
# ------
sentDic = vectorizer.vocabulary_
print(sentDic)
print("------")
print('john' in sentDic)  # 小文字
# True
print('jeff' in sentDic)  # 小文字
# False

# ------
# テキスト文データのスプリット(train-test)
sentences = df_yelp['sentence'].values
y = df_yelp['label'].values
sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, ran-dom_state=1000)
print(sentences_train[0:10])
print(y_train[0:10])

# ------
# パラメタの準備
arr_words  = df_voc.loc[:,"key"].values
#print(arr_words)
num_vocab  = len(arr_words)
arr_idxs   = list(range(num_vocab))
num_train  = len(sentences_train)
num_test   = len(sentences_test)

# ------
# BoWとの一致情報のみを学習データとして集約する
# ------
def vocab_reduction(num_data, sentences_data, arr_labels):
    arr_reduced_labels = []
    arr_reduced_sentences = []
    arr_reduced_nums = []
    # -----
    for i in range(num_data):     # num_train
        sentences  = sentences_data[i]
        vectorizer = CountVectorizer()
        vectorizer.fit([sentences])
        sentDic = vectorizer.vocabulary_
        # index:一致確認
        arr_matched_words = []
        arr_matched_idxs  = []
        for j in range(num_vocab):
            if arr_words[j] in sentDic: 
                arr_matched_words.append(arr_words[j])
                arr_matched_idxs.append(arr_idxs[j])
        # 4以上の一致ができた文だけを登録する
        if len(arr_matched_idxs) > 3:
            arr_reduced_labels.append(arr_labels[i])
            arr_reduced_sentences.append(arr_matched_idxs)
            arr_reduced_nums.append(len(arr_matched_idxs))
            #print("i: {},matched: {}".format(i,arr_matched_words))

    return arr_reduced_labels, arr_reduced_sentences, arr_reduced_nums
# -----
# Trainの場合
train_labels, train_sentences, train_nums = vocab_reduction(num_train, sentences_train, y_train)
print("------")
print(train_labels[0:10])
#print("------")
#print(train_sentences)
print("------")
print(train_nums[0:10])

# -----
# Testの場合
test_labels, test_sentences, test_nums = vocab_reduction(num_test, sentences_test, y_test)
print("------")
print(test_labels[0:10])
#print("------")
#print(test_sentences)
print("------")
print(test_nums[0:10])

# ------
# 説明変数マトリックス(Xs)
# 最大長さを２００とする(Padding)
# ------
max_train = len(train_labels)
max_test  = len(test_labels)
max_len   = 200
mx_train  = np.zeros([max_train, max_len])
mx_test   = np.zeros([max_test, max_len])
print("max_train: ",max_train)
print("max_test: ",max_test)

```

![imageJRL3-33-3](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-3.jpg)

C部長  ： “あれ？**学習するデータの数がこんなに少ない**んですか？たしかに、もともとのデータ数が1000件だけなんで・・・。”

QEU:FOUNDER  ： “今回の場合、解析しにくい短い文を消しました。”

C部長  ： “モデルを持たないディープラーニングでは、かなりきつい数じゃないかなぁ・・・。”

QEU:FOUNDER  ： “まあ、とりあえずやってみましょう。”

```python
# ------
# Create Matrix(1) - Padding方案（長さ２００）
# ------
def locate_matrix_no1(mx_padding, padding_sentences, padding_nums):
	# ---
    for i in range(len(padding_nums)):
        # ---
        temp_array = np.array([])
        sentence = padding_sentences[i]
        for j in range(len(sentence)):
            temp_array = np.append(temp_array, mx_snr[sentence[j],:])
        #print(len(temp_array))
        #print("-----")
        #print(temp_array)
        for k in range(len(temp_array)):
            if k > max_len -1:
                break
            else:
                mx_padding[i, k] = temp_array[k]
        #print("-----")
        #print(mx_padding[i, 0:100])
    # -----
    #print("-----")
    #print(mx_padding[0:10, 0:10])

    return mx_padding
# ------
# TRAIN
mx_train = locate_matrix_no1(mx_train, train_sentences, train_nums)
print("-----")
print(mx_train[0:10, 0:10])
# ------
# TEST
mx_test  = locate_matrix_no1(mx_test, test_sentences, test_nums)
print("-----")
print(mx_test[0:10, 0:10])

# ------
# [方案1]変数をディープラーニング用変数(X,y)に引き渡し
X_train  = mx_train
y_train  = np.array(train_labels)
X_test   = mx_test
y_test   = np.array(test_labels)

# ------
# [方案1]簡単なDeepLearning当てはめで解いてみる
from keras.models import Sequential
from keras import layers

input_dim = X_train.shape[1]  # Number of features(vocab)

model = Sequential()
model.add(layers.Dense(40, input_dim=input_dim, activation='relu'))
#model.add(layers.Dense(30, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])
model.summary()

```

![imageJRL3-33-4](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-4.jpg)

```python
# ------
# [方案1].fit()関数でトレーニングを開始しましょう。
history = model.fit(X_train, y_train, epochs=60, verbose=True,
                    validation_data=(X_test, y_test), batch_size=20)

# ------
# [方案1].evaluate()メソッドを使用して、モデルの精度を測定することができます。
# これはトレーニングデータとテストデータの両方に対して行うことができます。訓練データの方がテストデータよりも精度が高いことが予想されます。
# ニューラルネットワークを長く訓練すればするほどオーバーフィッティングが始まる可能性が高くなります。
loss, accuracy = model.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))
#Training Accuracy: 1.0000
#Testing Accuracy:  0.7884

# ------
# [方案1]学習特性のグラフ化（性能評価）
import matplotlib.pyplot as plt
plt.style.use('ggplot')

# loss	accuracy	val_loss	val_accuracy
def plot_history(history):
    acc = history.history['accuracy']
    eval_acc = history.history['val_accuracy']
    loss = history.history['loss']
    eval_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)
    # -----
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='train_accuracy')
    plt.plot(x, eval_acc, 'r', label='test_accuracy')
    plt.title('accuracy trend')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='train_loss')
    plt.plot(x, eval_loss, 'r', label='test_loss')
    plt.title('loss trend')
    plt.legend()

plot_history(history)
```

![imageJRL3-33-5](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-5.jpg)


**（参考：メトリックスを使わない、前回のEmbedding学習結果）**

![imageJRL3-33-6](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-6.jpg)

D先生 ： “通常のEmbeddingをやっている場合とあまり特性は変わらないです。”

QEU:FOUNDER  ： “逆にいうと、**T法のメトリックスはEmbeddingと同様のパフォーマンスを持っている**といえますね。ただし、EmbeddingのLatent_Factorの数は50次元であり、今回のメトリックスは10次元なので、モデルの当てはめ能力が若干弱いとは言えます。”

D先生 ： “前回のEmbeddingの実験では**50次元へのmax変換**を行いました。今回も同じで行きますか？”

QEU:FOUNDER  ： “我々は**min-max変換**で行きます。Minを10次元、Maxを10次元で、合計20次元というアンバイ・・・。Embeddingは0-1の値をとるのでMAXだけで十分意味を持ちますが、今回は**マイナス値もとっています**から・・・。あと、さすがに今回の10次元のベクトルは少なすぎたよね。”

```python
# ------
# Create Matrix(2) - Compile方案(min-max, plus mean)
# ------
def calc_minmax(mx_emb):
    arr_minmax = np.zeros(20)
    arr_all    = np.zeros(30)
    for i in range(10):
        arr_minmax[i]    = np.min(mx_emb[:,i])
        arr_minmax[i+10] = np.max(mx_emb[:,i])
        # -----
        arr_all[i]    = np.min(mx_emb[:,i])
        arr_all[i+10] = np.max(mx_emb[:,i])
        arr_all[i+20] = np.mean(mx_emb[:,i])
        
    return arr_minmax, arr_all

def locate_matrix_no2(compile_sentences, compile_nums):
	# ---
    for i in range(len(compile_nums)):      # len(compile_nums)
        # ---
        sentence = compile_sentences[i]
        #print(sentence)
        for j in range(len(sentence)):
            if j == 0: 
                mx_temp = [mx_snr[sentence[j],:]]
            else:
                mx_temp = np.concatenate([mx_temp, [mx_snr[sentence[j],:]]], axis = 0)
        # min-max配列をつくる
        arr_minmax, arr_all = calc_minmax(mx_temp)
        #print(arr_minmax)
        # ---
        if i == 0: 
            mx_compile  = [arr_minmax]
            mx2_compile = [arr_all]
        else:
            mx_compile  = np.concatenate([mx_compile, [arr_minmax]], axis = 0)
            mx2_compile = np.concatenate([mx2_compile, [arr_all]], axis = 0)

    return mx_compile, mx2_compile
# ------
# TRAIN
mx2_train, mx3_train = locate_matrix_no2(train_sentences, train_nums)
print("-----")
print(mx2_train[0:10, 0:10])
# ------
# TEST
mx2_test, mx3_test  = locate_matrix_no2(test_sentences, test_nums)
print("-----")
print(mx2_test[0:10, 0:10])

# ------
# [方案2]変数をディープラーニング用変数(X,y)に引き渡し
X_train  = mx2_train
y_train  = np.array(train_labels)
X_test   = mx2_test
y_test   = np.array(test_labels)

# ------
# [方案2]簡単なDeepLearning当てはめで解いてみる
input_dim = X_train.shape[1]  # Number of features(vocab)

model = Sequential()
model.add(layers.Dense(30, input_dim=input_dim, activation='relu'))
#model.add(layers.Dense(30, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])
model.summary()

# ------
# [方案2].fit()関数でトレーニングを開始しましょう。
history = model.fit(X_train, y_train, epochs=100, verbose=True,
                    validation_data=(X_test, y_test), batch_size=20)

# ------
# [方案2].evaluate()メソッドを使用して、モデルの精度を測定することができます。
# これはトレーニングデータとテストデータの両方に対して行うことができます。訓練データの方がテストデータよりも精度が高いことが予想されます。
# ニューラルネットワークを長く訓練すればするほどオーバーフィッティングが始まる可能性が高くなります。
loss, accuracy = model.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))
#Training Accuracy: 0.8139
#Testing Accuracy:  0.7778

# ------
# [方案2]学習特性のグラフ化（性能評価）
plot_history(history)

```

![imageJRL3-33-7](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-7.jpg)

D先生 ： “最終的なパフォーマンス（検証精度）はあまり変わりません。しかし、学習曲線の様子が全然変わりましたね。**オーバーフィッティング（検証ロスの増加）**の傾向が消えました。”

QEU:FOUNDER  ： “このやり方の方が、よりよいインプットだからでしょう。ただし、**Train精度が頭打ちになっている**ので、そもそものインプット情報量（ベクトル次元数）が足らないようです。”

D先生 ： “じゃあ、思い切って**平均値の情報を入れます**か？そうすれば、合計で30次元になります。”

QEU:FOUNDER  ： “「ダメでもともと」で、やってみましょう。”

```python
# ------
# [方案3]全(all)変数をディープラーニング用変数(X,y)に引き渡し
X_train  = mx3_train
y_train  = np.array(train_labels)
X_test   = mx3_test
y_test   = np.array(test_labels)

# ------
# [方案3]簡単なDeepLearning当てはめで解いてみる
input_dim = X_train.shape[1]  # Number of features(vocab)

model = Sequential()
model.add(layers.Dense(30, input_dim=input_dim, activation='relu'))
#model.add(layers.Dense(30, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])
model.summary()

# ------
# [方案3].fit()関数でトレーニングを開始しましょう。
history = model.fit(X_train, y_train, epochs=100, verbose=True,
                    validation_data=(X_test, y_test), batch_size=20)

# ------
# [方案3].evaluate()メソッドを使用して、モデルの精度を測定することができます。
# これはトレーニングデータとテストデータの両方に対して行うことができます。訓練データの方がテストデータよりも精度が高いことが予想されます。
# ニューラルネットワークを長く訓練すればするほどオーバーフィッティングが始まる可能性が高くなります。
loss, accuracy = model.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))
#Training Accuracy: 0.8939
#Testing Accuracy:  0.8413

# ------
# [方案3]学習特性のグラフ化（性能評価）
plot_history(history)

```

![imageJRL3-33-8](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-8.jpg)

QEU:FOUNDER  ： “予想通りの展開で、うまくいきました。**最終精度は84％程度**です。残念ながら、目標の85％には届かなかった。”

D先生 ： “今回は単位空間を10個しか持ちませんでしたが、もし20次元であればうまく行きそうです。”

QEU:FOUNDER  ： “あとは、学習するデータももう少し欲しい。そもそも、ディープラーニングで1000件の学習データはきついって・・・。最低でも2000件あれば、90％を超える精度は出そうです。”

D先生 ： “う～ん・・・。今回は**記念的なイベントになりました**。外観検査自動機のプロジェクトでCNNの代替としてRT法が使えることをすでに証明しています。今回は、**Embeddingの代替としてT法が使えることがわかった**わけです。”

![imageJRL3-33-9](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-9.jpg)

QEU:FOUNDER  ： “率直、今後ますますディープラーニングを使ったAIが存在感を高めることは自明です。そして、必然的に**タグチメソッドは「昔の名前で・・・」になっていく**でしょう。それでも、我々の試みを通じて、次世代に何らかの遺伝子を残せるとうれしい・・・。”

D先生 ： “大昔に動物に寄生した小動物がミトコンドリアになったように・・・。”

QEU:FOUNDER  ： “メソッドの威勢のいいときは揉み手・擦り手で近づき、そうでなければケツをまくる。**そんな人々が多い**中で、最後まで頑張って結果を残したね。我々こそ、正統なる「シン・タグチ愛好者」と言えるでしょう。是非、カンパをお願いします！！”

[＞寄付のお願い(click here)＜](
https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-created&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038
)
 
D先生 ： “今回のシリーズは、そもそも「Collaboration Filteringの再考」でした。やっと取り組めますね。”


## ～　まとめ　～

### ・・・　今年は大変な年になりそうです（のつづき）　・・・

QEU:FOUNDER ： “ポストGPT本命の**LLaMA**は、さらに進化しているようです。新しい派生システムもできてきているようです。”

[![MOVIE1](http://img.youtube.com/vi/QVC24JhQZS8/0.jpg)](http://www.youtube.com/watch?v=QVC24JhQZS8 "Is Vicuna the ChatGPT Killer?")

C部長 : “もう、この業界の動きがはやく、複雑すぎて、なにがなんやら・・・。”

QEU:FOUNDER ： “そのシステム、性能はとりあえずいいらしいよ。”

![imageJRL3-33-10](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-10.jpg)

C部長 : “CharGPT4との性能比較ですね。これならば、かなりイケそうな気がします。”

QEU:FOUNDER ： “コストも比較しているようです。”

![imageJRL3-33-11](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-11.jpg)

C部長 : “まあ、「これほどのことをする」には安いとはいえるけど、なんのコスト？”

QEU:FOUNDER ： “小生にも、現在のところわかりません。たぶん、学習にかける電気エネルギーじゃないかな？さらには、その派生形も出てきたりして・・・。”

![imageJRL3-33-12](/2023-05-10-QEUR23_SNLPS13/imageJRL3-33-12.jpg)

C部長 : “いったい、何が何やら・・・。”

D先生： “一つ確実なコト。もうすぐ誰でも、**ちょっとお金を掛ければGPT相当のものを作れるようになる**んですよ。一方、Prompt Engineeringについてもかなり整理されているようです。今後の問題は**アレに「何を学習させるか」*ですね。”

QEU:FOUNDER ： “GPTを何のコンサルタントにさせたいかですね。LLMのシステムの発展は今後も続くでしょう。でも、それを教えるコンテンツの問題は全く別です。すごく面白いテーマので、やってみたいね。”


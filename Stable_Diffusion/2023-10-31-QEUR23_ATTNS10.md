---
title: QEUR23_ATTNS10: ゲーム2048用の強化学習をやってみた（SOART with ITA）
date: 2023-10-31
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_ATTNS10: ゲーム2048用の強化学習をやってみた（SOART with ITA）

## ～ ITAは「横綱」の貫禄！(かな？) ～

C部長 : “それでは、この強化学習も最終コーナです。ＲＴメトリックス群のうち、ＳＮ比（ITA：η）を使いましょう。もちろん、プログラムを見ればわかりますが、ＲＴ法のＳＮ比のメトリックスは「距離」で代用されています。そして、この2048ゲームで、どこまで強化学習でスコアを上げられるのかをテストしています。”

![imageJRL3-50-1](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-1.jpg)

QEU:FOUNDER ： “今回は、我々が考案したＳＯＡＲＴメトリックスを使っています。ものすごく多くの部分がオリジナルから変わっています。とくに変わったのが、**SOARTは「3つのメトリックス」を持っている**ことです。ここで、Ｙ１は感度（β）、Ｙ２はＳＮ比（η）があります、そして、ここに我々は新しいγメトリックスを追加していったわけです。”

C部長 : “そして、この**Ｙ３メトリックス（γ）はＹ２メトリックス（η）に近い特性**を持っています。”

QEU:FOUNDER ： “ですから、今回のポイントはＹ３とＹ２は独立しているのか？また、Ｙ３（γ）はＹ２（η）と比較して、どれだけの表現力があるかを調査します。それでは、プログラムの紹介に入りますか。ドン・・・！”

```python
# ----
# 2048ゲームの簡単な強化学習(RT-SCORE, btY1 and Y2)
# 強化学習の参考HP(PYTORCH OFFICIAL)
# -ttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
# ----
import time
import math
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple, deque, Counter
from IPython.display import clear_output
# ---------------- 
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# ---------------- 
#import matplotlib.pyplot as plt
#%matplotlib inline

省略

# ---------------------------
# soaRT(3 items version)メトリックスを計算する
def calc_soaRT3(L1_loss, MSE_loss, tsr_sig_matrix, tsr_tani_array): 

    # ---------------------------
    # 変数の初期化
    btY1_yarray, Y2_yarray, gmY3_yarray = [], [], []

    # 繰り返し
    for iCmx in range(3):

        y = torch.tensor(tsr_sig_matrix[iCmx]).float()
        x = torch.tensor(tsr_tani_array).float()
        #print(y)
        #print(x)

        # 回転を計測(BETA)
        xx = torch.dot(x,x) + 0.0001
        xy = torch.dot(x,y) + 0.0001
        beta = xy/xx
        #print("iCmx:{}, beta:{}".format(iCmx, beta))

        # ユーグリッド距離(GAMMA) 
        gamma = MSE_loss(y, beta*x)
        
        # マンハッタン距離(ITA)
        mDistance   = L1_loss(y, beta*x)
        #print("mDistance: ", mDistance.item())
        
        # 値の対数変換（及び補正）
        log_beta  = math.log(beta.item())
        log_distance = math.log(mDistance.item()+1)
        log_gamma = 0.5*math.log(gamma.item()+1) - log_distance
        
        # 配列を追加する
        btY1_yarray.append(log_beta)
        gmY3_yarray.append(log_gamma)
        Y2_yarray.append(log_distance)

    return btY1_yarray, Y2_yarray, gmY3_yarray

# ---------------------------
# 畳み込みテンソルイメージを生成する
def create_tsrConv(mx_signal):

    # ----
    # パネルスコアを線形化する
    for i in range(4):
        for j in range(4):
            if mx_signal[i][j] > 0:
                mx_signal[i][j] = math.log2(mx_signal[i][j])

    # テンソル化する
    mx_signal = torch.tensor(mx_signal).float()
    #print("--- mx_signal ---")
    #print(mx_signal)

    # ----
    # 単位空間の畳み込み
    tani_image = torch.tensor([[apply_kernel(i,j, tani_kernel, mx_signal) for j in range(2)] for i in range(2)])
    tani_array = tani_image.numpy().flatten()
    tani_mean  = np.mean(tani_array) + 0.0001
    #print(tani_array)

    # ----
    # 信号空間（Clockwise）の畳み込みとメトリックス化
    acc_maxcw = np.zeros(3)
    acc_sigcw = np.zeros((3,4))
    #print(acc_sigcc)
    for k in range(3):
        sig_image = torch.tensor([[apply_kernel(i,j, cw_kernels[k], mx_signal) for j in range(2)] for i in range(2)])
        sig_array = sig_image.numpy().flatten()
        sig_max   = np.max(sig_array)
        #print(sig_array)
        acc_sigcw[k,:] = sig_array
        acc_maxcw[k]   = round(math.log(sig_max/tani_mean+1),4)
    #print(acc_sigcw)
    #print(acc_maxcw)

    # soaRT(3 items version)メトリックスを計算する
    btY1_cw, Y2_cw, gmY3_cw = calc_soaRT3(L1_loss, MSE_loss, acc_sigcw, tani_array)
    #print("btY1_cw",btY1_cw)
    #print("Y2_cw",Y2_cw)
    #print("gmY3_cw",gmY3_cw)
    #all_cw = np.array([np.max(btY1_cw),np.max(Y2_cw),np.max(gmY3_cw)])
    all_cwY1 = np.array(btY1_cw)
    all_cwY2 = np.array(Y2_cw)
    all_cwY3 = np.array(gmY3_cw)
    list_cw  = [all_cwY1, all_cwY2, all_cwY3]
    
    # ----
    # 信号空間（Counter Clockwise）の畳み込みとメトリックス化
    acc_maxcc = np.zeros(3)
    acc_sigcc = np.zeros((3,4))
    #print(acc_sigcc)
    for k in range(3):
        sig_image = torch.tensor([[apply_kernel(i,j, cc_kernels[k], mx_signal) for j in range(2)] for i in range(2)])
        sig_array = sig_image.numpy().flatten()
        sig_max   = np.max(sig_array)
        #print(sig_array)
        acc_sigcc[k,:] = sig_array
        acc_maxcc[k]   = round(math.log(sig_max/tani_mean+1),4)
    #print(acc_sigcc)
    #print(acc_maxcc)

    # soaRT(3 items version)メトリックスを計算する
    btY1_cc, Y2_cc, gmY3_cc = calc_soaRT3(L1_loss, MSE_loss, acc_sigcc, tani_array)
    #print("btY1_cc",btY1_cc)
    #print("Y2_cc",Y2_cc)
    #print("gmY3_cc",gmY3_cc)
    #all_cc = np.array([np.max(btY1_cc),np.max(Y2_cc),np.max(gmY3_cc)])
    all_ccY1 = np.array(btY1_cc)
    all_ccY2 = np.array(Y2_cc)
    all_ccY3 = np.array(gmY3_cc)
    list_cc  = [all_ccY1, all_ccY2, all_ccY3]
    
    # 特徴ベクトルを結合する(Y1andY3)
    array_Y1nY2 = np.hstack([all_cwY1,all_cwY2,all_ccY1,all_ccY2])
    #print(array_Y1nY2)

    return array_Y1nY2,list_cw,list_cc

省略

```

QEU:FOUNDER ： “実は、とくに説明することもないよね。今回は、Y1,Y2,Y3すべてのメトリックスを使うために、**「ちょこっと改造」**したぐらいで・・・。”

```python

省略

#=================================================
# PROGRAM INITIALIZATION
#=================================================
# Get number of actions from 2048's action space
n_actions = 4
# Get the number of state observations
n_observations = 12
SLEEP_TIME  = 0.01

# ----
# HYPER PARAMETERS
# BATCH_SIZE は、リプレイ バッファからサンプリングされたトランジションの数です。
# GAMMA は割引係数です。
# EPS_START is the starting value of epsilon
# EPS_END is the final value of epsilon
# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay
# TAU is the update rate of the target network
# LR is the learning rate of the ``AdamW`` optimizer
BATCH_SIZE = 128
GAMMA = 0.92
EPS_START = 0.999
EPS_END = 0.02
EPS_DECAY = 20000
TAU = 0.002
LR = 3e-4

# ---
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())

optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = ReplayMemory(1000)

num_failed = 1   # ボードが動かない(Nomove)
steps_done = 0
episode_durations = []
num_episodes = 10000
max_turn = 800

# --------------------------
# 初期化       
# --------------------------
# ボードを初期化する
board = init_board()

# 学習評価用の配列を初期化する
acc_episode = []
acc_turn = []
acc_score = []
acc_loss_epi = []
acc_failed_epi = []
acc_eps = []
# ---
# metrics
acc_cwY1s = []
acc_cwY2s = []
acc_cwY3s = []
acc_ccY1s = []
acc_ccY2s = []
acc_ccY3s = []
            
#=================================================
# PROGRAM RUN!
#=================================================
# ゲームが終了するまで繰り返す
# メインの処理を実行する
for i_episode in range(num_episodes):

    # ボードをリセットする関数(ただし、ゲームを5ターンだけ、あらかじめ動かしておく)
    board = reset_board(board)
    # ボードを表示する
    print("--- RESETED, GAME NO: {} ---".format(i_episode))
    print_board(board)
    # ----------   
    # 畳み込みテンソルイメージを生成する
    np_board = np.array(board)
    observation,list_cw,list_cc = create_tsrConv(np_board)
    # ----
    all_cwY1 = list_cw[0]
    all_cwY2 = list_cw[1]
    all_cwY3 = list_cw[2]
    all_ccY1 = list_cc[0]
    all_ccY2 = list_cc[1]
    all_ccY3 = list_cc[2]

    # ---
    #print("observation",observation)
    np_observation = np.array(observation)
    state = torch.tensor(np_observation, dtype=torch.float32, device=device).unsqueeze(0)
    #print("state:",state)

    # 単一ゲーム評価用の配列を初期化する
    arr_turn = []
    arr_reward = []
    arr_score = []
    arr_yzero = []
    arr_loss_game = []
    # ----
    last_nomove = False   # 前回にミス命令が起きたか
    i_turn = 0  # ターン数
    score = 0   # 累積スコア（マイナスな含まない）
    num_failed = 1   # ボードが動かない(Nomove)

    # ゲームを始める
    while True:

        # -----------
        # ゲームの進め方(ACTION)
        #arr_action = ["s", "a", "w", "d"]
        # s - 0 : DOWN
        # a - 1 : LEFT
        # w - 2 : UP
        # d - 3 : RIGHT
        # -----------
        # ゲームを実行する
        action, eps_threshold = select_action(state, last_nomove, num_failed)
        next_board, reward_raw, done = run_game(board, action.item())
        # NO MOVEの処理
        if reward_raw < 0:
            last_nomove = True
            num_failed = num_failed + 1
            score = score
        else:
            last_nomove = False
            score = score + reward_raw
        reward = torch.tensor([reward_raw], device=device)
        #print("state:",state)
        #print("reward:",reward)
        #print("done:",done)

        # ----------   
        # 畳み込みテンソルイメージを生成する
        np_next_board = np.array(next_board)
        next_observation,next_list_cw,next_list_cc = create_tsrConv(np_next_board)
        #print(observation)
        # ----
        next_all_cwY1 = next_list_cw[0]
        next_all_cwY2 = next_list_cw[1]
        next_all_cwY3 = next_list_cw[2]
        next_all_ccY1 = next_list_cc[0]
        next_all_ccY2 = next_list_cc[1]
        next_all_ccY3 = next_list_cc[2]

        if done or i_turn > max_turn:
            done = True
            next_state = None
        else:
            np_observation = np.array(next_observation)
            next_state = torch.tensor(np_observation, dtype=torch.float32, device=device).unsqueeze(0)

        # ------
        # DQN experience replay
        # Store the transition in memory
        memory.push(state, action, next_state, reward)

        # Move to the next state
        observation = next_observation
        board = next_board
        state = next_state
        all_cwY1 = next_all_cwY1
        all_cwY2 = next_all_cwY2
        all_cwY3 = next_all_cwY3
        all_ccY1 = next_all_ccY1
        all_ccY2 = next_all_ccY2
        all_ccY3 = next_all_ccY3

        # 最適化の 1 ステップを実行します (ポリシー ネットワーク上で)
        # NO MOVE以外で学習します
        loss = 0
        if (i_turn+1)%20 == 0:      #reward_raw >= 0:
            loss = optimize_model()
            #print("loss:",loss)
            # ターゲットネットワークの重みのソフトアップデート
            # θ′ ← τ θ + (1 −τ )θ′
            target_net_state_dict = target_net.state_dict()
            policy_net_state_dict = policy_net.state_dict()
            for key in policy_net_state_dict:
                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + tar-get_net_state_dict[key]*(1-TAU)
            target_net.load_state_dict(target_net_state_dict)
            # EPSのカウントアップ
            steps_done += 1

        # ------
        # ゲーム実行の結果を出力する
        if (i_turn+1)%100 == 0:
            print("i_episode:{0},i_turn:{1},action:{2},reward:{3},done:{4},score:{5}".format(i_episode,i_turn,action,reward,done,score))
            if i_turn > 100 and (i_turn+1)%200 == 0:
                print_board(board)

        # マトリックス化
        if i_turn == 0:
            mx_cwY1 = np.array([all_cwY1])
            mx_cwY2 = np.array([all_cwY2])
            mx_cwY3 = np.array([all_cwY3])
            mx_ccY1 = np.array([all_ccY1])
            mx_ccY2 = np.array([all_ccY2])
            mx_ccY3 = np.array([all_ccY3])
        else:
            mx_cwY1 = np.vstack((mx_cwY1,all_cwY1)) # 結合してみる
            mx_cwY2 = np.vstack((mx_cwY2,all_cwY2)) # 結合してみる
            mx_cwY3 = np.vstack((mx_cwY3,all_cwY3)) # 結合してみる
            mx_ccY1 = np.vstack((mx_ccY1,all_ccY1)) # 結合してみる
            mx_ccY2 = np.vstack((mx_ccY2,all_ccY2)) # 結合してみる
            mx_ccY3 = np.vstack((mx_ccY3,all_ccY3)) # 結合してみる

        # ----------   
        # ZERO COUNTER
        state_flatten = np.array(board).flatten()
        cc = Counter(state_flatten)
        arr_yzero.append(cc[0]) 

        # 配列へ要素を追加する
        arr_turn.append(i_turn)
        arr_reward.append(reward_raw)
        arr_score.append(score)
        arr_loss_game.append(loss)

        # ボードに動きがなければ、ゲームオーバーとする
        if done == True or i_turn > max_turn:
            print("Game over")
            # -----
            # ゲーム結果のグラフ化
            episode_durations.append(i_turn + 1)
            #plot_durations()

            # ----
            # METRICS
            mask_turn = np.min([80,i_turn-20])
            acc_cwY1s.append(np.max(mx_cwY1[mask_turn:,:]))
            acc_cwY2s.append(np.max(mx_cwY2[mask_turn:,:]))
            acc_cwY3s.append(np.max(mx_cwY3[mask_turn:,:]))
            acc_ccY1s.append(np.max(mx_ccY1[mask_turn:,:]))
            acc_ccY2s.append(np.max(mx_ccY2[mask_turn:,:]))
            acc_ccY3s.append(np.max(mx_ccY3[mask_turn:,:]))

            # -----
            # 学習評価用の配列を初期化する
            acc_episode.append(i_episode)
            acc_turn.append(i_turn)
            acc_score.append(score)
            acc_failed_epi.append(round(num_failed/(i_turn+1),5))
            acc_eps.append(eps_threshold)
            if None in arr_loss_game:
                print("None is present in the list")
                acc_loss_epi.append(10)
            else:
                acc_loss_epi.append(np.min(arr_loss_game))
            break
        else:
            i_turn = i_turn + 1

    # ----------
    # しばらくすれば表示が消えます
    if i_episode%50 == 0:
        time.sleep(SLEEP_TIME)
        clear_output(wait=True)

# -----
# 評価結果のグラフ化
print('Complete')
plot_durations(show_result=True)
plt.ioff()
plt.show()

```

QEU:FOUNDER ： “いままでの慣例で10000回の動的グラフをみてみましょう。”

![imageJRL3-50-2](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-2.jpg)

C部長 : “あんまり、前回と差がないですね。”

QEU:FOUNDER ： “これは、非常に微妙な差異なので、このグラフじゃ判別不可能ですよ。それでは、今回は、最後になるのでグラフ化のプログラムも晒します。このプログラムを改造すれば、なんでもグラフ出力できます。”

```python
# ----------------
# 学習結果のグラフ化(評価用A)
# ----------------
def show_graph_evalA(acc_episode,acc_turn,acc_score,acc_cwY2s,acc_cwY3s,acc_ccY2s,acc_ccY3s):

    fig = plt.figure(figsize=(10, 7))
    # -----
    # ax1
    ax1 = fig.add_subplot(2, 2, 1)
    ax1.set_title('GAME trend : maxTURN')
    ax1.set_xlabel('#GAME-TURN')
    ax1.set_ylabel('TURN')
    ax1.grid(True)
    ax1.plot(acc_episode, acc_turn, color='b')
    # -----
    # ax2
    ax2 = fig.add_subplot(2, 2, 2)
    ax2.set_title('GAME trend : MAXscore')
    ax2.set_xlabel('#GAME-SCORE')
    ax2.set_ylabel('score')
    ax2.grid(True)
    ax2.plot(acc_episode, acc_score, color='b')
    # -----
    # ax3
    ax3 = fig.add_subplot(2, 2, 3)
    ax3.set_title('RELATION SCATTER : score vs cw-ccY2s')
    ax3.set_xlabel('score')
    ax3.set_ylabel('Y1_metrics')
    ax3.grid(True)
    ax3.scatter(acc_score, acc_cwY2s, color='g')
    ax3.scatter(acc_score, acc_ccY2s, color='y')
    # -----
    # ax4
    ax4 = fig.add_subplot(2, 2, 4)
    ax4.set_title('RELATION SCATTER : score vs cw-ccY3s')
    ax4.set_xlabel('score')
    ax4.set_ylabel('Y3_metrics')
    ax4.grid(True)
    ax4.scatter(acc_score, acc_cwY3s, color='g')
    ax4.scatter(acc_score, acc_ccY3s, color='y')
    # -----
    fig.tight_layout()
    #fig.savefig("./AAC_img.png")
    plt.show()

# -----
# 学習結果のグラフ化(評価用A)
show_graph_evalA(acc_episode,acc_turn,acc_score,acc_cwY2s,acc_cwY3s,acc_ccY2s,acc_ccY3s)

```

QEU:FOUNDER ： “出力するグラフは、前回のモノとは見た目を少し変えました。やはり、コレを見ると、ガンマ値よりもＳＮ比の方が情報を多く持っているんじゃないかと思います。”

**（今回の学習結果：SN比:Y2での学習-1）**

![imageJRL3-50-3](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-3.jpg)

C部長 : “ただし、「スコア～メトリックス」散布図を見ると、Y2とY3は、かなり似た傾向をもっていますよね。”

**（今回の学習結果：SN比:Y2での学習-2）**

![imageJRL3-50-4](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-4.jpg)

D先生 ： “今回の強化学習の場合、学習の成功の度合いをもっとも表現できるのは「スコア～命令ミス発生率」の散布図です。ただし、コレを見ると前回（Y3）と今回(Y2)はほとんど同程度のパフォーマンスですね。”


**（前回の学習結果：Y3での学習）

![imageJRL3-50-5](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-5.jpg)

QEU:FOUNDER ： “もう一つのポイントであるＹ２とＹ３メトリックスの相関も、我々が懸念したほど強くはなかった。相関係数でいうと、「r=0.5」ぐらいかな？”

D先生 ： “結論、Ｙ３はそこそこに「使えるメトリックス」だと思います。変数の独立性を要求するマハラノビス距離には使わず、ディープラーニングにインプットするのであればね・・・。”

QEU:FOUNDER ： “よっしゃ！プロジェクト成功！！・・・ということで、次のシリーズに向けての準備をしましょう。”


## ～ まとめ ～

D先生(設定年齢65歳) ： “みなさん、もうお忘れと思いますが、コレ（↓）以外にも・・・。 “

![imageJRL3-50-6](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-6.jpg)

QEU:FOUNDER(設定年齢65歳) ： “こんなこと（↓）もやっています。でも、いつまで、これをやるんでしょうね？”

![imageJRL3-50-7](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-7.jpg)

QEU:FOUNDER ： “あの北方地域の戦争って、普通は冬には終わるもんなんですがね・・・。戦争状態では冬を越せないから。あの、ふつうはね・・・。”

D先生 ： “さらには、宗教のとても偉い人もやめろといっているんですが・・・。 “

![imageJRL3-50-8](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-8.jpg)

QEU:FOUNDER ： “どうなることやら・・・。”

![imageJRL3-50-9](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-9.jpg)

C部長 ： “2つの戦争には、**「いや～な共通点」（↑）**もあるし・・・。“

![imageJRL3-50-10](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-10.jpg)

D先生 ： “さらに、世界が分断されることだけは間違いないでしょうね。で・・・、FOUNDERは、「どちらを」応援しているんですか？“

![imageJRL3-50-11](/2023-10-31-QEUR23_ATTNS10/imageJRL3-50-11.jpg)

QEU:FOUNDER ： “そういえば、最近、食べとらんわ・・・。”

---
title: QEUR23_SNLPS3:　ナイーブ・ベイスによるSentiment Analysis (Yelp_Review、後編)
date: 2023-03-29
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_SNLPS3:　ナイーブ・ベイスによるSentiment Analysis (Yelp_Review、後編)

## ～　わかりやすい。それは一つのメリット　～

D先生 ： “今回は後編、ベイズを使ったSentiment Analysisの性能評価ですね。とくに説明することはあるかな・・・。”

![imageJRL3-13-1](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-1.jpg)

QEU:FOUNDER  ： “とにかく、**前回に紹介したjupyter notebookを読んでください**というしかありません。しいて追加するとすると、b+ΣRの式において、bはポジティブなレコード数とネガティブなレコード数がほぼ等しい場合には0になります。よって、今回の場合は考慮不要です。”

D先生 ： “あとは、特にいうことはありません。プログラムをドン・・・。”

```python
# -----
from fastbook import *
from fastai.text.all import *

# -----
import sklearn.feature_extraction.text as sklearn_text
from IPython.display import display,HTML
import pickle 

df_train = pd.read_csv('./yelp_reviews_org_train.csv')      # reviews_with_splits_lite(new)
df_train.head()

# -----
# N-を計算する(df_negative)
df_negative  = df_bayes_train_first[df_bayes_train_first.label=='negative']
arr_negative = set(df_negative['text'].values)
#print(arr_negative)

# 数値化
N_minus = len(arr_negative)
print(N_minus)

# -----
# N+を計算する(df_positive)
df_positive  = df_bayes_train_first[df_bayes_train_first.label=='positive']
arr_positive = set(df_positive['text'].values)
#print(arr_positive)

# 数値化
N_plus  = len(arr_positive)
print(N_plus)

# -----
# トークン別存在数を計算(positive)
arr_N_positive = [] 
arr_L_positive = [] 
arr_R_positive = [] 
arr_BgOfWd_vocab = df_BgOfWd_first.loc[:,"vocab"].values
for i, val_token in enumerate(arr_BgOfWd_vocab):
    df_temp  = df_positive[df_positive.name==val_token]
    C_plus    = len(df_temp)
    if i < 50:
        print("i: {}, val_token: {}, Cplus: {}".format(i, val_token, C_plus))
    arr_N_positive.append(N_plus+1)
    arr_L_positive.append(C_plus+1)
    # ---
    L_ratio = (C_plus+1)/(N_plus+1)
    arr_R_positive.append(round(L_ratio,5))

# -----
# トークン別存在数を計算(negative)
arr_N_negative = [] 
arr_L_negative = [] 
arr_R_negative = [] 
#arr_BgOfWd_vocab = df_BgOfWd_first.loc[:,"vocab"].values
for i, val_token in enumerate(arr_BgOfWd_vocab):
    df_temp  = df_negative[df_negative.name==val_token]
    C_minus   = len(df_temp)
    if i < 50:
        print("i: {}, val_token: {}, Cminus: {}".format(i, val_token, C_minus))
    arr_N_negative.append(N_minus+1)
    arr_L_negative.append(C_minus+1)
    # ---
    L_ratio = (C_minus+1)/(N_minus+1)
    arr_R_negative.append(round(L_ratio,5))  

# -----
# 各ボキャブラリ別登場数(Bag of Words)をデータフレームにする
# Column情報を追加する
df_BgOfWd_second = df_BgOfWd_first.copy()
len(df_BgOfWd_second)
df_BgOfWd_second["N_plus"]  = arr_N_positive
df_BgOfWd_second["L_plus"]  = arr_L_positive
df_BgOfWd_second["R_plus"]  = arr_R_positive
df_BgOfWd_second["N_minus"] = arr_N_negative
df_BgOfWd_second["L_minus"] = arr_L_negative
df_BgOfWd_second["R_minus"] = arr_R_negative
df_BgOfWd_second.head()

# -----
# 各ボキャブラリの登場数を検索する(Bag of Wordsを生成する)
arr_vocab   = df_BgOfWd_second.loc[:,"vocab"].values
arr_count   = df_BgOfWd_second.loc[:,"count"].values
arr_delete  = ['her-e','when','time','or','would','our','their','can','about','she','?','an','he','been','will','your','ni','ve','us','m','has','did','also','could']

# Bag of Wordsファイルの仕上げと出力
temp_R_positive = df_BgOfWd_second.loc[:, "R_plus"].values
temp_R_negative = df_BgOfWd_second.loc[:, "R_minus"].values
arr_log_ratio   = []
for i in range(len(df_BgOfWd_second)):
    val_log_ratio = math.log(temp_R_positive[i]/temp_R_negative[i])
    arr_log_ratio.append(round(val_log_ratio,5))
    if val_log_ratio > -0.8 and val_log_ratio < 0.8 and arr_count[i] < 100:
        arr_delete.append(arr_vocab[i])
        print("i:{}, vocab:{}, count:{}, log_ratio:{}".format(i,arr_vocab[i],arr_count[i], val_log_ratio))
# -----
df_BgOfWd_second["log_ratio"] = arr_log_ratio
df_BgOfWd_second.head()

# -----
# データの断捨離（第二弾）
# 情報量が少ない語彙は無意味なので削除しましょう(1)
df_BgOfWd_second = df_BgOfWd_second.query('vocab not in @arr_delete')
#print(df_BgOfWd_first[0:5])

# CSVファイルに出力する(Bag Of Words)
df_BgOfWd_second.to_csv("csv_BgOfWd_second.csv")

# -----
# 情報量が少ない語彙は無意味なので削除しましょう(2)
df_bayes_train_second = df_bayes_train_first.copy()
df_bayes_train_second = df_bayes_train_second.query('name not in @arr_delete')
#print(df_bayes_second_first[0:5])

# CSVファイルに出力する(Corpus)
df_bayes_train_second.to_csv("csv_bayes_train_second.csv")
```

D先生 ： “**「データの断捨離」というネーミング**がなんともはや・・・。要は、不要なデータを削除しただけでしょう？”

**(ポジティブ・ランキング)**

![imageJRL3-13-2](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-2.jpg)

**(ネガティブ・ランキング)**

![imageJRL3-13-3](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-3.jpg)

QEU:FOUNDER ： “プログラムのこの部分で、**R値の情報が入ったボキャブラリデータセット**が出力されます。問題は、この値を持つ単語（トークン）はプラスに大きい（positive）ほど、またはマイナスに大きいほど情報量があります。逆にいうと、「R値が0に近い単語をどうするか」ということです。”

D先生 ： “え～っと、FOUNDERのプログラムによると**R値が0付近であり、かつデータ(count)数が少ない単語を削除した**んですね。こういう考え方でいいんですか？”

QEU:FOUNDER  ： “さあ・・・。このロジックの評価は最後の予測段階でわかることです。”

```python
# -----
# POSITIVE文のレコードリスト
temp_search_text     = df_bayes_train_second[df_bayes_train_second.label=='positive'].loc[:,'text'].values
temp_search_text     = list(set(temp_search_text)); 
arr_search_positive  = temp_search_text[0:100]; arr_search_positive

# -----
# NEGATIVE文のレコードリスト
temp_search_text     = df_bayes_train_second[df_bayes_train_second.label=='negative'].loc[:,'text'].values
temp_search_text     = list(set(temp_search_text)); 
arr_search_negative  = temp_search_text[0:100]; arr_search_negative

# ----------------
#　100件の判定結果を見てみる(1)
# --- POSITIVE ---
arr_estimate = [] 
for i in arr_search_positive:       # max_train_record
    arr_search_vocab = df_bayes_train_second[df_bayes_train_second.text==i].loc[:,'name'].values
    print("POSITIVE - i: {}, vocabs: {}".format(i,arr_search_vocab))
    tempdf_BgOfWd_second = df_BgOfWd_second.query('vocab in @arr_search_vocab')
    #print(tempdf_BgOfWd_second)
    # -----
    val_ratio = tempdf_BgOfWd_second.sum()['log_ratio']
    arr_estimate.append(val_ratio)
    print("POSITIVE - i: {}, sum_of_val_ratio: {}".format(i, round(val_ratio,5)))
print(arr_estimate)
```

D先生 ： “なんか、予測性能がダメダメですね。これって、本来はポジティブなレコードでしょう？なのに、**マイナスのトータルR値**が頻繁にでています。”

![imageJRL3-13-4](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-4.jpg)

QEU:FOUNDER ： “少しだけプラスもあるね。まあ、つづいてネガティブの場合も見てみましょう。”

![imageJRL3-13-5](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-5.jpg)

QEU:FOUNDER  ： “ネガティブの場合には比較的うまく行っているようですね。”

D先生 ： “気が付いたんですが、**文が長いとマイナス値が大きくなっていません**？”

QEU:FOUNDER ： “そんな感じだね・・・”

D先生 ： “「Bag Of Words」データベースのR値にバイアスがあるんでしょう。中を見てもらえませんか？”

![imageJRL3-13-6](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-6.jpg)

QEU:FOUNDER  ： “上表のL値は、文中に存在するトークン数を示します。**ネガティブのL値はポジティブよりも圧倒的に数字が大きい**ですよね。”

D先生 ： “どういう意味だろ・・・。”

QEU:FOUNDER  ： “**人間の業**なんですよ。人は他人を誉めるときには短い文しか書かないが、批判するには目いっぱい気合を入れるでしょ（笑）。”

D先生 ： “そういうことか・・・（笑）。このバイアスの補正をしなければいけないですね。どうすればいいんだろう・・・。”

QEU:FOUNDER ： “ナイーブベイズの限界が見えてきたんだから、（ベイズは）ここらへんでいいでしょ。他にも交互作用やワード・オーダーの欠点もあるし・・・。”

**（交互作用の例文）**

### Positive  : This cuisine is healthy.
### Negative : This cuisine is not healthy.

**（単語の並びの例文）**

### Positive:   銭不是問題。
### Negative:  不、銭是問題。

D先生 ： “じゃあ、他のやり方を試すんですか？ロジスティック回帰とか・・・。”

QEU:FOUNDER ： “**T法(2)でロジスティック回帰**をやってみましょうよ。T法(2)では、交互作用の問題はある程度緩和できるかもしれません。”あっ、そうだ。重要なこと・・・。是非、カンパをください・・・。”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ： “T法でロジスティック回帰とは、おもしろいです。”


## ～　まとめ　～

### ・・・　つづきです　・・・

QEU:FOUNDER ： “こういう結果が出ました。”

![imageJRL3-13-7](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-7.jpg)

C部長 : “よくわかりました。**世の中が愛で満ちていれば良い**のですが・・・。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）：　「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER ： “実際には、そうではないわけで・・・。”

![imageJRL3-13-8](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-8.jpg)

C部長 : “でも、**そうでない国は衰退するだけ**です。”

QEU:FOUNDER ： “ちょっと幻滅するかもしれんが、そもそも**「資本」というのはそういうモン**かもしれんよ・・・。”

[![MOVIE1](http://img.youtube.com/vi/5s1fN34uN5A/0.jpg)](http://www.youtube.com/watch?v=5s1fN34uN5A "【白井聡 ニッポンの正体】呼び出されるマルクス 〜生きづらさの根源解き明かす「資本論」〜")

QEU:FOUNDER ： “資本は人間の幸せに関心がない。**ただただ価値増殖するだけのモノ**らしいです。”

![imageJRL3-13-9](/2023-03-29-QEUR23_SNLPS3/imageJRL3-13-9.jpg)

C部長 : “あ～あ、いっちゃった・・・。**今の若者の夢は会社員になること**なのに・・・。それにしても、この先生（↑）の資本論はY先生とは違った味がありますね。”

QEU:FOUNDER ： “おすすめです。”


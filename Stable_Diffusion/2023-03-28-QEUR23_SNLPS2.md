---
title: QEUR23_SNLPS2:　ナイーブ・ベイスによるSentiment Analysis (Yelp_Review、前編)
date: 2023-03-28
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---
## QEUR23_SNLPS2:　ナイーブ・ベイスによるSentiment Analysis (Yelp_Review、前編)

## ～　人間を感じるためのベイズ確率　～

D先生 ： “今回から、ベイズを使ったSentiment Analysisですね。ディープラーニングが発明された後、**すでに過去のものとなった手法**ですが・・・。”

QEU:FOUNDER  ： “まあ、とにかく計算が「軽い」手法なんで、今後も**「簡単なミス検出みたいな用途」**に使われていくんじゃない？さて、今回はこの先生の授業を使います。”

[![MOVIE1](http://img.youtube.com/vi/dt7sArnLo1g/0.jpg)](http://www.youtube.com/watch?v=dt7sArnLo1g "Sentiment Classification with Naive Bayes & Logistic Regression, contd. (NLP video 5)")

D先生 ： “へえ・・・。2019年時点では、Fast.aiは**自然言語処理(NLP)の授業**をやっていたんですね。じゃあ、我々はそのマネをするだけでいいじゃないですか・・・。”

QEU:FOUNDER  ： “小生も最初はそう思っていましたが、実はそうはなりませんでした。**Fastaiのシステムが2019年のversion1からversion2に変わった**んです。だから、授業のプログラムの多くがそのままで動かなくなりました。自分で作ったほうが速くなった。トークン化はいただいてはいますが・・・。”

D先生 ： “じゃあ、しょうがない。先生の理屈(↓)だけを「いただく」ことにして・・・。”

![imageJRL3-12-1](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-1.jpg)

QEU:FOUNDER ： “Youtubeのコメント欄にnotebookのリンクがありますので、詳しくはそれを見てください。ベイズ判別とは、つまるところトークン毎の確率を積算して予測します。批判している（negative）単語のR値は大きなマイナス値にします。一方、誉めている(positive)単語には大きなプラスを割り当てます。だから、学習のポイントはR値の計算手法だけです。では、プログラムをドン！！”

```python
# -----
from fastbook import *
from fastai.text.all import *

# -----
import sklearn.feature_extraction.text as sklearn_text
from IPython.display import display,HTML
import pickle 

df = pd.read_csv('./reviews_with_splits_lite(new).csv')      # reviews_with_splits_lite(new)
df.head()

# -----
# train-test-validに分離させます
df_train = df[df.split=='train']
# CSVファイルに出力する(Original Data)
df_train.to_csv("./yelp_reviews_org_train.csv")

# -----
# 今回使うデータ・レコードの数
max_train_record = len(df_train)
print("max_train_record: ", len(df_train))
#max_train_record  = 5000
max_BgOfWd  = 2000

# データを抽出する
txts = df_train.loc[:,"text"].values #; txts[0:5]
arr_label_org = df_train.loc[:,"label"].values #; arr_label_org
arr_split_org = df_train.loc[:,"split"].values #; arr_split_org
txts[0:5]

# -----
#　トークン変換用のインスタンスを生成する
spacy = WordTokenizer()
tkn = Tokenizer(spacy)
#print(tkn(txts[0]))

# ボキャブラリーのネタを生成する
toks = []
for i in range(max_BgOfWd):
    toks.append(tkn(txts[i]))
print(toks[0:5])

# -----
# ボキャブラリをセットアップして、ボキャブラリーのデータベースを生成する
num = Numericalize()
num.setup(toks)
coll_repr(num.vocab,20)

# -----
# トークンのネタを生成する
toks = []
for i in range(max_train_record):
    toks.append(tkn(txts[i]))
print(toks[0:5])

# テキストのトークン情報を数字に変換(Numericalization)する
nums = []
for i in range(max_train_record):
    nums.append(num(toks[i]))
print("-------------")
print(nums[0:5])

# -----
# TEST段階(1)
arr_vocab = num.vocab
nums_txt  = nums[0]
print("nums_txt: ", nums_txt)
print("len_nums_txt: ", len(nums_txt))

# -----
# TEST段階(2)
print("len_arr_vocab: ", len(arr_vocab))
print("arr_vocab: ", arr_vocab[0:20])
print("nums_txt: ", nums_txt)
print("token[0]: ",tkn(txts[0]))
print(" ----------------------- ")
txt_translate = ""
for i in range(len(nums_txt)):
    ref_number = nums_txt[i]
    txt_translate = txt_translate + arr_vocab[ref_number] + " "
print(txt_translate)

```

QEU:FOUNDER  ： “ちょっとここで一息、トークンの妥当性テストの結果を見てみましょう。”

D先生 ： “え～っと、数字のリストと辞書を結合して、もとの文が戻ってくるのかを見るわけですね。”

![imageJRL3-12-2](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-2.jpg)

D先生 ： “いちおう、もとに戻っているようです。”

QEU:FOUNDER ： “今回のベイズ判別には、この数字の変換機能はいらないですが、後で使うかもしれません。”

```python
# -----
# MASS PRODUCTION
arr_vocab = num.vocab
print("len_arr_vocab: ", len(arr_vocab))

arr_text  = []
arr_value = []
arr_name  = []
arr_label = []
arr_split = []
for i in range(max_train_record):
    nums_txt  = nums[i]
    #print("nums_txt: ", nums_txt)
    #print("len_nums_txt: ", len(nums_txt))
    for j in range(len(nums_txt)):
        ref_number = nums_txt[j].item()
        arr_text.append(i)
        arr_label.append(arr_label_org[i])
        arr_split.append(arr_split_org[i])
        arr_value.append(ref_number)
        arr_name.append(arr_vocab[ref_number])

# -----
# トークン保管用のデータ・フレームを生成する
mx_text  = np.array([arr_text]).T
mx_value = np.array([arr_value]).T
mx_label = np.array([arr_label]).T
mx_name  = np.array([arr_name]).T
mx_split = np.array([arr_split]).T
mx_bayes = np.concatenate([mx_text, mx_value, mx_label, mx_name, mx_split], axis=1)
df_bayes = pd.DataFrame(mx_bayes, columns=["text","value","label","name","split"]); df_bayes

# -----
# 各ボキャブラリの登場数を検索する(Bag of Wordsを生成する)
arr_deleteA = []
arr_deleteB = []
arr_seq   = []
arr_words = []
arr_count = []
for i in range(len(arr_vocab)):
    df_temp  = df_bayes[df_bayes.name==arr_vocab[i]]
    val_temp = len(df_temp)
    if i < 50:
        arr_deleteA.append(arr_vocab[i])
        print("i:{},vocab:{},count:{}".format(i,arr_vocab[i],val_temp))
    else:
        if val_temp < 4:
            arr_deleteB.append(arr_vocab[i])
    arr_seq.append(i)
    arr_words.append(arr_vocab[i])
    arr_count.append(val_temp)
arr_delete = arr_deleteA + arr_deleteB

# -----
# 各ボキャブラリ別登場数(Bag of Words)をデータフレームにする
mx_seq    = np.array([arr_seq]).T
mx_words  = np.array([arr_words]).T
mx_count  = np.array([arr_count]).T
mx_BgOfWd = np.concatenate([mx_seq, mx_words, mx_count], axis=1)
df_BgOfWd = pd.DataFrame(mx_BgOfWd, columns=["seq","vocab","count"]); df_BgOfWd

# -----
# データの断捨離（第一弾）
# DFをみたところ、ボキャブラリの上位50位とマイナー語彙は無意味なので削除しましょう(1)
df_BgOfWd_first = df_BgOfWd.query('vocab not in @arr_delete')
#print(df_BgOfWd_first)

# CSVファイルに出力する(Bag Of Words)
df_BgOfWd_first.to_csv("csv_BgOfWd_first.csv")

```

QEU:FOUNDER  ： “ボキャブラリのデータは、今後、段階を踏みながら洗練されていきます。さあて、最初はどんな状態かな？”

![imageJRL3-12-3](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-3.jpg)

D先生 ： “データセットにはボキャブラリと学習データ（コーパス）内の存在数だけ、とてもシンプル・・・(笑)。”

QEU:FOUNDER ： “この初期処理に時間がとてもかかるので、ここで保存しておくのはしょうがないんだ。同時にコーパスは、後で処理をしやすいように**トークン毎にレコード化**されています。”

```python
# DFをみたところ、ボキャブラリの上位50位とマイナー語彙は無意味なので削除しましょう(2)
df_bayes_train_first = df_bayes.query('name not in @arr_delete')
print(df_bayes_train_first)

# CSVファイルに出力する(Corpus)
df_bayes_train_first.to_csv("csv_bayes_train_first.csv")

```

QEU:FOUNDER  ： “これのコーパス出力はとてもシンプルですよ。多分、今後も「ほとんど変わらない」と思います。”

D先生 ： “それは単純なデータ構造ですが・・・。えっ！なんと、**300万レコード**もあるんですか？”

![imageJRL3-12-4](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-4.jpg)

QEU:FOUNDER ： “この情報を少なくさせるには、ボキャブラリを絞りこまなくてはならないです。具体的には、**「情報の少ない」トークン（単語）は削除していく**・・・”

D先生 ： “存在量(count)の大きな単語から削ると効率がいいですね。それでは、count値でソートしてみましょう。”

![imageJRL3-12-5](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-5.jpg)

QEU:FOUNDER  ： “小生の独断と偏見で削除する対象をリストアップ（黄色）しました。”

D先生 ： “う～ん・・・。これは微妙な判断・・・。なんか、「place」を残しちゃったんですかのは変？”

QEU:FOUNDER  ： “「nice place」はあるが「bad place」は慣用的にはない**て・・・（笑）。主観ですよ、あくまで・・・。”

D先生 ： “つまり、2つのボキャブラリに交互作用があるという意味でしょ？ナイーブ・ベイズ(Naïve Nayes)には、そこらへんの判別はできるんですか？”

QEU:FOUNDER ： “実はナイーブだから、無理・・・（笑）。**ナイーブという意味は、交互作用や単語の並びという重要な言語情報を無視しているという意味だから**・・・。”

**（交互作用の例文）**

### Positive  : This cuisine is healthy.
### Negative  : This cuisine is not healthy.


**（単語の並びの例文）**

### Positive  :  銭不是問題。
### Negative  :  不、銭是問題。

D先生 ： “確かに、「not」というトークンそのものには、肯定的、否定的な意味はありません。でも、それが入ると全く文の意味が変わります。”

QEU:FOUNDER ： “結論からいうと、**ベイズによる予測では正確度は70%程度いけばいい方**じゃない？しかし、ついでに面白いことがわかります。詳しくは次回に説明するけど、各単語のR値を計算したデータフレームを先回りして紹介しましょう。単語別のR値（Rachel ThomasのNLP参照）として、下表の「Log_Ratio」の数値を見てください。”

**(ポジティブ・ランキング)**

![imageJRL3-12-6](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-6.jpg)

**(ネガティブ・ランキング)**

![imageJRL3-12-7](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-7.jpg)

QEU:FOUNDER ： “なぜか、**ネガティブな単語の場合にはlog_ratio値の絶対値が大きい**ですね。”

D先生 ： “なぜだろう・・・。”

QEU:FOUNDER  ： “それは、人間だから・・・(笑)。じゃあ、次回につづく・・・。”


## ～　まとめ　～

### ・・・　つづきです　・・・

QEU:FOUNDER ： “コレ（↓）は**マタイ（福音書）**にあるんだよね。小生の好きなヨハネにはなかった。これは意外・・・。”

![imageJRL3-12-8](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-8.jpg)

C部長 : “深い、深い、宗教のお話でした・・・。でも、よくわかんないことがあります。**「愛する」ってなん**ですか？”

QEU:FOUNDER ： “は？”

C部長 : “「愛している(love)」は「好き(like)」の類似語でしょ？でも、**「愛する(love)」って、かなり違った意味**がありそうな気がします。”

QEU:FOUNDER ： “じゃあ、**Chat-GPT**に聞いてみましょう。”

C部長 : “そんなもん、国語辞書を見ればよさそうなものですが・・・。”

QEU:FOUNDER ： “聖書における「愛する」の意味でしょ？**GPTは西洋のコーパス（Corpus:言語資料）を使っているので、より正確なことがわかります。**それでは、GPT博士の回答をドン！！”

![imageJRL3-12-9](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-9.jpg)

C部長 : “博士は、聖書の言葉の文脈で「愛する」を調べてくださったのですね？じゃあ・・・、**「自らを愛する」**というのは？”

![imageJRL3-12-10](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-10.jpg)

QEU:FOUNDER ： “こういう結果が出ました。”

C部長 : “よくわかりました。**世の中が愛で満ちていれば良い**のですが・・・。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）：　「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER ： “実際には、そうではないわけで・・・。”

![imageJRL3-12-11](/2023-03-28-QEUR23_SNLPS2/imageJRL3-12-11.jpg)

C部長 : “でも、**そうでない国は衰退する**だけです。”


---
title: QEUR23_SNLPS7:　T法(2)による Sentiment Analysis (Yelp_Review、その4)
date: 2023-04-05
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_SNLPS7:　T法(2)による Sentiment Analysis (Yelp_Review、その4)

## ～　角度を変えてみているだけです　～

### ・・・　前回のつづきです　・・・

D先生 ： “こんなに単位空間によって出力が変わっちゃうと、結局のところ何を信じていいのやら・・・。”

![imageJRL3-17-1](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-1.jpg)

QEU:FOUNDER  ： “じゃあ、**バギング(bagging)**する・・・(笑)？”

D先生 ： “ふ～ん・・・、バギングねえ・・・。そういえば、この資料(↑)はJeremy HowardのFast.AIのモノ（Jupyter notebook）でしょ？”

QEU:FOUNDER  ： “そうです。これは**ランダム・フォレスト**のものね・・・。ランダム・フォレストの基礎は決定木であり、全てのデータを学習に使わずにサンプリングして複数回学習させて総合的に予測します。その方法がバギングというわけ・・・。我々のT法(2)において、単位空間を変えることがバギングとなるわけ・・・。・・・という考え方でよい？”

![imageJRL3-17-2](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-2.jpg)

D先生 ： “まずはBag_Of_Wordsの中身を見てみましょう。10種類の単位空間がありますので、回帰係数のデータが単位空間の10種類分だけでてきます。”

![imageJRL3-17-3](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-3.jpg)

D先生 ： “単位空間をPOSITIVEにしたのが左側のデータ群、一方、それをNEGATIVEにしたのが右側のデータ群です。パッと見た目わかるのが**回帰係数の数字の符号が変わっている**ことです。単位空間がPOSITIVEの場合はプラス値、NEGATIVEの場合にはマイナス値です。”

QEU:FOUNDER  ： “・・・でも、一部、POSITIVEデータ群の中にもマイナス値があるのは面白いですね。強い言葉である**「disappoint（失望した）」**の回帰係数にはマイナス値がありますね。その一方で***「perfectly（完璧に）」***はプラス値があります。”

D先生 ： “さて、FOUNDER・・・。このCSVファイルをどうやって出力したんですか？”

QEU:FOUNDER  ： “前回のプログラムを修正すれば出力できます。これくらいは自分でやりましょう。それでは、今回のプログラムをドン！”

```python
# ---------------------------
# Bag_Of_Wordsを計算する(For T-method(2))
# ---------------------------
from fastbook import *
from fastai.text.all import *
from collections import Counter
# -----
# 散布図を描きます
import matplotlib.pyplot as plt
%matplotlib inline

# ---------------------------
# Bag_Of_Wordsデータを読み込む
df_T2_Coef = pd.read_csv('./csv_vocab_T2Coef_second.csv')      # Bag_Of_Words(with Coefficient)

# 回帰係数
mx_coef    = df_T2_Coef.loc[:,"coef0":"coef9"].values
# ボキャブラリのリスト
arr_vocab  = df_T2_Coef.loc[:,"vocab"]
# 単位空間のボキャブラリの出現回数
mx_Xs_tani = df_T2_Coef.loc[:,"tani0":"tani9"].values
print("------ mx_coef ------")
print(mx_coef)
print("------ arr_vocab ------")
print(arr_vocab)
print("------ mx_Xs_tani ------")
print(mx_Xs_tani)

```

![imageJRL3-17-4](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-4.jpg)

```python
# ---------------------------
# T法(2)計算シートデータを読み込む
df_tani = pd.read_csv('./csv_T2Calc_first.csv')      # reviews_with_splits_lite(new)

arr_Y_tani = df_tani[df_tani["T2Split"]=="tani"].loc[:,"mod_logi"].values
len_line_tani = df_tani[df_tani["T2Split"]=="tani"].loc[:,"len_line"].values
print(arr_Y_tani)
print("------------")
print(len_line_tani)

# -----
# index指定でテスト用のサンプル文を抽出する
# NEGATIVE(from 400) - POSITIVE(from 20000)
idxs_sample = [401, 402, 403, 404, 405, 406, 20001, 20002, 20003, 20004, 20005, 20006,]  # NEGATIVE
atts_sample = ['pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg',]

# --------------------------
# 予測(Prediction)空間を計算する
# ---------------------------
# テスト空間（修正済みコーパス）生データを読み込む
df_pred = pd.read_csv('./csv_bayes_train_second.csv')      # reviews_with_splits_lite(new)
df_pred = df_pred.loc[:,"text":]
df_pred
```

![imageJRL3-17-5](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-5.jpg)

```python
# ---------------------------
# 予測空間のYの値(ロジット変換後)の初期設定
Y_pred_negative = -1.2
Y_pred_positive = 1.2
# -----
# 出現回数マトリックスを初期設定する
len_BgOfWd = len(arr_vocab)
len_tani   = 10
len_sample = len(idxs_sample)
print(len_sample)

# ---------------------------
# [予測空間]: サンプル文のボキャブラリをカウントする関数
def calc_T2vocab_pred(arr_vocab, arr_pred_sample):
    # Counterで出現回数を数える
    dic_counter   = Counter(arr_pred_sample)
    #print(dic_counter)
    # -----
    # T法(2)計算シートの情報からトークンの使用状況を抽出する
    arr_Xs_pred = np.array([0]*len(arr_vocab))
    #print("-------")
    #print(arr_Xs_pred)
    for iCnt, vocab in enumerate(arr_vocab):
        num_count = dic_counter[vocab]
        #print("iCnt:{} ,vocab:{} ,num_count: {}".format(iCnt, vocab, num_count))
        arr_Xs_pred[iCnt] = num_count
    return len(arr_pred_sample), arr_Xs_pred

# ---------------------------
# [単位空間]: T法(2)予測のデータ準備を行う
def calc_T2_prepare(sampleNo):
    # -----
    df_pred_sample = df_pred[df_pred.text == idxs_sample[sampleNo]]   
    # -----
    # サンプル文を単語列に分解する
    arr_pred_sample = df_pred_sample.loc[:,"name"].values

    if atts_sample[sampleNo] == 'pos':
        # -----
        # サンプルのY値を計算する
        Y_pred_sample = Y_pred_positive*len(arr_pred_sample)/1000
    else:
        # -----
        # サンプルのY値を計算する
        Y_pred_sample = Y_pred_negative*len(arr_pred_sample)/1000

    # サンプル文のボキャブラリをカウントする
    val_len, arr_Xs_pred = calc_T2vocab_pred(arr_vocab, arr_pred_sample)

    return arr_pred_sample, val_len, arr_Xs_pred, round(Y_pred_sample,5)

# ---------------------------
# [単位空間]: T法(2)による予測計算を行う
def calc_T2_estimate(taniNo, mx_coef, arr_Xs_pred, mx_Xs_tani):
    # 参照番号の文のトークンを抽出する
    arr_Xs_tani = mx_Xs_tani[:, taniNo]  #; 単位空間の件数
    arr_coeffs  = mx_coef[:, taniNo]  #; 回帰係数
    sum_value = 0.0
    for iCnt in range(len_BgOfWd):   # arr_set_vocab[0:10]
        est_item   = arr_Xs_pred[iCnt] - arr_Xs_tani[iCnt]
        value_item = arr_coeffs[iCnt] * est_item
        #print("iCnt: {}, est_item: {}, value_item: {}".format(iCnt, est_item, value_item))
        sum_value  = sum_value + value_item
    return round(sum_value, 5)

# ---------------------------
# 予測データを10件だけ抽出して、sampleNo別に予測する
mx_result = np.zeros([len_tani, len_sample])
for sampleNo in range(len_sample):

    arr_pred_sample, val_len, arr_Xs_pred, Y_pred_sample = calc_T2_prepare(sampleNo)
    print("---- sampleNO: {}, index: {}, Y_pred_sample: {} ---".format(sampleNo, idxs_sample[sampleNo], Y_pred_sample))

    # -----
    # １０種類の単位空間で予測を行う
    for taniNo in range(len_tani):
        Y_temp = calc_T2_estimate(taniNo, mx_coef, arr_Xs_pred, mx_Xs_tani)
        Y_predict = round(Y_temp + 1.5*arr_Y_tani[taniNo], 5)
        mx_result[taniNo, sampleNo] = Y_predict
        print("taniNo: {}, Y_predict: {}".format(taniNo, Y_predict))
# -----
print(mx_result)

import matplotlib.pyplot as plt
%matplotlib inline 

plt.title("Prediction data - negative(0-5)")
plt.plot(mx_result[:, 0], label="sample-NO0")
plt.plot(mx_result[:, 1], label="sample-NO1")
plt.plot(mx_result[:, 2], label="sample-NO2")
plt.plot(mx_result[:, 3], label="sample-NO3")
plt.plot(mx_result[:, 4], label="sample-NO4")
plt.plot(mx_result[:, 5], label="sample-NO5")
plt.legend()
plt.show()

plt.title("Prediction data - positive(6-11)")
plt.plot(mx_result[:, 6], label="sample-NO6")
plt.plot(mx_result[:, 7], label="sample-NO7")
plt.plot(mx_result[:, 8], label="sample-NO8")
plt.plot(mx_result[:, 9], label="sample-NO9")
plt.plot(mx_result[:, 10], label="sample-NO10")
plt.plot(mx_result[:, 11], label="sample-NO11")
plt.legend()
plt.show()
```

QEU:FOUNDER  ： “残念ながら、計算結果はありません・・・（笑）。”

D先生 ： “あれ？**T法（2）の総合推定値**を使って予測するんじゃなかったのですか？”

![imageJRL3-17-6](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-6.jpg)

QEU:FOUNDER  ： “やってみたら、結果がグチャグチャでお話になりませんでした（笑）。”

D先生 ： “じゃあ、これからどうするんですか？”

QEU:FOUNDER ： “別のやり方で評価します。”

```python
# ---------------------------
# [単位空間]: T法(2)を応用して、項目別重みカウント計算を行う
def calc_T2_count(taniNo, mx_coef, arr_Xs_pred):
    # 参照番号の文のトークンを抽出する
    arr_coeffs  = mx_coef[:, taniNo]  #; 回帰係数
    sum_value = 0.0
    for iCnt in range(len_BgOfWd):   # arr_set_vocab[0:10]
        if arr_Xs_pred[iCnt] > 0:
            if arr_coeffs[iCnt] > 0.05:
                sum_value = sum_value + 1*arr_Xs_pred[iCnt]
            elif arr_coeffs[iCnt] > 0.001 and arr_coeffs[iCnt] <= 0.05:
                sum_value = sum_value + 0.2*arr_Xs_pred[iCnt]
            elif arr_coeffs[iCnt] >= -0.05 and arr_coeffs[iCnt] < -0.001:
                sum_value = sum_value - 0.2*arr_Xs_pred[iCnt]
            elif arr_coeffs[iCnt] < -0.05:
                sum_value = sum_value - 1*arr_Xs_pred[iCnt]
                
    return sum_value

# ---------------------------
# 予測データを12件だけ抽出して、sampleNo別に予測する
mx_result = np.zeros([len_tani, len_sample])
for sampleNo in range(len_sample):
    # -----
    arr_pred_sample, val_len, arr_Xs_pred, Y_pred_sample = calc_T2_prepare(sampleNo)
    print("---- sampleNO: {}, index: {}, Y_pred_sample: {} ---".format(sampleNo, idxs_sample[sampleNo], Y_pred_sample))
    #print("val_len: {}, Y_pred_sample: {}".format(val_len, Y_pred_sample))
    #print("---- arr_Xs_pred ----")
    #print(arr_Xs_pred)  
    #print("---- arr_pred_sample ----")
    #print(arr_pred_sample)
    # -----
    # １０種類の単位空間で予測を行う
    for taniNo in range(len_tani):
        Y_predict = calc_T2_count(taniNo, mx_coef, arr_Xs_pred)
        mx_result[taniNo, sampleNo] = Y_predict
        print("taniNo: {}, Y_predict: {}".format(taniNo, Y_predict))
# -----
print(mx_result)

#import matplotlib.pyplot as plt
#%matplotlib inline 
# -----
plt.title("Prediction data - negative samples")
plt.plot(mx_result[:, 0], label="neg-NO0")
plt.plot(mx_result[:, 1], label="neg-NO1")
plt.plot(mx_result[:, 2], label="neg-NO2")
plt.plot(mx_result[:, 3], label="neg-NO3")
plt.plot(mx_result[:, 4], label="neg-NO4")
plt.plot(mx_result[:, 5], label="neg-NO5")
plt.grid()
plt.legend()
plt.show()

```

![imageJRL3-17-7](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-7.jpg)

D先生 ： “上のグラフの横軸は単位空間（10件）ですね。そのうち、初めの5件はPOSITIVE文の単位空間、残りの5件はNEGATIVE文です。えっ？単位空間によってメトリックスがこんなにちがうんですか？”

QEU:FOUNDER ： “**単位空間は学習データ（信号空間）を見る角度を変える**んです。具体的には、計測したい「ある文章」があるとして、***単位空間をPOSITIVEとすればその文章の「プラス側面を評価」できます**。その一方で単位空間をNEGATIVEにすれば、その文のマイナス側面を評価できます。”

D先生 ： “プラス側面といっても、さらにいろいろな側面があるわけで・・・。これが「T法(2)のバギング」の考え方ですね。”

```python
# -----
plt.title("Prediction data - positive samples")
plt.plot(mx_result[:, 6], label="pos-NO6")
plt.plot(mx_result[:, 7], label="pos-NO7")
plt.plot(mx_result[:, 8], label="pos-NO8")
plt.plot(mx_result[:, 9], label="pos-NO9")
plt.plot(mx_result[:, 10], label="pos-NO10")
plt.plot(mx_result[:, 11], label="pos-NO11")
plt.legend()
plt.show()

```

![imageJRL3-17-8](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-8.jpg)

D先生 ： “ただし、単位空間の影響が強すぎるのが問題ですね。サンプルをPOSIIVEにしても、NEGATIVEにしても大きな差異は出てこないです。”

QEU:FOUNDER ： “ただし、それでも小さな差異はあるわけで、そこらへんを**「総合的に評価」**する必要があるでしょうね・・・。さて、これから検出力の評価をしますけど、現状では良い結果は期待できないよ。”

```python
# ---------------------------
# 予測データを12件だけ抽出して、sampleNo別に予測する
arr_positive_result = []
arr_negative_result = []
for sampleNo in range(len_sample):
    count_positive = 0
    count_negative = 0
    for taniNo in range(len_tani):
        if mx_result[taniNo, sampleNo] > 0:
            count_positive = count_positive + 1
        elif mx_result[taniNo, sampleNo] < 0:
            count_negative = count_negative + 1
    arr_positive_result.append(count_positive)
    arr_negative_result.append(count_negative)
# -----
print("-------- samples :      negatives : positives ")
print("positive values:", arr_positive_result)
print("negative values:", arr_negative_result)
```

![imageJRL3-17-9](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-9.jpg)

D先生 ： “どうしても予測がマイナス値（NEGATIVE）に引っ張られていますね。その原因は？まあ・・・、ここでは、あえてT法(2)は使えるという前提で・・・（笑）。”

QEU:FOUNDER  ： “もともとベイズ予測でもNEGATIVEに引っ張られる傾向はありますから・・・。あとは**学習データはベイズ予測のときの10%程度しか使っていません**からね。”

D先生 ： “リベンジします？もっと学習データを増やして？”

QEU:FOUNDER ： “いや、もう勘弁して・・・。さすがに、ディープラーニングがNLPの主流になった現在において、**T法をこの手の自然言語処理のために使うことに意味があるとは思えません**。”

D先生 ： “ここは途中経過ですからね。”


## ～　まとめ　～

QEU:FOUNDER ： “ああ・・・。やっぱりそうだったか・・・。”

[![MOVIE1](http://img.youtube.com/vi/bNPMQAyj6Cs/0.jpg)](http://www.youtube.com/watch?v=bNPMQAyj6Cs "【AI活用術】ChatGPTを思い通りに動かすプロンプトの作り方")

C部長 : “今回の事例ではブログのサイトを自動構築したんですね。なんか・・・、使ったプロンプトは仕様書みたいですね。”

QEU:FOUNDER ： “だから、**ユーザーに近い製品を作るのはカンタン**ですよ。「テトリスを作りたい」というようにパターン化された場合とか・・・。”

C部長 : “さて、我々が、今、考えているのは？”

**(ShigenoriSoejima-JeanNouvel)**

![imageJRL3-17-10](/2023-04-05-QEUR23_SNLPS7/imageJRL3-17-10.jpg)

QEU:FOUNDER ： “**「EVERYBODY IS CREATIVE CLASS」**という理想の実現です。具体的には、その第一歩としてStable_DiffusionのPrompt_Engineeringを最適化します。より簡単な学習により、よりよい画像を作るプロンプトを設計することをやってみたいんです。”

C部長 : “ただし、その**「良い画像」というのが人によって違います**。だからこそアート（芸術）なわけで・・・。”

[![MOVIE2](http://img.youtube.com/vi/Tf-8F5q8Xww/0.jpg)](http://www.youtube.com/watch?v=Tf-8F5q8Xww "Lesson 11 2022: Deep Learning Foundations to Stable Diffusion")

QEU:FOUNDER ： “このような用途には、当然、より大規模な学習情報が必要なディープラーニングは使えません。逆に、そのような巨大なシステムになると、それをだれが使っても同じ結果になります。**アートの世界で「だれがやっても同じ」はない**でしょ(笑)？そういえば、Jeremy HowardがとうとうStable_Diffusionの本格的なレクチャーを始めたね。”

C部長 : “いまはGPT(自然言語処理関連)の方が人気があるんじゃないかな？”

QEU:FOUNDER ： “彼は、**より創造的なモノを作るシステムとしてはStable_Diffusionのやりかたが優れている**と考えているんです。別に、彼はAUTOMATIC1111でも簡単にやれるようなAIお絵描きテクノロジの教授をしたいとは思っていないんですよ。”


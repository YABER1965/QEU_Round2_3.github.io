---
title: QEUR23_ATTNS6: ゲーム2048用の強化学習をやってみた（RAW DATA）
date: 2023-10-26
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_ATTNS6: ゲーム2048用の強化学習をやってみた（RAW DATA）

## ～ 「こんなもの」だとは思う・・・。 ～

QEU:FOUNDER ： “やっと久々の強化学習に入れる。「強化学習」って覚えてる？”

C部長 : “思いっきり、わすれました！”

QEU:FOUNDER ： “じゃあ、いまはやりの**「学びなおし」**を行くよ。ドン！！”

![imageJRL3-46-1](/2023-10-26-QEUR23_ATTNS6/imageJRL3-46-1.jpg)

D先生 ： “総本山のpyTorchが発行した論文なので、イケているんだとは思います。でも、cartpoleの学習って、こんなに速かったっけ？”

QEU:FOUNDER ： “小生も同じ疑問を持ちました。プログラムを見ると、なんと**ゲームの毎ステップで学習**しています。いやあ、これは学習負荷がかかる！！そこそこに**強力なGPUがないとうまく動かない**です。”

C部長 : “まあ、「時は金なり」だし・・・。あと、今回は畳み込みメトリックスをやるんですか？”

QEU:FOUNDER ： “いや。**4x4の生データ**を使います。早速だから、プログラムをドン！！”

```python
# ----
# 2048ゲームの簡単な強化学習(CONV-SCORE)
# 強化学習の参考HP(PYTORCH OFFICIAL)
# -ttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
# ----
import math
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple, deque, Counter

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
# ---------------- 
#%matplotlib inline

# ---------------- 
# set up matplotlib
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display
plt.ion()

# if GPU is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---
Transition = namedtuple('Transition',
                        ('state', 'action', 'next_state', 'reward'))

#=================================================
# METRIC FUNCTIONS
#=================================================
# 畳み込み処理を実施する
#def apply_kernel(row, col, kernel, mx_tensor):
#    return (mx_tensor[row:row+3,col:col+3] * kernel).sum()

# ---------------------------
# 畳み込みテンソルイメージを生成する
def create_tsrConv(mx_signal):

    # ----
    # パネルスコアを線形化する
    for i in range(4):
        for j in range(4):
            if mx_signal[i][j] > 0:
                mx_signal[i][j] = math.log2(mx_signal[i][j])

    # 特徴ベクトルを結合する
    all_max = np.array(mx_signal.flatten())

    return all_max

# ---------------- 
# 2048ゲームのプログラム
# ---------------- 
# ボードのサイズ
SIZE = 4

# ボードを初期化する関数
def init_board():
    # ボードは二次元リストで表現する
    board = [[0] * SIZE for _ in range(SIZE)]
        
    return board

# ボードをリセットする関数(ただし、ゲームを5ターンだけ、あらかじめ動かしておく)
def reset_board(board):
    # ボードをいったん空にする
    for i in range(SIZE):
        for j in range(SIZE):
            board[i][j] = 0
    # ランダムな位置に2か4を配置する
    add_random_tile(board)
    add_random_tile(board)

    # ---
    # メインの処理を実行する
    for i in range(5):
        arr_action = [0, 1, 2, 3]
        action = random.choice(arr_action)
        #print("i:{0},action:{1}".format(i,action))
        
        # ゲームを実行する
        board,reward,done = run_game(board, action)
    
    return board

# ランダムな位置に2か4を配置する関数
def add_random_tile(board):
    # 空いているセルの座標をリストに格納する
    empty_cells = []
    for i in range(SIZE):
        for j in range(SIZE):
            if board[i][j] == 0:
                empty_cells.append((i, j))
    # 空いているセルがなければ何もしない
    if not empty_cells:
        return
    # 空いているセルの中からランダムに選ぶ
    i, j = random.choice(empty_cells)
    # 10%の確率で4を、90%の確率で2を配置する
    board[i][j] = 4 if random.random() < 0.1 else 2

# ボードを左にスライドする関数
def slide_left(board):
    # 報酬と変化があったかどうかのフラグ
    reward = 0
    changed = False
    # 各行について処理する
    for row in board:
        # まず0でないセルだけを抽出する
        non_zero_cells = [cell for cell in row if cell != 0]
        # 隣り合うセルが同じ値なら結合する
        merged_cells = []
        i = 0
        while i < len(non_zero_cells):
            if i + 1 < len(non_zero_cells) and non_zero_cells[i] == non_zero_cells[i + 1]:
                merged_cells.append(non_zero_cells[i] * 2)
                reward = reward + non_zero_cells[i] * 2
                i += 2
            else:
                merged_cells.append(non_zero_cells[i])
                i += 1
        # 結合した後の行の長さが元の行の長さより短ければ、0で埋める
        while len(merged_cells) < SIZE:
            merged_cells.append(0)
        # 元の行と異なれば、変化があったとする
        if merged_cells != row:
            changed = True
        # 元の行を更新する
        row[:] = merged_cells[:]
    return changed, reward

# ボードを右にスライドする関数（左にスライドした後に反転させる）
def slide_right(board):
    reverse_board(board)
    changed, reward = slide_left(board)
    reverse_board(board)
    return changed, reward

# ボードを上にスライドする関数（左に90度回転させた後に左にスライドし、元に戻す）
def slide_down(board):
    rotate_board(board)
    changed, reward = slide_left(board)
    rotate_board(board, -1)
    return changed, reward

# ボードを下にスライドする関数（右に90度回転させた後に左にスライドし、元に戻す）
def slide_up(board):
    rotate_board(board, -1)
    changed, reward = slide_left(board)
    rotate_board(board)
    return changed, reward

# ボードを反転させる関数（水平方向に反転）
def reverse_board(board):
    for row in board:
        row.reverse()

# ボードを回転させる関数（時計回りにn回90度回転）
def rotate_board(board, n=1):
    n %= 4 # nを0から3の範囲に正規化する
    for _ in range(n):
        board[:] = list(map(list, zip(*board[::-1])))

# ボードが埋まっているかどうかを判定する関数
def is_full(board):
    return all(cell != 0 for row in board for cell in row)

# ボードに動きがあるかどうかを判定する関数
def has_move(board):
    # 空いているセルがあれば動きがある
    if not is_full(board):
        return True
    # 隣り合うセルに同じ値があれば動きがある
    for i in range(SIZE):
        for j in range(SIZE):
            cell = board[i][j]
            if (i + 1 < SIZE and cell == board[i + 1][j]) or (j + 1 < SIZE and cell == board[i][j + 1]):
                return True
    # それ以外は動きがない
    return False

# ボードをテキストで表示する関数
def print_board(board):
    # 各セルの幅を揃えるために、最大桁数を求める
    max_width = max(len(str(cell)) for row in board for cell in row)
    # 各行について処理する
    for row in board:
        # セルの値を文字列に変換し、空白で埋める
        cells = [str(cell).rjust(max_width) for cell in row]
        # セルをパイプで区切って表示する
        print("|" + "|".join(cells) + "|")

# メインの処理
def run_game(board, action):
    global num_failed
    # 変数の導入
    reward = 0
    done = False
    
    # -----------
    # ゲームの進め方(ACTION)
    #arr_action = ["s", "a", "w", "d"]
    # s - 0 : DOWN
    # a - 1 : LEFT
    # w - 2 : UP
    # d - 3 : RIGHT
    # -----------
    # 入力に応じてボードをスライドさせる
    if action == 0:
        changed, reward = slide_down(board)
    elif action == 1:
        changed, reward = slide_left(board)
    elif action == 2:
        changed, reward = slide_up(board)
    elif action == 3:
        changed, reward = slide_right(board)
    else:
        print("Invalid input")
        #continue
    # ボードに変化があれば、ランダムな位置に新しいタイルを追加する
    if changed:
        add_random_tile(board)
        # ボードを表示する
        #print_board(board)
    else:
        #print("No move")
        reward = -100*num_failed
        
    # ボードに動きがなければ、ゲームオーバーとする
    if not has_move(board):
        #print("Game over")
        done = True

    return board, reward, done
```

QEU:FOUNDER ： “今回は、指示ミスが発生すると環境から**大きなマイナス報酬**がでます。”

```python
#=================================================
# REINFORCEMENT LEARNING
#=================================================
# ReplayMemory
class ReplayMemory(object):

    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        """Save a transition"""
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

# DQN
class DQN(nn.Module):

    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 256)
        self.layer2 = nn.Linear(256, 256)
        self.layer3 = nn.Linear(256, n_actions)
    # Called with either one element to determine next action, or a batch
    # during optimization. 
    # 次のアクションを決定するために 1 つの要素で呼び出されるか、最適化中にバッチで呼び出されます。
    # Returns tensor([[down0exp,left0exp,up0exp,right0exp]...]).
    
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)

# select_action
def select_action(state, last_nomove, num_failed):
    global steps_done
    sample = random.random()
    eps_threshold = EPS_END + (EPS_START - EPS_END) * \
        math.exp(-1. * steps_done / EPS_DECAY)
    #steps_done += 1
    if sample < eps_threshold or (num_failed > 200 and last_nomove == True):
        arr_action = [0, 1, 2, 3]
        return torch.tensor([[random.choice(arr_action)]], device=device, dtype=torch.long), eps_threshold

    else:
        with torch.no_grad():
            # t.max(1) は、各行の最大の列値を返します。
            # max result の 2 列目は max 要素が見つかった場所のインデックスであるため、期待される報酬がより大きいアクションを選択します。
            return policy_net(state).max(1)[1].view(1, 1), eps_threshold

def optimize_model():
    if len(memory) < BATCH_SIZE:
        return
    transitions = memory.sample(BATCH_SIZE)
    # Transpose the batch. This converts batch-array of Transitions
    # to Transition of batch-arrays.
    batch = Transition(*zip(*transitions))

    # 非最終状態のマスクを計算し、バッチ要素を連結する
    # (最終状態は、シミュレーションが終了した後の状態になります)
    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                          batch.next_state)), device=device, dtype=torch.bool)
    non_final_next_states = torch.cat([s for s in batch.next_state
                                                if s is not None])
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    # Q(s_t, a) を計算する - モデルは Q(s_t) を計算し、実行されたアクションの列を選択します。
    # これらは、policy_net に従って各バッチ状態に対して実行されるアクションです。
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # すべての次の状態について V(s_{t+1}) を計算します。
    # non_final_next_states のアクションの期待値は、「古い」target_net に基づいて計算されます。 max(1)[0] で最高の報酬を選択します。
    # これはマスクに基づいてマージされ、期待される状態値、または状態が最終的な場合は 0 になります。
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]
    # Compute the expected Q values
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # フーバー損失を計算する
    criterion = nn.SmoothL1Loss()
    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))

    # モデルを最適化する
    optimizer.zero_grad()
    loss.backward()
    # In-place gradient clipping
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
    optimizer.step()
    
    return round(loss.item(),5)
```

QEU:FOUNDER ： “さらに命令発行において、連続してエラーを起こさないようなロジックをとっています。”

```python
#=================================================
# MAIN ROUTINE
#=================================================
# ----------------
# 学習結果のグラフ化
# ----------------
# plot_durations
def plot_durations(show_result=False):
    plt.figure(1)
    durations_t = torch.tensor(episode_durations, dtype=torch.float)
    if show_result:
        plt.title('Result')
    else:
        plt.clf()
        plt.title('Training...')
    plt.xlabel('Episode')
    plt.ylabel('GAME-Duration')
    plt.plot(durations_t.numpy())
    # Take 100 episode averages and plot them too
    if len(durations_t) >= 100:
        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)
        means = torch.cat((torch.zeros(99), means))
        plt.plot(means.numpy())

    plt.pause(0.001)  # pause a bit so that plots are updated
    if is_ipython:
        if not show_result:
            display.display(plt.gcf())
            display.clear_output(wait=True)
        else:
            display.display(plt.gcf())

#=================================================
# PROGRAM INITIALIZATION
#=================================================
# Get number of actions from 2048's action space
n_actions = 4
# Get the number of state observations
n_observations = 16

# ----
# HYPER PARAMETERS
# BATCH_SIZE は、リプレイ バッファからサンプリングされたトランジションの数です。
# GAMMA は割引係数です。
# EPS_START is the starting value of epsilon
# EPS_END is the final value of epsilon
# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay
# TAU is the update rate of the target network
# LR is the learning rate of the ``AdamW`` optimizer
BATCH_SIZE = 128
GAMMA = 0.92
EPS_START = 0.999
EPS_END = 0.02
EPS_DECAY = 20000
TAU = 0.005
LR = 1e-4

# ---
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())

optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = ReplayMemory(1000)

num_failed = 1   # ボードが動かない(Nomove)
steps_done = 0
episode_durations = []
num_episodes = 10000
max_turn = 800

# --------------------------
# 初期化       
# --------------------------
# ボードを初期化する
board = init_board()

# 学習評価用の配列を初期化する
acc_episode = []
acc_turn = []
acc_score = []
acc_maxconv = []
acc_loss_epi = []
acc_failed_epi = []
acc_eps = []

#=================================================
# PROGRAM RUN!
#=================================================
# ゲームが終了するまで繰り返す
# メインの処理を実行する
for i_episode in range(num_episodes):

    # ボードをリセットする関数(ただし、ゲームを5ターンだけ、あらかじめ動かしておく)
    board = reset_board(board)
    # ボードを表示する
    print("--- RESETED, GAME NO: {} ---".format(i_episode))
    print_board(board)
    # ----------   
    # 畳み込みテンソルイメージを生成する
    np_board = np.array(board)
    observation = create_tsrConv(np_board)
    #print(observation)
    np_observation = np.array(observation)
    state = torch.tensor(np_observation, dtype=torch.float32, device=device).unsqueeze(0)
    #print("state:",state)

    # 単一ゲーム評価用の配列を初期化する
    arr_turn = []
    arr_reward = []
    arr_score = []
    arr_yzero = []
    arr_loss_game = []
    # ----
    last_nomove = False   # 前回に命令ミスが起きたか
    i_turn = 0  # ターン数
    score = 0   # 累積スコア（マイナスな含まない）
    num_failed = 1   # ボードが動かない(Nomove)

    # ゲームを始める
    while True:

        # -----------
        # ゲームの進め方(ACTION)
        #arr_action = ["s", "a", "w", "d"]
        # s - 0 : DOWN
        # a - 1 : LEFT
        # w - 2 : UP
        # d - 3 : RIGHT
        # -----------
        # ゲームを実行する
        action, eps_threshold = select_action(state, last_nomove, num_failed)
        next_board, reward_raw, done = run_game(board, action.item())
        # NO MOVEの処理
        if reward_raw < 0:
            last_nomove = True
            num_failed = num_failed + 1
            score = score
        else:
            last_nomove = False
            score = score + reward_raw
            #steps_done += 1
        reward = torch.tensor([reward_raw], device=device)
        #print("state:",state)
        #print("reward:",reward)
        #print("done:",done)

        # ----------   
        # 畳み込みテンソルイメージを生成する
        np_next_board = np.array(next_board)
        next_observation = create_tsrConv(np_next_board)
        #print(observation)

        if done or i_turn > max_turn:
            done = True
            next_state = None
        else:
            np_observation = np.array(next_observation)
            next_state = torch.tensor(np_observation, dtype=torch.float32, device=device).unsqueeze(0)

        # ------
        # DQN experience replay
        # Store the transition in memory
        memory.push(state, action, next_state, reward)

        # Move to the next state
        observation = next_observation
        board = next_board
        state = next_state

        # 最適化の 1 ステップを実行します (ポリシー ネットワーク上で)       
        # NO MOVE以外で学習します
        loss = 0
        if (i_turn+1)%20 == 0:
            loss = optimize_model()
            #print("loss:",loss)

            # ターゲットネットワークの重みのソフトアップデート
            # θ′ ← τ θ + (1 −τ )θ′
            target_net_state_dict = target_net.state_dict()
            policy_net_state_dict = policy_net.state_dict()
            for key in policy_net_state_dict:
                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + tar-get_net_state_dict[key]*(1-TAU)
            target_net.load_state_dict(target_net_state_dict)
            # EPSのカウントアップ
            steps_done += 1
            
        # ------
        # ゲーム実行の結果を出力する
        if (i_turn+1)%100 == 0:
            print("i_episode:{0},i_turn:{1},action:{2},reward:{3},done:{4},score:{5}".format(i_episode,i_turn,action,reward,done,score))
            if i_turn > 100 and (i_turn+1)%200 == 0:
                print_board(board)

        # マトリックス化
        if i_turn == 0:
            mx_cvmax = np.array([observation])
        else:
            mx_cvmax = np.vstack((mx_cvmax,observation)) # 結合してみる

        # ----------   
        # ZERO COUNTER
        state_flatten = np.array(board).flatten()
        cc = Counter(state_flatten)
        arr_yzero.append(cc[0]) 

        # 配列へ要素を追加する
        arr_turn.append(i_turn)
        arr_reward.append(reward_raw)
        arr_score.append(score)
        arr_loss_game.append(loss)

        # ボードに動きがなければ、ゲームオーバーとする
        if done == True or i_turn > max_turn:
            print("Game over")
            # -----
            # ゲーム結果のグラフ化
            episode_durations.append(i_turn + 1)
            plot_durations()

            # ----
            # CALC METRICS
            mask_turn = np.min([80,i_turn-20])
            acc_conv  = mx_cvmax[mask_turn:,:]

            # -----
            # 学習評価用の配列を初期化する
            acc_episode.append(i_episode)
            acc_turn.append(i_turn)
            acc_score.append(score)
            acc_maxconv.append(np.max(acc_conv))
            acc_failed_epi.append(round(num_failed/(i_turn+1),5))
            acc_eps.append(eps_threshold)
            if None in arr_loss_game:
                print("None is present in the list")
                acc_loss_epi.append(10)
            else:
                acc_loss_epi.append(np.min(arr_loss_game))

            break
            
        else:
            i_turn = i_turn + 1

# -----
# 評価結果のグラフ化
print('Complete')
plot_durations(show_result=True)
plt.ioff()
plt.show()
```

QEU:FOUNDER ： “いろいろ、お二人からコメントは出るだろうが・・・。一応あらかじめ、学習結果の事例を一枚だけ晒させてくれ。”

![imageJRL3-46-2](/2023-10-26-QEUR23_ATTNS6/imageJRL3-46-2.jpg)

QEU:FOUNDER ： “・・・、それでは質問を受け付けます。なにかある？”

C部長 : “はい！質問です。**「TAU」**って、なんですか？”

QEU:FOUNDER ： “ターゲット側の学習に掛ける**「緩和係数」**のようです。はっきりいって、このやり方は初めて見ました。詳しいことはわかりません。小生としては、わりきって**「使えればいい」**から・・・。ただし、TAUの値は最適化する必要はあると思います。”

D先生 ： “じゃあ、TAUの値を変えて学習してみてください。”

![imageJRL3-46-3](/2023-10-26-QEUR23_ATTNS6/imageJRL3-46-3.jpg)

D先生 ： “おどろいた。パフォーマンスの様子が、かなり変わりますね。”

QEU:FOUNDER ： “今回の2048ゲームの場合には、パフォーマンスには**「スコアが高くなること」**と**「指示ミス（動かない方向に指示する）を最小にすること」**の2つがあります。TAUは、この相反する特性を調整する能力があるような気がします。要するに、PyTorchの元ネタであるcartpoleと2048では、環境の複雑さのレベルがぜんぜん違いますからね。”

![imageJRL3-46-4](/2023-10-26-QEUR23_ATTNS6/imageJRL3-46-4.jpg)

D先生 ： “cartpoleは簡単な物理モデルなので、当てはめるべき関数は簡単ですよね。だから、毎ステップ学習でもメリットが出てきます。今回の場合も、毎回学習するんですか？”

QEU:FOUNDER ： “今回は20ステップに1回学習します。中間を取って・・・。それでは、TAU値を0.05として、10000回も「豪勢に」学習してみました。ちなみに、**0.05あたりが、もっともよいバランスの値ではないか**と思っています。それでは結果をドン！！”

![imageJRL3-46-5](/2023-10-26-QEUR23_ATTNS6/imageJRL3-46-5.jpg)

D先生 ： “「failed_epi」は**指示ミスの発生比率**なんですね？命令の70％もエラーがでるんですか？これはひどいですね。ただし、10000回の学習でこれなら、「こんなもんかな？」とも言えないこともないです。”

QEU:FOUNDER ： “昔、参考にした2048の強化学習では10万回以上学習してましたから・・・。小生は、10000回以上学習させるつもりはないからね。”

D先生 ： “あくまで、「メトリックスの特徴比較」のツールですね。”

QEU:FOUNDER ： “次は畳み込みメトリックス（6次元）をやってみましょう。”

D先生 ： “結果の予想は？”

QEU:FOUNDER ： “もっと悪くなります。6次元のメトリックスの中には、**指示エラーのリスクを検知するための十分な情報はない**んじゃないかな？”

C部長 : “まあ、畳み込みメトリックスはSOART3の中間評価段階ですから・・・。”

QEU:FOUNDER ： “SOART３の感度（6次元）だけで学習すると、**同じレベルになる**だろうし・・・。”

D先生 ： “次回につづく。”


## ～ まとめ ～

D先生 ： “とうとう、来るべきときが来ました。“

![imageJRL3-46-6](/2023-10-26-QEUR23_ATTNS6/imageJRL3-46-6.jpg)

QEU:FOUNDER ： “我々には「秘密兵器」があります。”

D先生 ： “この人（↓）たちのこと？ 確かに、「責任感」があり、「有能」です。多分・・・。 “

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）： 「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER ： “もちろん、この人たち（↑）にもおおいに期待しています。**自分でスゴイといっていました**し、信じています。尊敬しています。しかし、今回の件に対応するには、より多くの人の助けが必要です。彼（↓）なんかはどうか？”

![imageJRL3-46-7](/2023-10-26-QEUR23_ATTNS6/imageJRL3-46-7.jpg)

D先生 ： “この人なの＠？これは、ちょっと・・・。ホント！私の「J国スゴイ」のロスをどうやって埋め合わせればいいんですか！！ちょっと聞いてください！！私のアイデンティティは**「A国のつぎにスゴイJ国」**なんですよ！“

QEU:FOUNDER ： “う～ん・・・。じゃあ、みんなでサッカー場でゴミ拾いすればいいじゃないですか？”

C部長 : “もう、やっています。”

QEU:FOUNDER ： “う～ん、J国は温泉が有名だ。みんなで**温泉の中でSNSライブ**をすれば、「J国スゴイ」になるんじゃないか？”

D先生 ： “また、新たな「レジェンド」が誕生してしまいます（泣）！ “

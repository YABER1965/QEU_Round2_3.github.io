---
title: QEUR23_SNLPS12:　簡単なNLP「続編」 (yelp review - その3) 
date: 2023-05-08
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_SNLPS12:　簡単なNLP「続編」 (yelp review - その3) 

## ～　物事(BoW)を別の角度から見てみる　～

QEU:FOUNDER  ： “前回はBayes予測で使われる**BoW(Bag of Words)**のやり方を応用して、各単語の特性を散布図（↓）で見える化しました。今回は、この知見を使って単位空間を生成し、T法(2)のメトリックスを作成しましょう。”

![imageJRL3-32-1](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-1.jpg)

QEU:FOUNDER  ： “今回、単位空間用に選定した単語は以下の10個です。**ポジティブが5個、ネガティブが5個**になります。”

### ------
### positive words(5 pcs)
### great - 128
### amazing - 85
### delicious - 71
### friendly - 88
### nice - 85
### ------
### negative words(5 pcs)
### never - 77
### disappointed - 69
### bad - 57
### worst - 49
### too 49

QEU:FOUNDER ： “この単語のうち、１つが含まれる文を集めて単位空間を１つ作ります。すなわち、単語が10個なので、単位空間としての文章が10件できます。”

C部長  ： “やっぱり、T法(2)についてもうちょっと説明しておいたほうが・・・。”


**（T法(2)の概念図）**

![imageJRL3-32-2](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-2.jpg)

QEU:FOUNDER  ： “上図の青点群が**信号空間**とよばれます。T法は信号空間は3点以上あれば計算できます。一方、左下の赤点が**単位空間**であり、1点あれば計算できます。T法(2)というのは、単位空間が端になるので回帰分析用に使っても、精度がわるくて意味がないです。その様子が点線の広がりに現れています。”

**（T法(1)の概念図）**

![imageJRL3-32-3](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-3.jpg)

QEU:FOUNDER  ： “だから、昔は単位空間が信号空間の中心にあるT法(1)が良いとされていました。しかし、ディープラーニングの出現以降、回帰特性がT法(2)よりも少し良いだけのT法(1)に意味があるのかね？さらにいうと、「単位空間がバラツキの中間にある」というのは説明変数（X）がすべて連続数であることが前提になります。カテゴリ変数を扱ったとたんに「T法(1)の前提が崩れる」んですよ。**現在の科学技術の趨勢においてT法(1)の存在意義なんか無い**って・・・。”

C部長  ： “だから、**T法(2)をメトリックスの生成手段として割り切って使いたい**。ここでメトリックスとは感度（β）、SN比（η）のことだが・・・。”

## η　= (β:0点比例式の傾き)^2/(σ：0点比例式残差のばらつき)

QEU:FOUNDER  ： “つまり、βとηは極めて簡単な線形式のメトリックスです。これが今回の場合、単位空間を複数変えて出てきます。そして、原則として全ての単語について2つのメトリックスが出力されます。それでは、プログラムをドン！！”

```python
# ------
# Practical Text Classification With Python and Keras
# Applying T2 method as Embedding
# ------
from fastbook import *

filepath = './sentiment_analysis/yelp_labelled.txt'
df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\t')
print(df)

# ------
# データのスプリット(train-test)
from sklearn.model_selection import train_test_split

sentences = df['sentence'].values
y = df['label'].values

sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)
sentences_train[0:10]

# ------
# データのスプリット(train-positive, train-negative)
df_positive = df[df['label']==1]
sentences_positive = df_positive['sentence'].values

df_negative = df[df['label']==0]
sentences_negative = df_negative['sentence'].values
#sentences_negative[0:10]

# ------
# 信号空間: 文を確率に応じて、単純につなげて、「bootstrapping」を行う
# ------
# パラメタの設定
# probability　と logistic valueは21水準
arr_prob_signal  = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
            0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]
arr_logi_signal  = [-2.0,-1.2787,-0.9542,-0.7533,-0.6020,-0.4771,
            -0.3679,-0.2688,-0.1760,-0.0871,0,0.0871,0.1760,0.2688,
            0.3679,0.4771,0.6020,0.7533,0.9542,1.2787,2.0]
print("-------")
print("len_prob_signal: ",len(arr_prob_signal), " , arr_prob_signal: ",arr_prob_signal)
print("-------")
print("len_logi_signal: ",len(arr_logi_signal), " , arr_logi_signal: ",arr_logi_signal)

# ------
# create boot strapping for signal
def sentence_bootstrap_signal(idx):
    sentence = ""
    for i in range(20):
        if i < idx:
            sentence += np.random.choice(sentences_positive)
        else:
            sentence += np.random.choice(sentences_negative)
    return len(sentence), sentence

# boot strapping for signal
sentences_signal = []
logis_signal = []
for i, logi in enumerate(arr_logi_signal):
    for j in range(10):
        num_sentence, sentence = sentence_bootstrap_signal(i)
        sentences_signal.append(sentence)
        logis_signal.append(logi)
print(sentences_signal[0:10])
print(logis_signal)

# ------
# Tokenizerで語彙を数字化します
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=3000)
tokenizer.fit_on_texts(sentences_train)

# 信号空間用(for signal)
# 文を語彙番号ベクトルに変換する
Xtemp_signal = tokenizer.texts_to_sequences(sentences_signal)
# 固定長さに合わせる
Xsignal = pad_sequences(Xtemp_signal, padding='post', maxlen=maxlen)

# 語彙番号の最大値(マトリックスのコラム次元)
num_vcb_signal = np.max(Xsignal)
print(num_vcb_signal)

# 単位空間用(for tani)
Xtemp_positive = tokenizer.texts_to_sequences(sentences_positive)
X_positive = pad_sequences(Xtemp_positive, padding='post', maxlen=maxlen)
Xtemp_negative = tokenizer.texts_to_sequences(sentences_negative)
X_negative = pad_sequences(Xtemp_negative, padding='post', maxlen=maxlen)

# 生成された辞書のキー・リストをみる
arr_token_keys = list(tokenizer.word_index.keys())
#print("key of word index",arr_token_keys)

# 生成された辞書の値リストをみる
arr_token_values = list(tokenizer.word_index.values())
#print("values of word index",arr_token_values)
print("-------")
print(sentences_signal[2])
print(Xsignal[2])

# ------
# 信号空間のみ
# Y values for signal
nums_signal = []
for i in range(len(Xsignal)):
    nums_signal.append(len(Xtemp_signal[i]))
    logis_signal[i] = round(logis_signal[i]*len(Xtemp_signal[i])/200,3)
print(nums_signal)
print(logis_signal)

# ------
positive_words = ['great','amazing','delicious','friendly','nice']
negative_words = ['never','disappointed','bad','worst','too']

# ------
# 単位空間生成用のキーワードリストを生成する
positive_numbers = []
negative_numbers = []
for i in range(5):
    positive_numbers.append(tokenizer.word_index[positive_words[i]])
    negative_numbers.append(tokenizer.word_index[negative_words[i]])
print(positive_numbers)
print(negative_numbers)

# ------
# find keyword via number (for signal -> tani)
def find_keywords(Xsignal, key_number):
    arr_findwords = []
    for i in range(len(Xsignal)):
        val_findwords = 0
        for j in range(len(Xsignal[i])):
            if Xsignal[i, j] == key_number:
                val_findwords += 1
        arr_findwords.append(val_findwords)
    # -----
    val_count = 0
    for i in range(len(Xsignal)):
        if arr_findwords[i] > 0:
            val_count += 1
        
    return arr_findwords, val_count

# find keyword via number
# データの再結合(train-positive, train-negative)
X_combine = np.concatenate([X_positive, X_negative], axis=0)

# POSITIVE
for keyword in positive_words:
    arr_findwords, val_count = find_keywords(X_combine, tokenizer.word_index[keyword])
    print(keyword, val_count, arr_findwords)

# NEGATIVE
for keyword in negative_words:
    arr_findwords, val_count = find_keywords(X_combine, tokenizer.word_index[keyword])
    print(keyword, val_count)

# ------
# 単位空間: reference NOの2次元マトリックスを生成する
# ------
# パラメタの設定
# probability　と logistic valueは2水準
arr_prob_tani  = [1.0,0.0]
arr_logi_tani  = [2.0,-2.0]
print("-------")
print("len_prob_tani: ",len(arr_prob_tani), " , arr_prob_tani: ",arr_prob_tani)
print("-------")
print("len_logi_tani: ",len(arr_logi_tani), " , arr_logi_tani: ",arr_logi_tani)

# ------
# choose keyword via number for tani
def choose_keywords(X_combine, key_number):
    kcount = 0
    for i in range(X_combine.shape[0]):
        icount = 0
        for j in range(X_combine.shape[1]):
            if X_combine[i, j] == key_number:
                icount += 1
        if icount > 0:
            #print(X_combine[i,:10])
            if kcount == 0:
                mx_findwords = X_combine[i,:]
            else:
                #print(len(X_combine[i]))
                mx_findwords = np.vstack([mx_findwords,X_combine[i]])
            kcount += 1
    
    return mx_findwords, mx_findwords.shape

# ------
# create boot strapping for tani
def sentence_bootstrap_tani(flag, key_number):

    # データの再結合(train-positive, train-negative)
    #X_combine = np.concatenate([X_positive, X_negative], axis=0)
    mx_findwords, len_findword = choose_keywords(X_combine, key_number)
    for i in range(20):
        if i < len_findword[0]:
            if i == 0:
                mx_sentence = mx_findwords[i]
            else:
                mx_sentence = np.vstack([mx_sentence,mx_findwords[i]])
        else:
            seq_sentences = list(range(len(X_combine)))
            mx_sentence = np.vstack([mx_sentence,X_combine[np.random.choice(seq_sentences)]])
            
    return len(mx_sentence), mx_sentence

# ------
# boot strapping for tani
words_tani = []
tokens_tani = []
nums_tani = []
logis_tani = []
Xtani = np.zeros([10,num_vcb_signal])
# -----
# positive sentences
for i in range(5):
    num_sentence, mx_sentence = sentence_bootstrap_tani('positive', tokenizer.word_index[positive_words[i]])
    #print("---- {} ----".format(mx_sentence.shape))
    #print(mx_sentence)
    array_sentence = mx_sentence.flatten()
    array_sentence = array_sentence[array_sentence != 0]

    for j in range(len(array_sentence)):
        Xtani[i,j] = array_sentence[j]

    words_tani.append(positive_words[i])
    tokens_tani.append(tokenizer.word_index[positive_words[i]])   
    nums_tani.append(len(array_sentence))
    logis_tani.append(round(arr_logi_tani[0]*len(array_sentence)/1000,3))

# -----
# negative sentences
for i in range(5):
    num_sentence, mx_sentence = sentence_bootstrap_tani('negative', tokenizer.word_index[negative_words[i]])
    array_sentence = mx_sentence.flatten()
    array_sentence = array_sentence[array_sentence != 0]

    for j in range(len(array_sentence)):
        Xtani[i+5,j] = array_sentence[j]

    words_tani.append(negative_words[i])
    tokens_tani.append(tokenizer.word_index[negative_words[i]])   
    nums_tani.append(len(array_sentence))
    logis_tani.append(round(arr_logi_tani[1]*len(array_sentence)/200,3))

# -----
print(words_tani)
print(tokens_tani)
print(nums_tani)
print(logis_tani)
print("-----------")
print(Xtani)

# ------
# create　Tmethod matrix function (for signal and tani)
def create_matrix(mx_calc, X_calc):
    for i in range(len(X_calc)):
        for j in X_calc[i]:
            if j > 0 and j <= num_vcb_signal:
                mx_calc[i, int(j-1+0.01)] += 1
    return mx_calc

# T法(2)計算用マトリックスを生成する (for signal)
mXsignal = np.zeros([len(Xsignal), num_vcb_signal])
print("-----")
print(mXsignal.shape)

# call create_matrix for signal
mXsignal = create_matrix(mXsignal, Xsignal)
print("-----")
print(mXsignal[0:30, 0:30])

# T法(2)計算用マトリックスを生成する (for signal)
mXtani = np.zeros([len(Xtani), num_vcb_signal])
print("-----")
print(mXtani.shape)

# call create_matrix for signal
mXtani = create_matrix(mXtani, Xtani)
print("-----")
print(mXtani[0:30, 0:30])

# ------
# 信号データ数が5以上の項目だけを抽出する
# ------
# 信号空間について、探索
reduced_keys_signal = []
reduced_idxs_signal = []
for j, key in zip(arr_token_values, arr_token_keys):
    total_data = np.sum(mXsignal[:,j-1])
    if total_data > 5:
        #print(j, key, total_data)
        reduced_keys_signal.append(key)
        reduced_idxs_signal.append(j)
print(reduced_keys_signal)
print(reduced_idxs_signal)
print(len(reduced_idxs_signal))

# 信号、単位空間について、マトリックス削減
mx_reduced_signal = np.zeros([len(mXsignal),len(reduced_idxs_signal)])
mx_reduced_tani   = np.zeros([len(mXtani),len(reduced_idxs_signal)])
for i,j in enumerate(reduced_idxs_signal):
    mx_reduced_signal[:,i] = mXsignal[:,j-1]
    mx_reduced_tani[:,i]   = mXtani[:,j-1]
print(mx_reduced_signal)
print(mx_reduced_tani)

# ------
# T2メトリックス化を行う
# ------
num_vocab = mx_reduced_signal.shape[1]
print(num_vocab)

# ---------------------------
# YとXのノルム化
def calc_T2_norm(arr_Xtani, mx_Xsignal, val_Ytani, arr_Ysignal):

    #print(arr_Xtani)
    #print(mx_Xsignal)
    #print(val_Ytani)
    #print(arr_Ysignal)
    
    # 初期化
    mx_Xnorm  = mx_Xsignal.copy()
    arr_Ynorm = arr_Ysignal.copy()
    
    # 参照番号の文のトークンを抽出する
    for j in range(num_vocab):   # num_vocab
        mx_Xnorm[:, j] = mx_Xsignal[:, j] - arr_Xtani[j]
        #print("iCnt: {}, est_item: {}, value_item: {}".format(iCnt, est_item, value_item))
    arr_Ynorm = arr_Ysignal - val_Ytani
        
    return mx_Xnorm, arr_Ynorm

# -----
# 単位空間を選択する
i = 0
arr_Xtani = mx_reduced_tani[i]
val_Ytani = logis_tani[i]
mx_Xnorm, arr_Ynorm = calc_T2_norm(arr_Xtani, mx_reduced_signal, val_Ytani, np.array(logis_signal))
print(arr_Ynorm)
print("--------")
print(mx_Xnorm)

```

![imageJRL3-32-4](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-4.jpg)

D先生 ： “T法(2)では説明変数(X)と応答変数(Y)の**ノルム化（正規化）**が必要となります。”

C部長  ： “単に信号空間データから単位空間データを引いただけです。これから**M-X散布図というグラフ**が描けます。これは、中間評価手段としてとても便利なモノです。”

```python
# -----
# 散布図を描画する
# -----
import matplotlib.pyplot as plt
%matplotlib inline 

plt.figure(figsize=(12, 7))
plt.title("T2 data - negative(50-55)")
plt.scatter(arr_Ynorm, mx_Xnorm[:, 50]+0.4*np.random.rand()-0.2, label="word-{}".format(reduced_keys_signal[50]),alpha=0.5)
plt.scatter(arr_Ynorm, mx_Xnorm[:, 51]+0.4*np.random.rand()-0.2, label="word-{}".format(reduced_keys_signal[51]),alpha=0.5)
plt.scatter(arr_Ynorm, mx_Xnorm[:, 52]+0.4*np.random.rand()-0.2, label="word-{}".format(reduced_keys_signal[52]),alpha=0.5)
plt.scatter(arr_Ynorm, mx_Xnorm[:, 53]+0.4*np.random.rand()-0.2, label="word-{}".format(reduced_keys_signal[53]),alpha=0.5)
plt.scatter(arr_Ynorm, mx_Xnorm[:, 54]+0.4*np.random.rand()-0.2, label="word-{}".format(reduced_keys_signal[54]),alpha=0.5)
plt.scatter(arr_Ynorm, mx_Xnorm[:, 55]+0.4*np.random.rand()-0.2, label="word-{}".format(reduced_keys_signal[55]),alpha=0.5)
plt.legend()
plt.grid()
plt.show()

```

![imageJRL3-32-5](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-5.jpg)

QEU:FOUNDER  ： “このグラフの単語のほとんどはStopword(ポジでもネガでもない)ですが、ただ一つ**「never」だけがかなり強いネガティブ特性**を持っているはずです。”

C部長 ： “ たしかに、「never」の分布にはマイナス傾きの0点比例式の傾向が見られます。”

QEU:FOUNDER  ： “これから、いよいよメトリックス（βとη）の生成に行きましょう。”

```python
# ---------------------------
# T2メトリックス計算の関数定義
def calc_metricsT2m(len_signal, num_vocab, mx_Xnorm, arr_Ynorm):
    # -----
    # 感度とSN比
    arr_beta    = np.zeros(num_vocab)
    arr_snr     = np.zeros(num_vocab) 
    # -----
    # 有効除数の計算
    yuko = 0.0
    for jCol in range(len_signal):
        yuko += float(arr_Ynorm[jCol]) ** 2
    #print('有効除数=:{0:.4f}'.format(yuko))
    # -----
    if yuko > 0.0:
        for iRow in range(num_vocab):
            # -----
            # 線形式の計算
            sum_array = 0.0
            for jCol in range(len_signal):
                sum_array += float(arr_Ynorm[jCol]) * mx_Xnorm[jCol, iRow]
                #print('配列ix={0},jy={1}の合計値{2}'.format(iRow,jCol,sum_array))
            lnr_yvalue = sum_array
            beta_yvalue = sum_array / float(yuko)
            # -----
            # 全変動ST及び各種中間指標SB,SE,VE,η
            sum_array = 0.0
            for jCol in range(len_signal):
                sum_array += float(mx_Xnorm[jCol, iRow]) ** 2
                # print('配列i={0},jy={1}の合計値{2}'.format(iRow,iRow,sum_array))
            st_yvalue = sum_array
            sb_yvalue = lnr_yvalue ** 2 / float(yuko)
            se_yvalue = st_yvalue - sb_yvalue
            # -----
            ve_yvalue = se_yvalue / float(num_vocab - 1.0)
            # -----
            # 代入
            arr_beta[iRow]  = round(beta_yvalue, 8)
            if se_yvalue > 0.0001:
                arr_snr[iRow]   = round(sb_yvalue / se_yvalue, 6)
            else:
                arr_snr[iRow]   = 0
        return arr_beta, arr_snr
    else:
        # -----
        # 異常処理
        return [0], [0]

# ---------------------------
# 単位空間毎の感度とSN比を計算する
len_tani   = mx_reduced_tani.shape[0]
len_signal = mx_reduced_signal.shape[0]
mx_beta = np.zeros([len_tani,num_vocab])     # 単位空間数(10) x ボキャブラリ数
mx_snr  = np.zeros([len_tani,num_vocab])     # 単位空間数(10) x ボキャブラリ数
print(mx_beta)

# -----
# 単位空間を選択する
for i in range(len_tani):

    arr_Xtani = mx_reduced_tani[i]
    val_Ytani = logis_tani[i]
    mx_Xnorm, arr_Ynorm = calc_T2_norm(arr_Xtani, mx_reduced_signal, val_Ytani, np.array(logis_signal))
    #print(arr_Ynorm)
    #print("--------")
    #print(mx_Xnorm)

    # T2メトリックスを生成する
    arr_beta, arr_snr = calc_metricsT2m(len_signal, num_vocab, mx_Xnorm, arr_Ynorm)
    print("--- iTani: {} ---".format(i))
    print("arr_beta: ", arr_beta)
    print("arr_snr: ", arr_snr)
    # -----
    mx_beta[i,:] = arr_beta
    mx_snr[i,:]  = arr_snr

# -----
# SN比を修正する(マイナス符号付き)
mx2_snr = mx_snr
for i in range(len_tani):
    for j in range(num_vocab):
        if mx_beta[i,j] < 0:
            mx2_snr[i,j]  =  -1 * mx_snr[i,j]

# -----
# データフレーム化
mx_out = np.concatenate([mx_beta.T,mx2_snr.T],axis=1)
#print(mx_out)

arr_columnA = ["beta{}".format(i) for i in range(10)]
arr_columnB = ["snr{}".format(i) for i in range(10)]
arr_column  = arr_columnA + arr_columnB
df_out = pd.DataFrame(mx_out, columns=arr_column)
df_out['key'] = reduced_keys_signal
df_out['idx'] = reduced_idxs_signal
print(df_out)

```

![imageJRL3-32-6](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-6.jpg)

D先生 ： “あれ？SN比に面白い処理をしていますね。SN比はマイナスになることはないのに・・・。”

QEU:FOUNDER  ： “感度（β）の傾きを見て、**SN比にマイナスを付加**しているんです。ご存じのように、**「符号は強力な情報」**ですからね。さらに、SN比の定義「η=β/σ」より、ηにはすでにβの要素も入っているので**符号だけを移植**したんです。”

D先生 ： “面白い。よっしゃ、これで解析も終わりだ・・・。”

QEU:FOUNDER  ： “センセ、センセ・・・。10次元のSN比マトリックスを見てどうやって結果の検証をするんですか？最後に、**PCA（主成分分析）**をつかって散布図グラフを出力しましょう。”

```python
# -----------
# PCAを計算する
from sklearn.decomposition import PCA
import math

# データの読み込み
df_pca = df_out.loc[:,"snr0":"snr9"]
print(df_pca)

# 変数の標準化
df_pca_std = df_pca.apply(lambda x: (x-x.mean())/x.std(), axis=0)
print(df_pca_std)

# 主成分分析の実行
pca = PCA()
pca.fit(df_pca_std)

# データを主成分に変換
pca_row = pca.transform(df_pca_std)

# 主成分得点を求める
plt.figure(figsize=(12, 7))

x = pca_row[:, 0]
y = pca_row[:, 1]

for j in range(num_vocab):
    if x[j] < 0:
        x[j]  =  -1 * math.sqrt(abs(x[j]))
    else:
        x[j]  =  math.sqrt(x[j])
    # ----
    if y[j] < 0:
        y[j]  =  -1 * math.sqrt(abs(y[j]))
    else:
        y[j]  =  math.sqrt(y[j])

plt.scatter(x, y, alpha=0.6)
plt.grid()
plt.xlabel("PC1")
plt.ylabel("PC2")
 
annotations = df_out.loc[:,"key"].values
for i, label in enumerate(annotations):
    if x[i] > -1.2 and x[i] < 1.2 and y[i] > -1.2 and y[i] < 1.2:
        continue
    else:
        plt.annotate(label, (x[i], y[i]))
plt.show()

```

![imageJRL3-32-7](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-7.jpg)

QEU:FOUNDER  ： “**PC1（第一次主成分）がマイナスではポジティブ、プラスではネガティブになっています**ね。PC2（第二次主成分）はなんだろう？”

D先生 ： “PC2が大きいほど、良しあし(ネガ・ポジ)をダイレクトに評価しているような気がします。”

QEU:FOUNDER  ： “つまり、このデータを**Embeddingの値に使おうと思います**。前回のEmbeddingは乱数値なので、それよりはマシでしょう。”

D先生 ： “前回は予測精度が80％程度でしたから、85％までいけたら合格としましょう。”


## ～　まとめ　～

### ・・・　今年は大変な年になりそうです　・・・

QEU:FOUNDER ： “人生、長生きすれば、いろんなこともあるもんだ・・・。今回のニュースには率直にいって、ひっくり返るくらい驚きました。”

[![MOVIE1](http://img.youtube.com/vi/6GvB5lZJqcE/0.jpg)](http://www.youtube.com/watch?v=6GvB5lZJqcE "Jeremy Howard demo for Mojo launch")

C部長 : “わざわざ、御大のJeremy Howardがお出ましか・・・。それにしても、**Python言語ベースがC言語なみに速く動作する**のは大変な魅力ですね。機械学習にテクノメトリックスを使うために、わざわざ速く動作するJulia言語を勉強したFounder・・・。う～ん、残念・・・（笑）。”

QEU:FOUNDER ： “いや、Julia言語を使うことで問題点が良く見えてきたから・・・。小生も、できればPython言語を使いたかったんです。ですから、これは良い知らせ！！ただし、1年ぐらいたたないと、本当のところはわからないでしょうけどね。あとね、とうとう3次元のStable_Diffusionが現れたらしい・・・。”

![imageJRL3-32-8](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-8.jpg)

C部長 : “(Stable_Diffusionの3次元化は)時間の問題だと思っていましたがね。ただ、どんなデータで学習したんだろうか・・・。2次元（画像）のデータ？それとも、3Dモデルのデータ？”

![imageJRL3-32-9](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-9.jpg)

QEU:FOUNDER ： “アーカイブ(↑)によると3次元のデータのようです。もしも、2次元データから3次元データが生まれれば画期的なのだが・・・。まあ、それも時間の問題と言えるが・・・。さらに、いよいよChatBOTの**FOSS時代**の幕が開けました。もう時代の流れは速くって、モデルの主流は**GPTからLLaMAに移る**でしょうね。”

![imageJRL3-32-10](/2023-05-08-QEUR23_SNLPS12/imageJRL3-32-10.jpg)

C部長 : “**FOSS**って何？”

D先生： “**フリーでオープンなソフトウェア**です。LLaMAはGPTよりも軽量で、個人でもファインチューンが可能です。いよいよ、**個人が自分でトレーニングしたChatBOTを持つ**ようになるでしょう。はっきりいって、1年以内に始まるよ・・・。”

[![MOVIE1](http://img.youtube.com/vi/4L27X78UOkU/0.jpg)](http://www.youtube.com/watch?v=4L27X78UOkU "【ChatGPTでコンサルは消える？】コンサルバブル大崩壊／泥臭い経験こそ武器になる／Google登場以上のインパクト／人間はAIの伝達役になる／デスクワークの価値がなくなる")

QEU:FOUNDER ： “小生は以前、言ってたでしょ？すぐにそうなるって・・・。”

D先生 : “サルさんは、これから大変ですね。”

QEU:FOUNDER ： “**人間にならなきゃいけない**から・・・(笑)。”


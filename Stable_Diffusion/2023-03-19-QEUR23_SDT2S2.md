---
title: QEUR23_SDT2S2:　展覧会の絵(その2, including img2img)
date: 2023-03-19
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_SDT2S2:　展覧会の絵(その2, including img2img)

## ～　安心してください、あなたは「とても創造的」です　～

### ・・・　前回の続きです　・・・

D先生 ： “前回のFOUNDERの話によれば、今回は画像をインプットし、それを加工して修正された画像を出力するという「img2imgシステム」について紹介するんでしたよね。”

*** ・・・・　＜前回ルックバック：はじめ＞　・・・・ ***

QEU:FOUNDER ： “少なくとも、小生がこれから作るシステムは「自分が好きなアートを見出すため」に役立つものです。ちなみにね、いま、いろいろprompt engineeringのノウハウをブログに書いている人がいます。しかし、見た目だけ良いアートを作るのは極めてカンタンです。CLIP-interrogationとimg2imgのシステムを繰り返し使っていけば、仕上がりが自然によくなります。”

*** ・・・・　＜前回ルックバック：おわり＞　・・・・ ***


QEU:FOUNDER  ： “それはそうなんですが、システム紹介の前に展覧会をしましょうよ。今回はまとめ画像ではなく**「単品」の代表作**を少し見てみましょう。まずはこれから・・・。”

**(ShigenoriSoejima-JeanNouvel)**

![imageJRL3-3-1](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-1.png)

**(TatsunokoProduction-RinkoKawauchi)**

![imageJRL3-3-2](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-2.png)

QEU:FOUNDER  ： “まとめ画像で見るよりも、単品画像で見るほうが恰好がいいよね。アニメ系のついてに、浮世絵系もみてみましょうか・・・。”

**(RichardDadd-Hokusai)**

![imageJRL3-3-3](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-3.png)

**(VictorNizovtsev-Hokusai)**

![imageJRL3-3-4](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-4.png)

D先生 ： “さすがに格好がいいですね。ここで、洋物の画像も鑑賞しましょう。”

**(MiltonCaniff-RinkoKawauchi)**

![imageJRL3-3-5](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-5.png)

**(LawrenceAlmaTadema-StephanMartiniere)**

![imageJRL3-3-6](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-6.png)

QEU:FOUNDER ： “あのイラストは**「いかにも古き良きA国っぽくて好き」**なんだなぁ・・・。ちなみに、単品画像の鑑賞会はこれで終わりです。今回は、img2imgシステムの紹介がメインだから・・・。”

D先生 ： “せっかくきれいな画像を見れたんだし、もうちょっとエンジョイしたかった・・・。”

QEU:FOUNDER ： “今回、あえて単品画像を見せた目的は、実は**このような画像がシステムのインプットとなる**ことを例示したかったためです。当たり前ながら、AIで良い作品を効率よく（不良率低く）生成したいと思えば、過去に生成したよさげな画像をインプットしたくなるよね。”

D先生 ： “それは自然な発想です。”

QEU:FOUNDER ： “別に、これほどキレイな画像でなくともユーザーが作った適当なスケッチでも十分に反応します。それでは、プログラムを使って動かしてみましょう。いままでtext2imgシステムを少しだけ改造すればできますよ。”

```python
# -----
import pandas as pd
import numpy as np
from datetime import datetime
from PIL import Image
from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, EulerDis-creteScheduler

# introduce an initial image
init_image = Image.open("testimage.png").convert("RGB")
display(init_image)

# --- STABLE DIFFUSION <START> ---
import torch
from torch import autocast
import ipywidgets as widgets
from ipywidgets import interact

%cd ~/../notebooks

## Use the lists below to select our model type, image output resolution, and the computer number format for inference
device = 'cuda'
## We recommend either v1-5 (index 1) or v2 (index 2)
#model_id = ['hakurei/waifu-diffusion',"../datasets/stable-diffusion-diffusers/stable-diffusion-v1-5/",'../datasets/stable-diffusion-diffusers-v2/stable-diffusion-2/']
model_id = ['./model/vae/waifu-diffusion',"../datasets/stable-diffusion-diffusers/stable-diffusion-v1-5/",'../datasets/stable-diffusion-diffusers-v2/stable-diffusion-2/']

## We strongly recommend using the v1-5 and v1-4 models with size 512, and the v2 models with size 768.
size = [512, 768]
## If you are using a GPU with ~ 8 GBs of RAM, like the Free-GPU Notebooks, then use torch.float16
## otherwise, either selection works
dtype = [torch.float16, torch.float32]

# Set your values here - defaults are for v2, size 768 x 768 with FP16 computer number format
model_ = model_id[1]
size_ = size[0] 
precision = dtype[0]

# -----
# Depending on the selections made above, the type of model loaded will change using this short if/else statement
print(f'Now loading Stable Diffusion {model_} model ({precision})...')
scheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfold-er="scheduler")
#scheduler = LMSDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfold-er="scheduler")
# -----
if precision == torch.float16:
    if model_ == '../datasets/stable-diffusion-diffusers-v2/stable-diffusion-2/':
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_, torch_dtype=torch.float16, revision="fp16", scheduler = scheduler)
        pipe = pipe.to(device)
    else:
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_, torch_dtype=torch.float16, revision="fp16")
        pipe = pipe.to(device)
else:
    if model_ == '../datasets/stable-diffusion-diffusers-v2/stable-diffusion-2/':
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_, scheduler = scheduler)
        pipe = pipe.to(device)
    else:
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_)
        pipe = pipe.to(device)
print('Model now loaded. Head to next cell to generate images.')

# -----
# create grid function
def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size
    
    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid

# -----
# parameters
rows = 2
cols = 3
model_name = "SDV14"
sample_num = rows*cols
lst = []
print(f'Generating samples from Stable Diffusion {model_} checkpoint ({precision})')

# -----
# プロンプトを生成する
str_goodness   = "Tatsunoko Productions"
str_landscape  = "Rinko Kawauchi"
prompt   = '((1girl:1.3),blue eye,in the style of {0},standing,looking at viewer), (beach, in the style of {1}:1.1),high quality'.format(str_goodness, str_landscape)
n_prompt = "low quality, BW, bad anatomy, deformed, signature"
# -----
generator = torch.Generator("cuda").manual_seed(0)  

# -----
for j in range(sample_num):
    a = pipe(prompt, negative_prompt=n_prompt, generator=generator, 
             guidance_scale=8.0,
             image=init_image, 
             strength=0.35)['images'][0]
    lst.append(a)
    display(a)

    #生成日時をファイル名にして保存
    date = datetime.now().strftime("%Y%m%d_%H%M%S")
    fig_imgname = './outputs/gen-image-{0}-{1}-p{2}.png'.format(date,model_name,j)
    print("Individual picture - {} is done".format(fig_imgname))
    display(a)
    a.save(fig_imgname)

# -----
# create grid
grid = image_grid(lst, rows, cols)
date = datetime.now().strftime("%Y%m%d_%H%M%S")
fig_lstname = './outputs/grid-image-{0}-{1}.png'.format(date,model_name)
print("Grid of pictures - {} is done".format(fig_lstname))
# you can save the grid with
display(grid)
grid.save(fig_lstname)

```

QEU:FOUNDER ： “前回と同様に、**「女の子と犬が海岸のそばを歩いている画像」**を入力します。”

**(入力画像と修正画像)**

![imageJRL3-3-7](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-7.png)

D先生 ： “あれ？ここで**「strengthというパラメタ」**ってなんですか？”

QEU:FOUNDER ： “これは**「画像修正の強さ」**を表します。今回の場合には0.35なので修正はごくわずかです。この写真には女の子と犬がいます。しかし、プロンプトには犬はありません。つまり、修正が強くなればなるほど犬が消えていきます。”

D先生 ： “ああ、1枚の画像で**犬が消えた**のはそういうワケね・・・。”

QEU:FOUNDER ： “でもね・・・。それでも**「構図」は継承されるか**ら、インプット画像はないよりもよっぽどいいですよ。なによりも、我々のような「ヘタクソな絵描き」でもアウトプットはステキな絵になります。”

D先生 ： “それは助かります・・・。”

QEU:FOUNDER ： “うまくなったら、strengthを少しづつ変更すればいいんですよ。”

**(FrankCho-ToddHido)**

![imageJRL3-3-8](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-8.png)

D先生 ： “イラストづくりなんか、らくちんらくちん・・・(笑)。”

QEU:FOUNDER ： “これからは、**世の中の「創造性の定義」が少しづ変化してくる**と思いますよ。そもそも考えてみてください。Text2imgとimg2imgを組み合わせて画像を作ると、まさに**「無限に多様な絵画」**をかけるんですよ。そうすると、「自分が作った絵はオリジナルである」と言わざると得ないよね。さて・・・。是非、カンパをください・・・。”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ： “つぎは、img2imgで展覧会をやりましょう。”


## ～　まとめ　～

C部長 : “FOUNDER、ここで質問があります。なぜ、QEUシステムのアウトプットには**「単品画像とまとめ画像」**の2種類があるんですか？”

**（単品画像）**

![imageJRL3-3-9](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-9.png)

**（まとめ画像の例）**

![imageJRL3-3-10](/2023-03-19-QEUR23_SDT2S2/imageJRL3-3-10.png)

QEU:FOUNDER ： “単品画像はもちろん観賞用になります。同時に、img2imgのインプットになりますがね・・・。一方、まとめ画像は**パラメタの特性を把握するためのサンプリングデータ**です。それにしても、出力には額縁がある画像が多いんだよね。これには困った・・・。”

C部長 : “たぶん、関数を学習するときに額縁付きの画像を入力するんでしょうね。”

QEU:FOUNDER ： “もしも、**あの額縁がなければ作品の品質が一気に上がる**んだが・・・。”

C部長 : “まあ、現在のところは研究用として発展途上のシステムなので、いづれは改善するんじゃないですか？”

[![MOVIE1](http://img.youtube.com/vi/t2SisAVaS-w/0.png)](http://www.youtube.com/watch?v=t2SisAVaS-w "Predator – BBC Documentary on Johnny Kitagawa by Mobeen Azhar & Megumi Inman")

QEU:FOUNDER ： “そうであってほしいが・・・。**「いずれはいずれは・・・」で、全く改善させずに今日に至った事例もある**からね。”

C部長 : “むしろ当事者以外が改善に熱心という・・・。”

QEU:FOUNDER ： “ああ・・・、恥ずかしい・・・(笑)。”




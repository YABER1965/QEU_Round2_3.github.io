---
title: QEUR23_SNLPS11:　簡単なNLP「続編」 (yelp review - その2) 
date: 2023-05-05
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_SNLPS11:　簡単なNLP「続編」 (yelp review - その2) 

## ～　T法(2)の用途は意外に深いかも・・・　～

QEU:FOUNDER  ： “以上で序盤の紹介はおわりです。前回のトライアルでわかったように、Embeddingには課題があります。それを何とか解決したい。”

```python
# ------
# Embeddingレイヤーの出力を、Denseレイヤーに差し込むには、Denseレイヤーのための連続した
# 入力を準備するFlattenレイヤーを間に追加する必要があります
# これらのEmbedding層の重みはランダムな重みで初期化され、学習時にバックプロパゲーションで調整されます。
from keras.models import Sequential
from keras import layers

embedding_dim = 50
model = Sequential()
model.add(layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=maxlen))
model.add(layers.Flatten())
model.add(layers.Dense(30, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.summary()
```

D先生 ： “そもそもGloVeなどの既成の言語データベースを使えばいいじゃないですか？わざわざ、それを**カスタムメイド**で作らなくとも・・・。”

QEU:FOUNDER  ： “Embeddingは、別に言語だけで作われているのではないです。たとえば（↓）・・・。”

![imageJRL3-31-1](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-1.jpg)

QEU:FOUNDER  ： “化学や高分子の世界とか、**いろいろ応用分野はある**と思いますよ。そんなモノに使えるような「汎用的な巨大データベースはまだない」と思うし・・・。”

C部長  ： “なるほどね・・・。”

QEU:FOUNDER  ： “これからT法(2)を使うためのデータを生成しましょう。前回と同じレストランの口コミ（yelp）データを使います。”

C部長  ： “う～ん・・・。T法(2)って皆さんはわかるかなぁ・・・。”

[![imageJRL3-31-2](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-2.jpg)](https://jpnqeur23sdttwo.blogspot.com/2023/03/qeur23snlps4t2-sentiment-analysis.html "T法(2)による Sentiment Analysis (Yelp_Review、その1)")

QEU:FOUNDER  ： “昔、すでに説明した***（↑CLICK）***ので細かくはやりません。・・・でも、あえて簡潔に説明すると、T法は手間こそは少しだけかかるが、少ないデータ数で解析できるので良い方法だと思います。また、この方法を回帰分析の代替だと考えるから使いづらいのであって、メトリックスの生成手段だと考えるべきだと思います。それではプログラムと結果をドン！！”

```python
# Practical Text Classification With Python and Keras
from fastbook import *

filepath = './sentiment_analysis/yelp_labelled.txt'
df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\t')
print(df)
```

![imageJRL3-31-3](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-3.jpg)

```python
# ------
# データのスプリット(train-test)
from sklearn.model_selection import train_test_split

sentences = df['sentence'].values
y = df['label'].values

sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)
sentences_train[0:100]

# ------
# データのスプリット(train-positive, train-negative)
df_positive = df[df['label']==1]
#df_positive.head()
sentences_positive = df_positive['sentence'].values
sentences_positive[0:10]

# ----
df_negative = df[df['label']==0]
sentences_negative = df_negative['sentence'].values
sentences_negative[0:10]

# ------
# 信号空間: 文を確率に応じて、単純につなげて、「bootstrapping」を行う
# ------
# パラメタの設定
# probability　と logistic valueは21水準
arr_prob_signal  = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
            0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]
arr_logi_signal  = [-2.0,-1.2787,-0.9542,-0.7533,-0.6020,-0.4771,
            -0.3679,-0.2688,-0.1760,-0.0871,0,0.0871,0.1760,0.2688,
            0.3679,0.4771,0.6020,0.7533,0.9542,1.2787,2.0]
print("-------")
print("len_prob_signal: ",len(arr_prob_signal), " , arr_prob_signal: ",arr_prob_signal)
print("-------")
print("len_logi_signal: ",len(arr_logi_signal), " , arr_logi_signal: ",arr_logi_signal)

# ------
# create boot strapping for signal
def sentence_bootstrap_signal(idx):
    sentence = ""
    for i in range(20):
        if i < idx:
            sentence += np.random.choice(sentences_positive)
        else:
            sentence += np.random.choice(sentences_negative)
    return len(sentence), sentence

# boot strapping for signal
sentences_signal = []
nums_signal = []
logis_signal = []
for i, logi in enumerate(arr_logi_signal):
    for j in range(10):
        num_sentence, sentence = sentence_bootstrap_signal(i)
        #print(num_sentence, logi, sentence)
        sentences_signal.append(sentence)
        nums_signal.append(len(sentence))
        logis_signal.append(round(logi*len(sentence)/1000,3))
print(sentences_signal[0:10])
print(nums_signal)
print(logis_signal)

```

![imageJRL3-31-4](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-4.jpg)

D先生 ： “応答変数(Y)の値が、文の長さで調整されていますね。”

C部長  ： “**加法モデル**なので、このように補正しないと結果がおかしくなるでしょうね。おっと、ボケ訳なのに、かっこいいことを発言してしまった。”

```python
# ------
# Tokenizerで語彙を数字化します
# 信号空間と単位空間用
from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=3000)
tokenizer.fit_on_texts(sentences_train)

# signal-taniに対して処理する
X_signal = tokenizer.texts_to_sequences(sentences_signal)
#X_tani   = tokenizer.texts_to_sequences(sentences_tani)

vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index
print("-------")
print(sentences_signal[2])
print(X_signal[2])
```

![imageJRL3-31-5](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-5.jpg)

```python
# ------
# 生成された辞書をみる
# The word_index assigns a unique index to each word present in the text. 
# This unique integer helps the model during training purposes.
print("The word index",tokenizer.word_index)

```

![imageJRL3-31-6](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-6.jpg)

```python
# ------
# 生成された辞書のキー・リストをみる
arr_token_keys = list(tokenizer.word_index.keys())
#print("key of word index",arr_token_keys)

# 生成された辞書の値リストをみる
arr_token_values = list(tokenizer.word_index.values())
#print("values of word index",arr_token_values)

# ------
# 文から語彙マトリックスを生成します
# 信号空間と単位空間用
from keras_preprocessing.sequence import pad_sequences

maxlen = 1500
X_signal = pad_sequences(X_signal, padding='post', maxlen=maxlen)
X_tani  = pad_sequences(X_tani, padding='post', maxlen=maxlen)
print(X_signal[:50, :30])

# ------
# 語彙番号の最大値
num_vcb_signal = np.max(X_signal)
print(num_vcb_signal)
# 1746
```

![imageJRL3-31-7](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-7.jpg)

```python
# ------
# マトリックスを生成する (for signal)
mx_signal = np.zeros([len(X_signal), num_vcb_signal])
print(mx_signal.shape)

# create_matrix function (for signal)
def create_matrix(mx_signal, X_signal):
    for i in range(len(X_signal)):
        for j in X_signal[i]:
            if j > 0 and j <= num_vcb_signal:
                mx_signal[i, j-1] += 1
    return mx_signal

# call create_matrix for signal
mx_signal = create_matrix(mx_signal, X_signal)
print(mx_signal[0:30, 0:30])
```

![imageJRL3-31-8](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-8.jpg)

QEU:FOUNDER  ： “これでT法(2)を計算できるような「文毎に集計された語彙発生頻度マトリックス」が生成されます。マトリックスの項目数（語彙の数）は前回と同様に1714件です。”

C部長 ： “これは信号空間です。同様に単位空間のマトリックスは作らないんですか？”

QEU:FOUNDER  ： “いや、今回は単位空間を生成するための準備段階です。つづきに行きましょう。”

```python
# ------
# 語彙マトリックスを生成する(train-positive, negative)
maxlen = 500
X_positive = tokenizer.texts_to_sequences(sentences_positive)
X_positive = pad_sequences(X_positive, padding='post', maxlen=maxlen)
X_negative = tokenizer.texts_to_sequences(sentences_negative)
X_negative = pad_sequences(X_negative, padding='post', maxlen=maxlen)
print(X_positive[:50, :30])

# ------
# 語彙マトリックスを生成する(train-positive)
mx_positive = np.zeros([len(X_positive), num_vcb_signal])
print(mx_positive.shape)

# call create_matrix for positive
mx_positive = create_matrix(mx_positive, X_positive)
print(mx_positive[0:30, 0:30])

# ----
# 語彙マトリックスを生成する(train-negative)
mx_negative = np.zeros([len(X_negative), num_vcb_signal])
print(mx_negative.shape)

# call create_matrix for negative
mx_negative = create_matrix(mx_negative, X_negative)
print(mx_negative[0:30, 0:30])

```

![imageJRL3-31-9](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-9.jpg)

```python
# ------
# 単位空間の選定準備
# ------
# データ数が８以上、かつ偏向しているモノだけを抽出する
arr_freq_positive = []
arr_freq_negative = []
arr_freq_diff = []
arr_reduced_keys = []
arr_reduced_idxs = []
for j, key in zip(arr_token_values, arr_token_keys):
    val_freq_positive = np.sum(mx_positive[:, j-1])
    val_freq_negative = np.sum(mx_negative[:, j-1])
    val_freq_diff = val_freq_positive - val_freq_negative
    # ---
    val_freq_all = val_freq_positive + val_freq_negative
    if val_freq_all > 8 and abs(val_freq_diff)/val_freq_all > 0.3:
        #print(j, key, val_freq_positive, val_freq_negative, val_freq_diff, )
        arr_freq_positive.append(val_freq_positive+1)
        arr_freq_negative.append(val_freq_negative+1)
        arr_freq_diff.append(val_freq_diff)
        arr_reduced_keys.append(key)
        arr_reduced_idxs.append(j)
print(arr_reduced_keys)
print(arr_freq_diff)
print(len(arr_freq_diff))

```

![imageJRL3-31-10](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-10.jpg)

QEU:FOUNDER  ： “口コミがポジティブか、それともネガティブかを比較できるように語彙別リストを生成します。もちろん、ポジでもネガでもない語彙（Stop Word）は解析には意味がないので、**それらを削除してリストを圧縮します**。最後に、グラフを使って全体を把握しましょう。”

```python
# ------
# アノテーション付き散布図
import matplotlib.pyplot as plt
 
# データをを生成
x = np.log10(arr_freq_positive)
y = np.log10(arr_freq_negative)
s = arr_reduced_keys

# 散布図を描画
plt.figure(figsize = (14, 9))
plt.scatter(x, y, s=80, c="blue", marker="*", alpha=0.5)

# アノテーション
for key, posx, posy in zip(s,x,y):
    plt.annotate("{}".format(key), (posx, posy))

plt.xlabel("freq-positive")
plt.ylabel("freq-negative")
plt.grid(True)
plt.show()
```

![imageJRL3-31-11](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-11.jpg)

QEU:FOUNDER  ： “グラフ上に文字を書き込んでいる(annotation)のでわかりやすいでしょ？”

D先生 ： “グラフ左上に近いプロットはネガティブな語彙です。一方、右下に近いプロットはポジティブな語彙です。「なるほど・・・」と思うような語彙が並んでいます。”

QEU:FOUNDER  ： “この知見を使って単位空間を生成します。次に進みましょう。”



## ～　まとめ　～

### ・・・　創発について、またまた前回のつづきです　・・・

QEU:FOUNDER ： “この記事(↓)はニューラルネットを大きくしていく**突然に創発的な（または～に見える）機能が発揮される**ことについて論じています。しかし、それがなぜ起こるのかは著者にもよく分からないようです。だから、「もっとシステムを大きくしたらどうか」が結論のようですが・・・。小生、個人的には、他にもマルチモーダル(テキストと画像を学習させる)などのやり方が重要と思いますがね・・・。”

![imageJRL3-31-12](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-12.jpg)

C部長 : “そもそも、**「創発とはなにか？」という定義が重要だ**と思いますが・・・。”

![imageJRL3-31-13](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-13.jpg)

QEU:FOUNDER ： “さすが！最近のC先生はキレが違います。さて、テスト方法の紹介をチラッと覗いたところでは、それらは簡単なタスクではあるんですが、本来はChatGPTに期待していなかったモノ（機能）のようです。でも、我々としては当然にこととして、「創発=創造的(Creative)」と考えますよね。そして、この記事の著者は、この唐突に創発する挙動は**「二項分布的である」**と論じております。久々に統計R言語を使って遊んでみました。エイっ！！”

![imageJRL3-31-14](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-14.jpg)

C部長 : “つまり、**創発の傾向は成功率(ｐ)が「極めて高い(p>0.9)」の二項分布に相当する**と考えているのですね・・・。これは逆に言うと、グラフ横軸のスコアポイント(「スコア=モデルのサイズ?」)が極めて高い時に初めて創発する機能が出てくると・・・。”

QEU:FOUNDER ： “二項分布はｐが極めて低くなると欠陥分布と言われるポアソン分布になります。すなわり、今回の場合には**「逆ポアソン分布」**ですね。なんか、この創発を「逆欠陥管理」として考えると、小生はちょっと腑に落ちないんだけどね。”

C部長 : “またまた難しそうな話を・・・。そんなのはＤ先生とやってくださいな。ちょっと呼んできます・・・(笑)。”

D先生： “・・・呼ばれてきました（笑）。なんかお呼びでしょうか？”

![imageJRL3-31-15](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-15.jpg)

QEU:FOUNDER ： “現在のところ、LLM（大規模言語モデル）の発展は**Scaling_Law(大きければ強い)に従っている**といわれます。この著者も、それを踏まえてモデルを大きくすれば、AIは「より強く創発する」と考えていますが・・・。本当にそうなのか？彼が逆品質理論をベースに論じているのが気になる・・・。”

D先生 : “**欠陥管理、品質理論は合格状態は１つのみで、それ例外は全て不良とすることを前提とします**。”

QEU:FOUNDER ： “しかしながら、創発、すなわち**クリエーションのモードは無限にある**でしょ？”

C部長 : “あっ！そんなことを考えていたの・・・！？”

![imageJRL3-31-16](/2023-05-05-QEUR23_SNLPS11/imageJRL3-31-16.jpg)

QEU:FOUNDER ： “我々の実感として、もし創発したいんだったら、他にいろいろなやり方があるべきなんじゃないかな？仮に特許出願がクリエーションであると考えてみましょう。組み合わせ特許というモノがありうるので、システムに特許を組み合わせて、今までにない特許を作ってくれと言えばいいんじゃないか？最近、G社とやめたAIの大御所の発言も気になるし・・・。”

C部長 : “あれ？G社ってAI方面では落ち目じゃなかったの？”

QEU:FOUNDER ： “小生は、もともと、そうは思っていなかったので・・・。たぶん彼は知っているんですよ、すごいことを・・・。おそらく、それはScaling_Lawの事じゃないと思いますよ。”


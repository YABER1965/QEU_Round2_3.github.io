---
title: QEUR23_SNLPS10:　簡単なNLP「続編」 (yelp review - その1) 
date: 2023-05-02
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_SNLPS10:　簡単なNLP「続編」 (yelp review - その1) 

## ～　またまた、T法（２）の話に戻りますが・・・　～

QEU:FOUNDER  ： “さて、T法にゆっくり話を戻したいが、その前に**KerasによるSentiment_analysis予測**をやります。”

D先生 ： “あれ？ソレって、ちょっと前にやりましたよねえ。あまりよい結果にはならなかったでしたが・・・。”

QEU:FOUNDER  ： “今回は別の観点を使ってやります。さて、ディープラーニングを使った言語データを処理（NLP）するための基本をみてみましょう。ちょうど、よいブログ（↓）を見つけたし・・・。”

![imageJRL3-30-1](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-1.jpg)

C部長  ： “あれ？PyTorchを使わないんですか？”

QEU:FOUNDER  ： “小生の持論は、なにはともあれ**「迷ったらKeras」**です・・・（笑）。KerasはPyTorchに比較して自由度こそ落ちますが初心者でもプログラムを組みやすいし、何が起こっているのかがわかりやすいんですよ。それでは、いきなりプログラムにいきます。”

```python
# Practical Text Classification With Python and Keras
import pandas as pd

filepath = './sentiment_analysis/yelp_labelled.txt'
df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\t')
print(df)

# ------
# scikit-learnライブラリが提供するCountVectorizerを利用して文のベクトル化を行うことができます。
# これは各文章の単語を取り込み、文中のすべてのユニークな単語の語彙を作成します。
# この語彙を利用して単語の数を表す特徴ベクトルを作成することができます
from sklearn.feature_extraction.text import CountVectorizer

sentences = ['John likes ice cream', 'John hates chocolate.']
vectorizer = CountVectorizer(min_df=0, lowercase=False)
vectorizer.fit(sentences)
print(vectorizer.vocabulary_)

# ------
# この語彙は各単語の索引の役割も果たしています。
# 各文章を取り出して、前回の語彙をもとに単語の出現率を求めることができます。
# 語彙は文中の5つの単語すべてで構成され、それぞれが語彙の中の1つの単語を表しています。
# 前の2つの文を取り出してCountVectorizerで変換すると、文の各単語のカウントを表すベクトルが得られます
vectorizer.transform(sentences).toarray()

# ------
# データのスプリット(train-test)
from sklearn.model_selection import train_test_split

sentences = df['sentence'].values
y = df['label'].values

sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)

```

![imageJRL3-30-2](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-2.jpg)

D先生 ： “前回のSentiment_Analysisでもレストランの口コミでした。今回は、文が短めのようですね・・・。”

C部長  ： “文章が短い方が、判別がやりやすくなるでしょうね。”

```python
# ------
# 前回のBOWモデルで使用した文のベクトル化を再度行います。
# この作業にはCountVectorizerを再び使用することができます。
# 訓練データのみを使用して語彙を作成します。
# この語彙を使ってトレーニングセットとテストセットの各文章の特徴ベクトルを作成することができます
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
vectorizer.fit(sentences_train)

X_train = vectorizer.transform(sentences_train)
X_test  = vectorizer.transform(sentences_test)
#X_train
print(X_train.toarray()[:10,:30])

```

![imageJRL3-30-3](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-3.jpg)

```python
# ------
# 得られた特徴ベクトルには750個のサンプルがあり、これは訓練とテストの分割後に得られた訓練サンプルの数であることがわかります。
# 各サンプルは1714次元で語彙のサイズに相当します。また、スパース行列が生成されています。
# これは非ゼロ要素が数個しかない行列のために最適化されたデータ型で、非ゼロ要素のみを追跡することでメモリ負荷を軽減します。
# CountVectorizerはトークン化を行い、以前に語彙で見たように文章をトークンの集合に分離します。
# ---
# ロジスティック回帰はシンプルかつ強力な線形モデルであり、数学的に言えば、入力された特徴ベクトルに基づいて0と1の間で回帰する形式です。
# カットオフ値（デフォルトでは0.5）を指定することで、回帰モデルが分類に使用されます。
# LogisticRegression 分類器を提供するためにscikit-learn ライブラリを使用します。
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression()
classifier.fit(X_train, y_train)
score = classifier.score(X_test, y_test)
print("Accuracy:", score)
#Accuracy: 0.796

```

![imageJRL3-30-4](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-4.jpg)

QEU:FOUNDER  ： “C先生、大当たり！！このデータベースであるならば、**ロジスティック分析でも80％ぐらいの判別精度**をたたき出すことができます。これをベースラインとしてディープラーニングによる予測に行きましょう！！”

```python
# ------
# それでは、前回のロジスティック回帰モデルに改良を加えられるかどうか見てみましょう。
# 先ほどの例で構築した X_train と X_test 配列を使用することができます。
# モデルを構築する前に特徴ベクトルの入力次元を知る必要があります。
# 次の層は自動的な形状推論を行うことができるため、これは最初の層でのみ行われます。
# Sequentialモデルを構築するためには以下のように順番に1つずつレイヤーを追加していきます：
from keras.models import Sequential
from keras import layers

input_dim = X_train.shape[1]  # Number of features(vocab)

model = Sequential()
model.add(layers.Dense(30, input_dim=input_dim, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])
model.summary()

```

![imageJRL3-30-5](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-5.jpg)

QEU:FOUNDER  ： “**Kerasのありがたみはsummary機能でディープラーニングの構造がわかる**ことです。”

### 1714(語彙の数)　x 30(ノード数) = 51450 connections

C部長 ： “ つまり、1714次元の語彙の出現頻度マトリックスから予測するんですね。ただし、どうせ**「中身は0ばかり」のベクトル**なんでしょうね。”

```python
# ------
# .fit()関数でトレーニングを開始しましょう。
history = model.fit(X_train, y_train, epochs=60, verbose=True,
                    validation_data=(X_test, y_test), batch_size=20)

# ------
# ここで、.evaluate()メソッドを使用して、モデルの精度を測定することができます。
# これはトレーニングデータとテストデータの両方に対して行うことができます。訓練データの方がテストデータよりも精度が高いことが予想されます。
# ニューラルネットワークを長く訓練すればするほどオーバーフィッティングが始まる可能性が高くなります。
loss, accuracy = model.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))

```

![imageJRL3-30-6](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-6.jpg)

```python
# ------
import matplotlib.pyplot as plt
plt.style.use('ggplot')

# loss	accuracy	val_loss	val_accuracy
def plot_history(history):
    acc = history.history['accuracy']
    eval_acc = history.history['val_accuracy']
    loss = history.history['loss']
    eval_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='train_accuracy')
    plt.plot(x, eval_acc, 'r', label='test_accuracy')
    plt.title('accuracy trend')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='train_loss')
    plt.plot(x, eval_loss, 'r', label='test_loss')
    plt.title('loss trend')
    plt.legend()

plot_history(history)

```

![imageJRL3-30-7](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-7.jpg)

QEU:FOUNDER  ： “びっくりするのは、**こんなにスカスカ（疎:sparse）なインプットでもある程度の精度が出る**んですよね。しかし、それでも高級な手法(ディープラーニング)でもロジスティック予測と変わらないレベルなんです。ここで、入力情報を密（dense）にするための検討を始めます。”

```python
# ------
# まず、Tokenizerユーティリティクラスを使って、テキストコーパスを整数のリストにベクトル化することができます。
# 各整数はコーパス全体を符号化する辞書の値に対応し、辞書のキーは語彙そのものです。
# num_wordsというパラメータを追加することができ、これは語彙の大きさを設定する役割を果たします。
# そして、最も一般的なnum_wordsの単語が保持されることになります。前の例で用意したテストとトレーニングデータがあります：
from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=3000)
tokenizer.fit_on_texts(sentences_train)

X_train = tokenizer.texts_to_sequences(sentences_train)
X_test = tokenizer.texts_to_sequences(sentences_test)

vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index
print(sentences_train[2])
print(X_train[2])
#Of all the dishes, the salmon was the best, but all were great.
#[11, 43, 1, 171, 1, 283, 3, 1, 47, 26, 43, 24, 22]

# ------
# 各単語のインデックスはTokenizerオブジェクトのword_index辞書を見ることで確認することができます
for word in ['the', 'salmon', 'was', 'best']:
    print('{}: {}'.format(word, tokenizer.word_index[word]))

```

![imageJRL3-30-8](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-8.jpg)

```python
# ------
# 問題点として、各テキスト列はほとんどの場合、単語の長さが異なることが挙げられます。
# この問題に対処するために、pad_sequence()を使用すると単純にゼロで単語の列を埋めることができます。
# デフォルトではゼロを前置するようになっていますが、私たちはゼロを後置したいのです。
# 一般に、ゼロを前置するか後置するかは重要ではありません。
# さらに、maxlenパラメータを追加してシーケンスの長さを指定したいと思います。
# これにより、その数値を超える配列はカットされます。
from keras_preprocessing.sequence import pad_sequences

maxlen = 100
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test  = pad_sequences(X_test, padding='post', maxlen=maxlen)
print(X_train[0, :])

```

![imageJRL3-30-9](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-9.jpg)

QEU:FOUNDER  ： “文の長さの最大値を100単語に指定して、前から順番に語彙番号で埋めてしまいます。それをディープラーニングに入力していきます。**Embeddingで変換**してね・・・。”

```python
# ------
# Embeddingレイヤーの出力を、Denseレイヤーに差し込むには、Denseレイヤーのための連続した
# 入力を準備するFlattenレイヤーを間に追加する必要があります
# これらのEmbedding層の重みはランダムな重みで初期化され、学習時にバックプロパゲーションで調整されます。
from keras.models import Sequential
from keras import layers

embedding_dim = 50
model = Sequential()
model.add(layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=maxlen))
model.add(layers.Flatten())
model.add(layers.Dense(30, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.summary()

```

![imageJRL3-30-10](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-10.jpg)

D先生 ： “ほう、なるほど・・・。Summaryの次元を見れば、embeddingが何をやっているのかわかりますね。そういえば、Embeddingの重みはどのように決めたんですか？”

QEU:FOUNDER  ： “例によって、**KerasでもEmbeddingの重みの初期値はランダムに決まっています**。それでは、つづきましょう・・・。”

```python
# ------
# このモデルは文の順番に来る単語をそのまま入力ベクトルとして受け取ります。
# 以下のようにして学習させることができます
history = model.fit(X_train, y_train,
                    epochs=50,
                    verbose=True,
                    validation_data=(X_test, y_test),
                    batch_size=20)


# ------
loss, accuracy = model.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))
plot_history(history)

```

![imageJRL3-30-11](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-11.jpg)

C部長  ： “あんまり予測精度は高くないですね。**ランダムな初期値って、あんまり役に立たない**ですね。”

D先生： “まあ、いきなり決めつけはよくないので、もうちょっと工夫をしてみましょう。”

```python
# ------
# エンベッディングを扱うもう一つの方法は、エンベッディングの後にMaxPooling1D/AveragePooling1DやGlobalMaxPooling1D/GlobalAveragePooling1D層を使う方法です。
# プーリングレイヤーは入力される特徴ベクトルをダウンサンプリングする（サイズを小さくする）方法と考えることができます。
# 最大プーリングの場合は各特徴次元についてプール内の全特徴の最大値をとります。平均プーリングの場合は平均値を取りますが、大きな値を強調するため、
# 最大プーリングがより一般的に使用されているようです。
#from keras.models import Sequential
#from keras import layers

#embedding_dim = 50
model = Sequential()
model.add(layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=maxlen))
model.add(layers.GlobalMaxPool1D())
model.add(layers.Dense(30, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.summary()

```

![imageJRL3-30-12](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-12.jpg)

QEU:FOUNDER  ： “**マックス・プーリングで圧縮**してみました。そうするとディープラーニングに引き渡す入力量はたった**50次元**になります。”

C部長  ： “そんなもんで精度がでるんですか？”

```python
# ------
history = model.fit(X_train, y_train,
                    epochs=30,
                    verbose=True,
                    validation_data=(X_test, y_test),
                    batch_size=20)
                    
# ------        
loss, accuracy = model.evaluate(X_train, y_train, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))
plot_history(history)

```

![imageJRL3-30-13](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-13.jpg)

QEU:FOUNDER  ： “なんと、逆に精度があがるんですね・・・(笑)。つまり、圧縮しないと「ありがたみがない」のかもね。”

C部長  ： “それでも上がり代が少ないです。結局は、**Embeddingの重み次第**だと思いますが・・・。”

QEU:FOUNDER  ： “この疑問をもちつつ、次に進みましょう。”


## ～　まとめ　～

### ・・・　前回のつづきです　・・・

QEU:FOUNDER ： “これこそが、**「真のイノベーション、新しい産業革命」の正体**ですよ。いままで、「〇〇こそ！イノベーション！！」とか「一石全鳥！！！」とかいう売り込みで儲けられた人たちは大打撃を受けるんじゃないかな？ただし、本当の、それこそ真のイノベーションを持っている人は今後に大発展をする可能性を持っています。Pre_trainされたシステムに自分のノウハウを組み込んで、自分のGPTを生成して儲ければいいんだし・・・。”

![imageJRL3-30-14](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-14.jpg)

C部長 : “そんな未来は面白そうですね。例えば、学会の過去の論文をブチ込んでみれば、その内容をGPT様が**自律的に学習**して、学生のかわりに勝手に論文を作成したりして・・・(笑)。”

QEU:FOUNDER ： “**そういうのはあるかもしれないし、ないかもしれない。**”

C部長 : “えっ！あるの・・・！？ボクは冗談で言ったんですが。”

![imageJRL3-30-15](/2023-05-02-QEUR23_SNLPS10/imageJRL3-30-15.jpg)

QEU:FOUNDER ： “この記事はニューラルネットを大きくしていくと**突然に創発的な（または～に見える）機能が発揮されること**について論じています。しかし、それがなぜ起こるのかは著者にもよく分からないようです。とにかく「もっとシステムを大きくしたらどうか」が彼の結論のようですが・・・。小生、個人的には、他にも**マルチモーダル(テキストと画像を学習させる)**などの導入がより重要と思いますがね・・・。”

C部長 : “どちらにしち、「そこまでする」のはちょっとね・・・。”

QEU:FOUNDER ： “早く、**GPT-4がだれでも気軽にカスタマイズして使える**世界になればいいですね。”



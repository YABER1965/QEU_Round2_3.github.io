---
title: QEUR23_MLTS5:　閑話休題～「Embedding」ってなんだ！？(その2) 
date: 2023-05-01
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_MLTS5:　閑話休題～「Embedding」ってなんだ！？(その2) 

## ～　「天下統一」後の世界（つづき）　～

QEU:FOUNDER  ： “さて、GPTがさらに進化をつづけています。もう、しばらくするとprompt_engineeringもいらなくなるのか？”

[![MOVIE1](http://img.youtube.com/vi/rbqm1ANfwVk/0.jpg)](http://www.youtube.com/watch?v=rbqm1ANfwVk "【エンジニア終了？】全自動AI「AutoGPT」が完全自動でプログラムを組む過程がヤバすぎる")

D先生 ： “・・・というわけでもないですよ。あの**「超御大」**がコースを作ったほどだし・・・。”

![imageJRL3-25-1](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-1.jpg)

C部長  ： “あれ？OKさんじゃなくって！？誰？この人・・・。”

QEU:FOUNDER  ： “この人、**AI界のレジェンド**・・・（笑）。まあ、詳しくはWikiで調べてください。”

D先生 ： “ほう、なるほど・・・。C国とA国にはライバル関係はあっても、現実には対立関係がないということがわかります。**メディアで聞いている対立は、ただ単に演出されたものにすぎません**。”

![imageJRL3-25-2](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-2.jpg)

QEU:FOUNDER  ： “ちなみに**画像認識ですでにデファクトになりつつあるResNetは「どこの人」が開発した**の？愛国ネトウヨはAIを使っちゃダメです。・・・さて、ここから本題。**前回は「Embeddingとはなにか？」でとまっちゃった**んでしたね。”

```python
# -----
class DotProduct(Module):
    def __init__(self, n_users, n_movies, n_factors):
        self.user_factors = Embedding(n_users, n_factors)
        self.movie_factors = Embedding(n_movies, n_factors)
        
    def forward(self, x):
        users = self.user_factors(x[:,0])
        movies = self.movie_factors(x[:,1])
        return (users * movies).sum(dim=1)


# -----
model = DotProduct(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3)
```

C部長 ： “**潜在変数(latent factor)**を設定して、その内積によって損失を計算したいという意図はわかりました。でも、プログラムのEmbeddingがなにかがわからないと、プログラムと概念がつながらないんです。”

![imageJRL3-25-3](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-3.jpg)

QEU:FOUNDER  ： “とあるC国のブログの内容を参考にして、Embeddingとは何かを見ていきましょう。簡単なPyTorchプログラムです、ドン！”

```python
# Embeddingとは
import torch
import torch.nn as nn

# 変数：
# Embedding.weight (Tensor) - 学習可能な重み (num_embeddings, Embedding_dim) で、 (0, 1) から初期化されます。
# つまり、pytorchのnn.Embedding()は各単語ベクトルに対応するw-weightを自動的に学習することができます。
# パラメタ
# num_embeddings - エンベディングディクショナリの大きさ、ディクショナリに単語がいくつあるのか
# embedding_dim - 各エンベディング・ベクトルの大きさ

# Example：
# an Embedding module containing 10 tensors of size 3
embedding = nn.Embedding(10, 3)
# a batch of 2 samples of 4 indices each
input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
embedding(input)
#tensor([[[-0.0251, -1.6902,  0.7172],
#         [-0.6431,  0.0748,  0.6969],
#         [ 1.4970,  1.3448, -0.9685],
#         [-0.3677, -2.7265, -0.1685]],
#
#        [[ 1.4970,  1.3448, -0.9685],
#         [ 0.4362, -0.4004,  0.9400],
#         [-0.6431,  0.0748,  0.6969],
#         [ 0.9124, -2.3616,  1.1151]]])
```

![imageJRL3-25-4](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-4.jpg)

QEU:FOUNDER  ： “一応、普通にEmbeddingを使ってワンホットを潜在変数に変換してみました。”

C部長  ： “あれ？ワン・ホットはどこにあるんですか？”

### １　→　[0,1,0,0,0…]
### ２　→　[0,0,1,0,0…]
### ３　→　[0,0,0,1,0…]

D先生： “ 変数inputに代入されている**テンソルの整数値はワンホットになっています**。このプログラムでは、ワンホットベクトルは10次元（0～9）だけ定義されています。そして、各ワンホットベクトルは3次元の潜在変数に圧縮されます。”

QEU:FOUNDER  ： “ついでにいえば、nn.Embeddingはディープラーニング用の関数なので、テンソルには重み値のほかに、自動微分の値が附属しています。じゃあ、つづきにいきましょう・・・。”

```python
# 勾配を外す
detached_input = embedding(input)
detached_input.detach()
```

![imageJRL3-25-5](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-5.jpg)

```python

# 重みの中を見てみる
embedding.weight

```

![imageJRL3-25-6](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-6.jpg)

C部長  ： “なるほど、**nn.Embeddingはこんな構造になっているんですか。Detach指示で、自動微分の勾配値を削除すれば、重み値テンソルだけが得られます**。でも、ちょっと変ですね。プログラムを実行するごとに潜在変数の値が変わってしまいます。”

QEU:FOUNDER  ： “０以外はね・・・。この値は**初期値にすぎない**ので、値を**ランダムに出力**しているだけです。つぎは重みを指定してみましょう。”

```python
# <from_pretrained>インスタンス
# 与えられた2D FloatTensorから埋め込みインスタンスを作成する。
# パラメータ．
# embeddings - 埋め込みの重みを含むFloatTensor．
# 1次元目は num_embeddings (ここでは2)として，2次元目は Embedding_dim (ここでは3)として Embeddingに渡される．

# Example：
# FloatTensor containing pretrained weights
weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
embedding = nn.Embedding.from_pretrained(weight)
# Get embeddings for index 1
input = torch.LongTensor([1])
embedding(input)
#tensor([[ 4.0000,  5.1000,  6.3000]])

```

![imageJRL3-25-7](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-7.jpg)

```python
# 重みの中を見てみる（再び）
embedding.weight

# 現在の重みのサイズは？
embedding.weight.shape
#embedding.weight.size()
```

![imageJRL3-25-8](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-8.jpg)

C部長  ： “Pre_trainingにより、重みを外部から指定できるんですね。これで、Embeddingとは何か、どのように値が学習されるのかがわかりました。”

QEU:FOUNDER  ： “**Embeddingというのは、Sparse(疎)であるワンホットベクトルを、より密(dense)なベクトルに圧縮します**。NLP（自然言語処理）では、Embeddingの後で関数の当てはめが行なわれます。一方、collaboration_filteringは特殊な例であり、内積を損失関数として最小になるように重みを変更するわけです。”

C部長 ： “ これらの事柄がわかって初めて、言語系のディープラーニングを理解出来るのか。ああ・・・、道は長いですね。 ”

QEU:FOUNDER  ： “言語系のディープラーニングはAIの主流になりつつあります。我々もちょっと遅かったが、どんなものかを理解するのはよいことだと思います。”


## ～　まとめ　～

C部長 : “前のブログの話にもどりますが、FOUNDERは、なぜ**「天下統一が来た！」**と思ったんですか？確かにChatGPTはすごいとは思いますが・・・。”

[![MOVIE2](http://img.youtube.com/vi/fCEHdyLkjNE/0.jpg)](http://www.youtube.com/watch?v=fCEHdyLkjNE "站在巨人的肩膀上, 迁移学习 Transfer Learning")

C部長 : “あれ？FOUNDERが敬愛している、あのC国の（強化学習の）大先生がなぜかYoutubeに出てきた・・・（笑）。”

QEU:FOUNDER ： “**「站在巨人的肩膀上」**って表現、かっこいいと思わない？ここでGPTの意味ですが、**Generative Pre-trained Transformer**です。ResNetのような画像判別のシステムに対して転移学習がつかえるというのは、小生も分かっていました。しかし、言語処理のこのように巨大なシステムでも転移学習が適用可能とは・・・。”

![imageJRL3-25-9](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-9.jpg)

C部長 : “・・・とても驚きですね。しかし、それが可能であるからこそ、最近、大量のGPTシステムが出現しているんです。**個人レベルのプロジェクト、非常に低予算でも、Pre-trainのシステムに自分独自のノウハウ（情報・知識）を注入して新しいモノをリリースすることができます。**”

![imageJRL3-25-10](/2023-05-01-QEUR23_MLTS5/imageJRL3-25-10.jpg)

QEU:FOUNDER ： “これこそが、**「真のイノベーション、新しい産業革命」の正体**ですよ。いままで、「〇〇こそ！イノベーション！！」とか「一石全鳥！！！」とかいう売り込みで儲けられた人たちは**大打撃を受ける**んじゃないかな？ただし、本当の、それこそ**真のイノベーションを持っている人は今後に大発展をする可能性を持っています**。Pre_trainされたシステムに自分のノウハウを組み込んで、自分のGPTを生成して儲ければいいんだし・・・。”

C部長 : “QEU_GPTか・・・。”

QEU:FOUNDER ： “できるといいねぇ・・・（笑）。”


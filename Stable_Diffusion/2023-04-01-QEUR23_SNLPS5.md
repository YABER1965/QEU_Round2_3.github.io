---
title: QEUR23_SNLPS5:　T法(2)による Sentiment Analysis (Yelp_Review、その2)
date: 2023-04-01
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "Stable_Diffusion", "Collaborative filtering", "AI art"]
excerpt: T法(2)をテクノメトリックスとして使ったAIアートの最適化
---

## QEUR23_SNLPS5:　T法(2)による Sentiment Analysis (Yelp_Review、その2)

## ～　ベイズとT法の類似性（前編）　～

D先生 ： “いよいよ、因縁のBag_Of_Wordsファイルの生成です。ちなみに、この番組は**「高齢者によるイノベーション」**です。久々だなァ・・・。このコメント・・・。”

![imageJRL3-15-1](/2023-04-01-QEUR23_SNLPS5/imageJRL3-15-1.jpg)

QEU:FOUNDER  ： “**このファイル(↑)の生成手順**って、今回の場合はすごくややこしいんです。従って、プログラムもややこしい。いきなりプログラムをドン！”

```python
# -----
# Bag_Of_Wordsを計算する(For T-method(2))
# -----
from fastbook import *
from fastai.text.all import *
from collections import Counter

# -----
# 生データを読み込む
#df_train = pd.read_csv('./yelp_reviews_org_train.csv')      # reviews_with_splits_lite(new)
#df_train.head()

# -----
# 修正済みコーパス・データを読み込む
df_corpus = pd.read_csv('./csv_T2Corpus.csv')      # reviews_with_splits_lite(new)
df_corpus

```

![imageJRL3-15-2](/2023-04-01-QEUR23_SNLPS5/imageJRL3-15-2.jpg)

```python
# -----
# T法(2)計算シートCSVファイルに出力する
df_T2Calc = pd.read_csv('./csv_T2Calc.csv')      # reviews_with_splits_lite(new)
df_T2Calc

```

![imageJRL3-15-3](/2023-04-01-QEUR23_SNLPS5/imageJRL3-15-3.jpg)

D先生 ： “この２つのデータフレームが今回のインプットになります。単位空間が10レコードであり、信号空間が21水準です。本当に少ないデータ数ですね。”

QEU:FOUNDER ： “T法の場合には**単位空間が１レコード、信号空間が3レコードあれば予測ができます**。”

D先生 ： “しかも、**項目数（説明変数X）がいくら多くても問題はありません**。それでは説明をつづけましょう。”

```python
# -------
# コーパスで使用されている単語集（重複あり）をリスト化する
arr_vocab = df_corpus.loc[:,"name"].values

# 重複を解消して、ボキャブラリをリスト化する
arr_set_vocab = list(set(arr_vocab)) 
arr_numb_vocab = [i for i in range(len(arr_set_vocab))]
arr_set_vocab[0:10], len(arr_set_vocab)
#arr_numb_vocab[0:10]

# -----
# 参照TEXT-NOを抽出する
arr_seq_text = df_corpus.loc[:,"text"].values
arr_seq_text = list(set(arr_seq_text))
#print(arr_seq_text[0:50])
#print(arr_seq_text[0:50])

# -----
# positive - nagativeのトークン件数を数える
arr_cnt_positive = []
arr_cnt_negative = []
arr_cnt_total = []
for vocab in arr_set_vocab:   # arr_set_vocab[0:10]
    temp_df_positive = df_corpus[(df_corpus.name==vocab) & (df_corpus.label=="positive")]
    temp_df_negative = df_corpus[(df_corpus.name==vocab) & (df_corpus.label=="negative")]
    len_df_positive  = len(temp_df_positive)
    len_df_negative  = len(temp_df_negative)
    #print("vocab: {}, positive: {}, negative: {}".format(vocab,len_df_positive,len_df_negative))
    arr_cnt_positive.append(len_df_positive)
    arr_cnt_negative.append(len_df_negative)
    arr_cnt_total.append(len_df_positive+len_df_negative)
#print(arr_cnt_negative[0:20])
#print("--------------")
#print(arr_cnt_positive[0:20])

# -----
# Bag_Of_Words用のデータ・フレームを生成する
mx_seq   = np.array([arr_numb_vocab]).T
mx_vocab = np.array([arr_set_vocab]).T
mx_positive = np.array([arr_cnt_positive]).T
mx_negative = np.array([arr_cnt_negative]).T
mx_total = np.array([arr_cnt_total]).T
mx_T2_BgOfWd  = np.concatenate([mx_seq, mx_vocab, mx_positive, mx_negative, mx_total], axis=1)
df_T2_BgOfWd  = pd.DataFrame(mx_T2_BgOfWd, col-umns=["NO","vocab","positive","negative","total"])

# CSVファイルに出力する(Bag_Of_Words)
#df_T2_BgOfWd.to_csv("csv_T2_BgOfWd.csv")
df_T2_BgOfWd.head()

# -----
# T法(2)計算シートの参照NOマトリックスを読み込む
mx_refNo = df_T2Calc.loc[:, "ref0":"ref19"].values
mx_refNo

# -----
# T法(2)計算シートの行ごとのメトリックスを計算する関数
def calc_T2vocab(lineNo):

    # 参照番号の文のトークンを抽出する
    arr_refNo = mx_refNo[lineNo,:]  #; arr_refNo
    acc_tok_corpus = np.array([])
    for val_refNo in arr_refNo:   # arr_set_vocab[0:10]
        temp_df_corpus = df_corpus[df_corpus.text==val_refNo]
        arr_ref_corpus = temp_df_corpus.loc[:,"name"].values
        #print("vocab: {}, positive: {}, negative: {}".format(vocab,len_df_positive,len_df_negative))
        acc_tok_corpus = np.append(acc_tok_corpus, arr_ref_corpus)
    acc_tok_corpus = np.array(acc_tok_corpus).flatten()

    # setでボキャブラリの重複を解消させる
    arr_set_vocab = list(set(acc_tok_corpus))

    # Counterで出現回数を数える
    arr_counter = Counter(acc_tok_corpus)

    return len(acc_tok_corpus), acc_tok_corpus, arr_set_vocab, arr_counter

# -----
# Bag of Wordsのデータを回復する
arr_vocab_BgOfWd = df_T2_BgOfWd.loc[:,"vocab"].values
print(arr_vocab_BgOfWd[0:50])

# 出現回数マトリックスを初期設定する
len_BgOfWd  = len(arr_vocab_BgOfWd)
mx_count = np.array([[0]*31 for i in range(len_BgOfWd)])

# -----
# T法(2)計算シートの情報からトークンの使用状況を抽出する
arr_len    = []
mx_tok_corpus  = []
mx_set_vocab   = []
mx_counter     = []
# -----
for lineNo in range(31):
    val_len, arr_tok_corpus, arr_set_vocab, dic_counter = calc_T2vocab(lineNo)

    # -----
    for iCnt, vocab in enumerate(arr_vocab_BgOfWd):
        num_count = dic_counter[vocab]
        #print("vocab:{} ,num_count: {}".format(vocab, num_count))
        mx_count[iCnt, lineNo] = num_count

    # -----
    arr_len.append(val_len)
    mx_tok_corpus.append(arr_tok_corpus)
    mx_set_vocab.append(arr_set_vocab)
    mx_counter.append(dic_counter)
# -----
print(arr_len)
#print(mx_tok_corpus[0:5])
#print(mx_set_vocab[0:5])
#print(mx_counter[0:5])
# -----
print(mx_count[0:10, 0:10])

# -----
# 各ボキャブラリの信号空間(n=21)での出現件数を計算する
arr_ttl_cnt = [0]*len_BgOfWd
# -----
for vocabNo in range(len_BgOfWd):
    # 各ラインでスライスして合計
    arr_line  = mx_count[vocabNo, 0:21]
    val_total = np.sum(arr_line)
    arr_ttl_cnt[vocabNo] = val_total
# -----
print(arr_ttl_cnt[0:50])

```

D先生 ： “あれ？なぜコメント文に「各ボキャブラリの信号空間(n=21)での出現件数を計算する」と書いているんですか？T法では単位空間を解析に使わないということですか？”

QEU:FOUNDER ： “T法(2)では、単位空間のデータと信号空間データの役割が違います。ここで、**T法(2)でつかう感度βとSN比ηの考え方**を説明します。まずは信号空間のデータをY’-X’散布図にプロットします。”

![imageJRL3-15-4](/2023-04-01-QEUR23_SNLPS5/imageJRL3-15-4.jpg)

D先生 ： “X-Y散布図ではなく、Y’-X’（散布図）というのがミソですね。”

- **Y’ = Y(信号空間) – Y(単位空間)**
- **X’ = X(信号空間) – X(単位空間)**

QEU:FOUNDER ： “くわしくは、以前のT法(2)の説明をみてください。ここでは、どういう散布状態でSN比ηが大きくなるのかを説明します。例に挙げた3種の散布図のうち、SN比がもっとも大きいのが左図、小さいのが右図です。”

D先生 ： “散布図のバラツキに傾向があるかどうか（相関）で3図を評価すると、右の図でもかなり良いです。しかし、右図の場合には**分布が0点を通っていない点**がSN比に影響を与えるんです。”

QEU:FOUNDER  ： “その0点を通るかどうかは、単位空間で決まります。”

```python
# -----
#　Bag_Of_Wordsにデータを追加する
df_T2_BgOfWd['sig_ttl'] = arr_ttl_cnt
#df_T2_BgOfWd

# T2計算マトリックス
arr_column_signal = ["sig{}".format(i) for i in range(21)]
arr_column_tani   = ["tani{}".format(i) for i in range(10)]
arr_column   = arr_column_signal + arr_column_tani; arr_column
df_T2matrix = pd.DataFrame(mx_count, columns=arr_column); df_T2matrix

# 3つのデータフレームを結合する
temp_T2_BgOfWd = pd.concat([df_T2_BgOfWd, df_T2matrix], axis=1)  #; df_T2_BgOfWd
len(temp_T2_BgOfWd)

# -------
# Bag of Wordsデータベースをさらに小さくする
df_T2_BgOfWd_first = temp_T2_BgOfWd[temp_T2_BgOfWd.loc[:,'total'] > '6']
print(len(df_T2_BgOfWd_first))

# CSVファイルに出力する(Corpus for T2 analysis)
df_T2_BgOfWd_first.to_csv("csv_T2_BgOfWd_first.csv")
#df_T2_BgOfWd_first
```

![imageJRL3-15-5](/2023-04-01-QEUR23_SNLPS5/imageJRL3-15-5.jpg)

D先生 ： “おっと・・・。**もとは3402件あったBag_Of_Wordsのレコードが、409件まで減ってしまいました**。信号空間数のしきい値を6件以上と決めたんですね？”

QEU:FOUNDER ： “6件は、T法の場合には妥当な数量だと思います。”

```python
# -------
# T法(2)計算シートCSVファイルに情報を追加する
df_T2Calc_first = df_T2Calc
df_T2Calc_first['len_line'] = arr_len
#df_T2Calc_first

# 修正されたロジットを計算する（基準は1000）
mod_logi = np.array(arr_len)*df_T2Calc_first.loc[:,'logi'].values/1000
df_T2Calc_first['mod_logi'] = mod_logi
#df_T2Calc_first

# -----
# T法(2)計算シートCSVファイルに出力する
df_T2Calc_first.to_csv('./csv_T2Calc_first.csv')      # reviews_with_splits_lite(new)

```

![imageJRL3-15-6](/2023-04-01-QEUR23_SNLPS5/imageJRL3-15-6.jpg)

QEU:FOUNDER  ： “そして、T法の（信号空間と単位空間）レコード定義データに、応答変数(Y)として**修正ロジット(mod_logi)値**のコラムが追加されます。”

D先生 ： “あれ？各文のトークン数(len_line)という数字がありますが、この数字は新しいBag_Of_Wordsでは変わっちゃうでしょ？”

QEU:FOUNDER  ： “べつに、このまま使っておいてもいいんじゃない？もし、これを変えたいんであればエクセルでも変えられるけどね・・・。もともと、これは「（文が）長い-短い」という参考的な情報でしかないですよ。ロジット値（本来は0-1）だから・・・。”

D先生 ： “そうねえ・・・。逆に言うと、むしろデータ数が少ないこと（信号数が21件）がネックになるでしょうね。”

QEU:FOUNDER  ： “品質統計の知見では、**信頼できる情報を得るには、連続数の情報は30件以上が必要であり、不良(0/1)情報では300件以上が必要**です。ただし、今回300件の情報を作ろうとしたら、もともとのデータを同じサイズになりますから我々としてはあまり計算時間をかけたくない・・・(笑)。”

D先生 ： “あくまで、これはテストです。**ナイーブ・ベイズとT法はほぼ同じ性質を持つ**ことを証明するという・・・。それでは、次のステップはいよいよ感度とSN比を計算しましょう。”

## ～　まとめ　～

QEU:FOUNDER ： “久々に、イケメンコーナーをやりますか・・・。”

![imageJRL3-15-7](/2023-04-01-QEUR23_SNLPS5/imageJRL3-15-7.jpg)

C部長 : “ああ・・・、これは久々だ・・・。”

[![MOVIE1](http://img.youtube.com/vi/PJa2pOU8ceM/0.jpg)](http://www.youtube.com/watch?v=PJa2pOU8ceM "れいわ政治的のど自慢大会2023 東京都・江戸川区！（3月30日19:00〜）")

QEU:FOUNDER ： “相変わらず**パワフル**だわ、この人・・・。”

![imageJRL3-15-8](/2023-04-01-QEUR23_SNLPS5/imageJRL3-15-8.jpg)

C部長 : “ホント、体力がありますね。”

QEU:FOUNDER ： “今やっている選挙は地方のものだから、中央のメッセージをそのまま受け入れるのはどうかとは言えますが・・・。結局、人が人を選ぶというわけで・・・。”

C部長 : “エリート様は好き嫌いで人を選ぶのはけしからんというのでしょうが・・・。”

QEU:FOUNDER ： “激動する世の中で、公約が有効になるのかもあやしい。**好きか嫌いでいい**んじゃない？あくまで個人的な意見ですがね。”

